{"cells":[{"cell_type":"markdown","metadata":{"id":"HVQuq23Q9Hyo"},"source":["# Training our first Convolutional Neural Network\n","\n","In this notebook we are going to build and train our first Convolutional Neural Network (CNN). We will also familiarize with the usage of Batch Normalization (BN) layers, and see it in action on [LeNet](https://ieeexplore.ieee.org/document/726791)."]},{"cell_type":"markdown","metadata":{"id":"MOI2KG4MeVBS"},"source":["## Batch Normalization"]},{"cell_type":"markdown","metadata":{"id":"UbwoewN0ZXod"},"source":["The aim of **batch normalization layers** is to standardize features within a mini-batch, such that their mean is 0 and variance is 1.\n","\n","More details can be found in the original [paper](https://arxiv.org/abs/1502.03167).\n","\n","$BN(x_{i, k}) = \\gamma_{k} \\frac{x_{i, k} - \\mu_{B, k}}{\\sqrt{\\sigma^{2}_{B,k} + \\epsilon}} + \\beta_{k}$\n","\n","With $\\gamma_{k}$ and $\\beta_{k}$ being learnable parameters.\n","\n","The intuitive idea behind BN is as follows: a neural network is trained using mini-batches, and the distribution of inputs **varies** from one batch to the other. Difference in distributions between mini-batches can cause the training to be **unstable** and heavily **dependant on the initial weights** of the network. Therefore, this kind of transformation (transforming the inputs to have mean 0 and unit variance) guarantees that input distribution of each layer remains **unchanged across mini-batches**.\n","\n","More interestingly, we will learn how to code BN layer from scratch using PyTorch. Let's start by importing the necessary libraries, as usual."]},{"cell_type":"markdown","metadata":{"id":"VtZwK8aDBxmK"},"source":["We start, as usual, by importing the necessary libraries."]},{"cell_type":"code","source":["%pip install tensorboard"],"metadata":{"id":"ordBiPI93Af3"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sIZU13zsxAM0"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torchvision\n","import torchvision.transforms as T\n","import torch.nn.functional as F\n","from torch.utils.tensorboard import SummaryWriter\n","from tqdm import tqdm # Progress bars!"]},{"cell_type":"markdown","metadata":{"id":"mC3UJ-c5c4kv"},"source":["We will first implement a BN layer for 1D tensors and then expand the implementation to the 2D case."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wYijFBBac-V1"},"outputs":[],"source":["\"\"\"\n","Applies Batch Normalization over a 1D input (or 2D tensor)\n","\n","Shape:\n","  Input: (N, C)\n","  Output: (N, C)\n","\n","Input Parameters:\n","  in_features: number of features of the input activations\n","  track_running_stats: whether to keep track of running mean and std. (default: True)\n","  affine: whether to scale and shift the normalized activations. (default: True)\n","  momentum: the momentum value for the moving average. (default: 0.9)\n","\n","Usage:\n","  >>> # with learable parameters\n","  >>> bn = BatchNorm1d(4)\n","  >>> # without learable parameters\n","  >>> bn = BatchNorm1d(4, affine=False)\n","  >>> input = torch.rand(10, 4)\n","  >>> out = bn(input)\n","\"\"\"\n","\n","class BatchNorm1d(nn.Module):\n","    def __init__(self, in_features, track_running_stats=True, affine=True, momentum=0.9):\n","        super().__init__()\n","\n","        self.in_features = in_features\n","        self.track_running_stats = track_running_stats\n","        self.affine = affine\n","        self.momentum = momentum\n","\n","        if self.affine:\n","            self.gamma = nn.Parameter(torch.ones(self.in_features, 1))\n","            self.beta = nn.Parameter(torch.zeros(self.in_features, 1))\n","\n","        if self.track_running_stats:\n","            # register_buffer registers a tensor as a buffer that will be saved as part of the model\n","            # but which does not require to be trained, differently from nn.Parameter\n","            self.register_buffer('running_mean', torch.zeros(self.in_features, 1))\n","            self.register_buffer('running_std', torch.ones(self.in_features, 1))\n","\n","    def forward(self, x):\n","        # Transpose (N, C) to (C, N)\n","        x = x.transpose(0, 1).contiguous().view(x.shape[1], -1)\n","\n","        # Calculate batch mean\n","        mean = x.mean(dim=1).view(-1, 1)\n","\n","        # Calculate batch std\n","        std = x.std(dim=1).view(-1, 1)\n","\n","        # During training keep running statistics (moving average of mean and std)\n","        if self.training and self.track_running_stats:\n","            # No computational graph is necessary to be built for this computation\n","            with torch.no_grad():\n","                self.running_mean = self.momentum * self.running_mean + (1 - self.momentum) * mean\n","                self.running_std = self.momentum * self.running_std + (1 - self.momentum) * std\n","\n","        # During inference time\n","        if not self.training and self.track_running_stats:\n","            mean = self.running_mean\n","            std = self.running_std\n","\n","        # Normalize the input activations\n","        x = (x - mean) / std\n","\n","        # Scale and shift the normalized activations\n","        if self.affine:\n","            x = x * self.gamma + self.beta\n","\n","        # Transpose back to original shape\n","        return x.transpose(0, 1)\n","\n","\"\"\"\n","Applies Batch Normalization over a 2D or 3D input (4D tensor)\n","\n","Shape:\n","  Input: (N, C, H, W)\n","  Output: (N, C, H, W)\n","\n","Input Parameters:\n","  in_features: number of features of the input activations\n","  track_running_stats: whether to keep track of running mean and std. (default: True)\n","  affine: whether to scale and shift the normalized activations. (default: True)\n","  momentum: the momentum value for the moving average. (default: 0.9)\n","\n","Usage:\n","  >>> # with learable parameters\n","  >>> bn = BatchNorm2d(4)\n","  >>> # without learable parameters\n","  >>> bn = BatchNorm2d(4, affine=False)\n","  >>> input = torch.rand(10, 4, 5, 5)\n","  >>> out = bn(input)\n","\"\"\"\n","\n","class BatchNorm2d(nn.Module):\n","    def __init__(self, in_features, track_running_stats=True, affine=True, momentum=0.9):\n","        super().__init__()\n","\n","        self.in_features = in_features\n","        self.track_running_stats = track_running_stats\n","        self.affine = affine\n","        self.momentum = momentum\n","\n","        if self.affine:\n","            self.gamma = nn.Parameter(torch.ones(self.in_features, 1))\n","            self.beta = nn.Parameter(torch.zeros(self.in_features, 1))\n","\n","        if self.track_running_stats:\n","            # register_buffer registers a tensor as a buffer that will be saved as part of the model\n","            # but which does not require to be trained, differently from nn.Parameter\n","            self.register_buffer('running_mean', torch.zeros(self.in_features, 1))\n","            self.register_buffer('running_std', torch.ones(self.in_features, 1))\n","\n","    def forward(self, x):\n","        # Transpose (N, C, H, W) to (C, N, H, W)\n","        x = x.transpose(0, 1)\n","\n","        # Store the shape\n","        c, bs, h, w = x.shape\n","\n","        # Collapse all dimensions except the 'channel' dimension\n","        x = x.contiguous().view(c, -1)\n","\n","        # Calculate batch mean\n","        mean = x.mean(dim=1).view(-1, 1)\n","\n","        # Calculate batch std\n","        std = x.std(dim=1).view(-1, 1)\n","\n","        # Keep running statistics (moving average of mean and std)\n","        if self.training and self.track_running_stats:\n","            with torch.no_grad():\n","                self.running_mean = self.momentum * self.running_mean + (1 - self.momentum) * mean\n","                self.running_std = self.momentum * self.running_std + (1 - self.momentum) * std\n","\n","        # During inference time\n","        if not self.training and self.track_running_stats:\n","            mean = self.running_mean\n","            std = self.running_std\n","\n","        # Normalize the input activations\n","        x = (x - mean) / std\n","\n","        # Scale and shift the normalized activations\n","        if self.affine:\n","            x = x * self.gamma + self.beta\n","\n","        return x.view(c, bs, h, w).transpose(0, 1)"]},{"cell_type":"markdown","metadata":{"id":"TLLNs9dy9pAM"},"source":["## Building our first CNN: LeNet-5\n","\n","![image.png](attachment:2cafc123-66e4-4713-9b91-e648b72a6388.png)\n","\n","In order to build this model, we are going to need some **convolutional** and some **fully connected** layers. The former can be easily defined by exploiting the `torch.nn.Conv2D` module from PyTorch. Remember you can always take a look at the [documentation](https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html)! We will also be using pooling operations (Max Pooling) to reduce the size of the feature maps. In particular, we are going to use the `torch.nn.functional.max_pool2d` module (details can be found, as usual, in the [docs](https://pytorch.org/docs/stable/generated/torch.nn.MaxPool2d.html)). Furthermore, for this model we are going to need the Rectified Linear Unit (ReLU) activation, available in the `torch.nn.functional.relu` module (details [here](https://pytorch.org/docs/stable/generated/torch.nn.functional.relu.html#torch.nn.functional.relu)).\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jtyBrLjJxNNm"},"outputs":[],"source":["class LeNet(nn.Module):\n","    def __init__(self, with_bn=False):\n","        super().__init__()\n","        self.with_bn = with_bn\n","\n","        # input channel = 3, output channels = 6, kernel size = 5\n","        # input image size = (28, 28), image output size = (24, 24)\n","        self.conv1 = nn.Conv2d(in_channels=3, out_channels=6, kernel_size=(5, 5))\n","        self.bn1 = BatchNorm2d(6) if self.with_bn else nn.Identity()\n","\n","        # input channel = 6, output channels = 16, kernel size = 5\n","        # input image size = (12, 12), output image size = (8, 8)\n","        self.conv2 = torch.nn.Conv2d(in_channels=6, out_channels=16, kernel_size=(5, 5))\n","        self.bn2 = BatchNorm2d(16) if self.with_bn else nn.Identity()\n","\n","        # input dim = 5 * 5 * 16 (H x W x C), output dim = 120\n","        self.fc3 = torch.nn.Linear(in_features=5 * 5 * 16, out_features=120)\n","        self.bn3 = BatchNorm1d(120) if self.with_bn else nn.Identity()\n","\n","        # input dim = 120, output dim = 84\n","        self.fc4 = torch.nn.Linear(in_features=120, out_features=84)\n","        self.bn4 = BatchNorm1d(84) if self.with_bn else nn.Identity()\n","\n","        # input dim = 84, output dim = 10\n","        self.fc5 = torch.nn.Linear(in_features=84, out_features=10)\n","\n","    def forward(self, x):\n","        # First convolutional layer + relu\n","        x = self.conv1(x)\n","        x = self.bn1(x)\n","        x = F.relu(x)\n","\n","        # Max Pooling with kernel size = 2\n","        # Output size = (12, 12)\n","        x = F.max_pool2d(x, kernel_size=2)\n","\n","        # Second convolutional layer + relu\n","        x = self.conv2(x)\n","        x = self.bn2(x)\n","        x = F.relu(x)\n","\n","        # Max Pooling with kernel size = 2\n","        # Output size = (4, 4)\n","        x = F.max_pool2d(x, kernel_size=2)\n","\n","        # Flatten the feature maps into a long vector (-> (bs, 4*4*16))\n","        x = x.view(x.shape[0], -1)\n","\n","        # First linear layer + relu\n","        x = self.fc3(x)\n","        x = self.bn3(x)\n","        x = F.relu(x)\n","\n","        # Second linear layer + relu\n","        x = self.fc4(x)\n","        x = self.bn4(x)\n","        x = F.relu(x)\n","\n","        # Output layer (linear)\n","        x = self.fc5(x)\n","\n","        return x"]},{"cell_type":"markdown","metadata":{"id":"obE13C39_2g1"},"source":["## Optimizer & cost function\n","We are going to use the familiar [Stochastic Gradient Descent (SGD)](https://pytorch.org/docs/stable/generated/torch.optim.SGD.html) optimizer and the [Cross Entropy Loss](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html) for our optimization."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JO_my8CC0Uhn"},"outputs":[],"source":["def get_cost_function():\n","    cost_function = torch.nn.CrossEntropyLoss()\n","    return cost_function\n","\n","def get_optimizer(net, lr, wd, momentum):\n","    optimizer = torch.optim.SGD(net.parameters(), lr=lr, weight_decay=wd, momentum=momentum)\n","    return optimizer"]},{"cell_type":"markdown","metadata":{"id":"qV2v8x0ZBG8y"},"source":["## Training and test steps\n","We are going to implement our training and test pipelines as discussed in the previous lab sessions."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nQmB7Kir8Y56"},"outputs":[],"source":["def train_one_epoch(net, data_loader, optimizer, cost_function, device=\"cuda\"):\n","    samples = 0.0\n","    cumulative_loss = 0.0\n","    cumulative_accuracy = 0.0\n","\n","    # Set the network to training mode\n","    net.train()\n","\n","    # Iterate over the training set\n","    for batch_idx, (inputs, targets) in enumerate(data_loader):\n","        # Load data into GPU\n","        inputs = inputs.to(device)\n","        targets = targets.to(device)\n","\n","        # Forward pass\n","        outputs = net(inputs)\n","\n","        # Loss computation\n","        loss = cost_function(outputs,targets)\n","\n","        # Backward pass\n","        loss.backward()\n","\n","        # Parameters update\n","        optimizer.step()\n","\n","        # Gradients reset\n","        optimizer.zero_grad()\n","\n","        # Fetch prediction and loss value\n","        samples += inputs.shape[0]\n","        cumulative_loss += loss.item()\n","        _, predicted = outputs.max(dim=1) # max() returns (maximum_value, index_of_maximum_value)\n","\n","        # Compute training accuracy\n","        cumulative_accuracy += predicted.eq(targets).sum().item()\n","\n","    return cumulative_loss/samples, cumulative_accuracy / samples * 100\n","\n","def test(net, data_loader, cost_function, device=\"cuda\"):\n","    samples = 0.0\n","    cumulative_loss = 0.0\n","    cumulative_accuracy = 0.0\n","\n","    # Set the network to evaluation mode\n","    net.eval()\n","\n","    # Disable gradient computation (we are only testing, we do not want our model to be modified in this step!)\n","    with torch.no_grad():\n","        # Iterate over the test set\n","        for batch_idx, (inputs, targets) in enumerate(data_loader):\n","\n","            # Load data into GPU\n","            inputs = inputs.to(device)\n","            targets = targets.to(device)\n","\n","            # Forward pass\n","            outputs = net(inputs)\n","\n","            # Loss computation\n","            loss = cost_function(outputs, targets)\n","\n","            # Fetch prediction and loss value\n","            samples+=inputs.shape[0]\n","            cumulative_loss += loss.item() # Note: the .item() is needed to extract scalars from tensors\n","            _, predicted = outputs.max(1)\n","\n","            # Compute accuracy\n","            cumulative_accuracy += predicted.eq(targets).sum().item()\n","\n","    return cumulative_loss / samples, cumulative_accuracy / samples * 100"]},{"cell_type":"markdown","metadata":{"id":"-xaALwsMD2vj"},"source":["## Data loading\n","In this block we are going to define our **data loading** utility. Differently from last time, in this case we are going to introduce **normalization**. This step is needed in order **bound** our values to the `[-1,1]` range, and obtain a **stable** training process for our network. This can be achieved by using the `torchvision.transforms.Normalize()` module (details [here](https://pytorch.org/vision/main/generated/torchvision.transforms.Normalize.html))."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"j1CzNOKhCe3h"},"outputs":[],"source":["def get_data(batch_size, test_batch_size=256, dataset='mnist'):\n","    # Prepare data transformations and then combine them sequentially\n","    if dataset == 'mnist':\n","        transform = T.Compose([                                                      # Compose the above transformations into one\n","            T.ToTensor(),                                                            # Convert Numpy to Pytorch Tensor\n","            T.Lambda(lambda x: F.pad(x, (2, 2, 2, 2), 'constant', 0)),               # Pad zeros to make MNIST 32 x 32\n","            T.Lambda(lambda x: x.repeat(3, 1, 1)),                                   # Make MNIST RGB instead of grayscale\n","            T.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])                   # Normalize the Tensors between [-1, 1]\n","        ])\n","    elif dataset == 'svhn':\n","        transform = T.Compose([                                                      # Compose the above transformations into one\n","            T.ToTensor(),                                                            # Convert Numpy to Pytorch Tensor\n","            T.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])                   # Normalize the Tensors between [-1, 1]\n","        ])\n","\n","    # Prepare dataset\n","    if dataset == 'mnist':\n","        full_training_data = torchvision.datasets.MNIST('./data/mnist', train=True, transform=transform, download=True)\n","        test_data = torchvision.datasets.MNIST('./data/mnist', train=False, transform=transform, download=True)\n","    elif dataset == 'svhn':\n","        full_training_data = torchvision.datasets.SVHN('./data/svhn', split='train', transform=transform, download=True)\n","        test_data = torchvision.datasets.SVHN('./data/svhn', split='test', transform=transform, download=True)\n","\n","    # Create train and validation splits\n","    num_samples = len(full_training_data)\n","    training_samples = int(num_samples * 0.8 + 1)\n","    validation_samples = num_samples - training_samples\n","    training_data, validation_data = torch.utils.data.random_split(full_training_data, [training_samples, validation_samples])\n","\n","    # Initialize dataloaders\n","    train_loader = torch.utils.data.DataLoader(training_data, batch_size, shuffle=True, drop_last=True, num_workers=8)\n","    val_loader = torch.utils.data.DataLoader(validation_data, test_batch_size, shuffle=False, num_workers=8)\n","    test_loader = torch.utils.data.DataLoader(test_data, test_batch_size, shuffle=False, num_workers=8)\n","\n","    return train_loader, val_loader, test_loader"]},{"cell_type":"markdown","metadata":{"id":"APuCISvfCrP6"},"source":["## Put it all together!\n","We are now ready to combine all the ingredients defined so far into our **training procedure**. We define a main function that **initializes** everything, **trains** the model over multiple epochs and **logs** the results."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ur0qy-JBChFy"},"outputs":[],"source":["def main(run_name,\n","         batch_size=128,\n","         device=\"cuda\",\n","         learning_rate=0.01,\n","         weight_decay=0.000001,\n","         momentum=0.9,\n","         epochs=25,\n","         dataset=\"mnist\",\n","         with_bn=False):\n","    # Creates a logger for the experiment\n","    writer = SummaryWriter(log_dir=f\"runs/{run_name}\")\n","\n","    # Get dataloaders\n","    train_loader, val_loader, test_loader = get_data(batch_size=batch_size, test_batch_size=batch_size, dataset=dataset)\n","\n","    # Instantiate model and send it to cuda device\n","    net = LeNet(with_bn=with_bn).to(device)\n","\n","    # Instatiate optimizer and cost function\n","    optimizer = get_optimizer(net, learning_rate, weight_decay, momentum)\n","    cost_function = get_cost_function()\n","\n","    # Run a single test step beforehand and print metrics\n","    print(\"Before training:\")\n","    train_loss, train_accuracy = test(net, train_loader, cost_function, device=device)\n","    val_loss, val_accuracy = test(net, val_loader, cost_function, device=device)\n","    test_loss, test_accuracy = test(net, test_loader, cost_function, device=device)\n","    print(f\"\\tTraining loss {train_loss:.5f}, Training accuracy {train_accuracy:.2f}\")\n","    print(f\"\\tValidation loss {val_loss:.5f}, Validation accuracy {val_accuracy:.2f}\")\n","    print(f\"\\tTest loss {test_loss:.5f}, Test accuracy {test_accuracy:.2f}\")\n","\n","    # Add values to plots\n","    writer.add_scalar(\"train/loss\", train_loss, 0)\n","    writer.add_scalar(\"train/accuracy\", train_accuracy, 0)\n","    writer.add_scalar(\"val/loss\", val_loss, 0)\n","    writer.add_scalar(\"val/accuracy\", val_accuracy, 0)\n","    writer.add_scalar(\"test/loss\", test_loss, 0)\n","    writer.add_scalar(\"test/accuracy\", test_accuracy, 0)\n","\n","    # Iterate over the number of epochs\n","    pbar = tqdm(range(epochs), desc=\"Training\")\n","    for e in pbar:\n","        # Train and log\n","        train_loss, train_accuracy = train_one_epoch(net, train_loader, optimizer, cost_function, device=device)\n","        val_loss, val_accuracy = test(net, val_loader, cost_function, device=device)\n","        # print(f\"Epoch: {e + 1:d}\")\n","        # print(f\"\\tTraining loss {train_loss:.5f}, Training accuracy {train_accuracy:.2f}\")\n","        # print(f\"\\tVal loss {val_loss:.5f}, Val accuracy {val_accuracy:.2f}\")\n","        # print(\"-----------------------------------------------------\")\n","\n","        # Add values to plots\n","        writer.add_scalar(\"train/loss\", train_loss, e + 1)\n","        writer.add_scalar(\"train/accuracy\", train_accuracy, e + 1)\n","        writer.add_scalar(\"val/loss\", val_loss, e + 1)\n","        writer.add_scalar(\"val/accuracy\", val_accuracy, e + 1)\n","\n","        pbar.set_postfix(train_loss=train_loss, train_accuracy=train_accuracy, val_loss=val_loss, val_accuracy=val_accuracy)\n","\n","    # Compute and print final metrics\n","    print(\"After training:\")\n","    train_loss, train_accuracy = test(net, train_loader, cost_function, device=device)\n","    val_loss, val_accuracy = test(net, val_loader, cost_function, device=device)\n","    test_loss, test_accuracy = test(net, test_loader, cost_function, device=device)\n","\n","    print(f\"\\tTraining loss {train_loss:.5f}, Training accuracy {train_accuracy:.2f}\")\n","    print(f\"\\tValidation loss {val_loss:.5f}, Validation accuracy {val_accuracy:.2f}\")\n","    print(f\"\\tTest loss {test_loss:.5f}, Test accuracy {test_accuracy:.2f}\")\n","\n","    # Add values to plots\n","    writer.add_scalar(\"train/loss\", train_loss, epochs + 1)\n","    writer.add_scalar(\"train/accuracy\", train_accuracy, epochs + 1)\n","    writer.add_scalar(\"val/loss\", val_loss, epochs + 1)\n","    writer.add_scalar(\"val/accuracy\", val_accuracy, epochs + 1)\n","    writer.add_scalar(\"test/loss\", test_loss, epochs + 1)\n","    writer.add_scalar(\"test/accuracy\", test_accuracy, epochs + 1)\n","\n","    # Close writer\n","    writer.close()"]},{"cell_type":"markdown","metadata":{"id":"wGxHI0uxFCI0"},"source":["## Run!"]},{"cell_type":"markdown","metadata":{"id":"Q4fdehh4jgGC"},"source":["Let's first train on mnist and svhn without BN"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":511784,"status":"ok","timestamp":1713199016084,"user":{"displayName":"Francesco Tonini","userId":"09699855296546747021"},"user_tz":-120},"id":"hWKua57qE79H","outputId":"3dd6c085-9bbf-4fd9-dc30-1d2f6f16938c"},"outputs":[{"name":"stdout","output_type":"stream","text":["Before training:\n","\tTraining loss 0.01799, Training accuracy 11.21\n","\tValidation loss 0.01804, Validation accuracy 11.37\n","\tTest loss 0.01819, Test accuracy 11.35\n"]},{"name":"stderr","output_type":"stream","text":["Training: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 25/25 [00:49<00:00,  1.97s/it, train_accuracy=100, train_loss=9.05e-6, val_accuracy=99, val_loss=0.000398]"]},{"name":"stdout","output_type":"stream","text":["After training:\n"]},{"name":"stderr","output_type":"stream","text":["\n"]},{"name":"stdout","output_type":"stream","text":["\tTraining loss 0.00000, Training accuracy 99.99\n","\tValidation loss 0.00040, Validation accuracy 98.98\n","\tTest loss 0.00036, Test accuracy 99.02\n"]}],"source":["main(\"mnist\", dataset=\"mnist\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":600041,"status":"ok","timestamp":1713199616107,"user":{"displayName":"Francesco Tonini","userId":"09699855296546747021"},"user_tz":-120},"id":"CdoGcuokXtHp","outputId":"c9267ee5-9f58-4a0e-c14e-efbd5d772819"},"outputs":[{"name":"stdout","output_type":"stream","text":["Using downloaded and verified file: ./data/svhn/train_32x32.mat\n","Using downloaded and verified file: ./data/svhn/test_32x32.mat\n","Before training:\n","\tTraining loss 0.01803, Training accuracy 6.31\n","\tValidation loss 0.01812, Validation accuracy 6.50\n","\tTest loss 0.01810, Test accuracy 6.13\n"]},{"name":"stderr","output_type":"stream","text":["Training: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 25/25 [00:59<00:00,  2.39s/it, train_accuracy=96.5, train_loss=0.000863, val_accuracy=88.5, val_loss=0.00466]"]},{"name":"stdout","output_type":"stream","text":["After training:\n"]},{"name":"stderr","output_type":"stream","text":["\n"]},{"name":"stdout","output_type":"stream","text":["\tTraining loss 0.00078, Training accuracy 96.73\n","\tValidation loss 0.00466, Validation accuracy 88.48\n","\tTest loss 0.00521, Test accuracy 87.00\n"]}],"source":["main(\"svhn\", dataset=\"svhn\")"]},{"cell_type":"markdown","metadata":{"id":"aXZS0FX5jm7r"},"source":["... and now with BN"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WpJMYIcljqw-"},"outputs":[],"source":["main(\"mnist_bn\", dataset=\"mnist\", with_bn=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"07ig4dpLjtms"},"outputs":[],"source":["main(\"svhn_bn\", dataset=\"svhn\", with_bn=True)"]},{"cell_type":"markdown","metadata":{"id":"CnwVosZWkd2q"},"source":["Let's visualize the results with TensorBoard"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qn3opAVUkfq6","outputId":"5ae2a5ac-93fb-47fd-c19a-e7ca4373742a"},"outputs":[{"data":{"text/html":["\n","      <iframe id=\"tensorboard-frame-dad40fc26f046a54\" width=\"100%\" height=\"800\" frameborder=\"0\">\n","      </iframe>\n","      <script>\n","        (function() {\n","          const frame = document.getElementById(\"tensorboard-frame-dad40fc26f046a54\");\n","          const url = new URL(\"/\", window.location);\n","          const port = 6006;\n","          if (port) {\n","            url.port = port;\n","          }\n","          frame.src = url;\n","        })();\n","      </script>\n","    "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"}],"source":["%load_ext tensorboard\n","%tensorboard --logdir=runs"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Cse1TQQA2_4K"},"outputs":[],"source":[]}],"metadata":{"accelerator":"TPU","colab":{"gpuType":"V28","provenance":[{"file_id":"17I9n-kFZCUaNYTiMRdbt0dxsBDwNR2en","timestamp":1646048063947}]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.19"}},"nbformat":4,"nbformat_minor":0}