{"cells":[{"cell_type":"markdown","metadata":{"id":"HVQuq23Q9Hyo"},"source":["# Deep Learning Lab #1 - My first Neural Network\n","\n","Congratulation! You survived the first laboratory of the course. After seeing the basics of PyTorch, let us build and train a small neural network for digit recognition. We will use the MNIST and SVHN dataset and a simple MLP to do that."]},{"cell_type":"markdown","metadata":{"id":"TLLNs9dy9pAM"},"source":["As a first step, let's install and import the modules we need. The `torch` module contains all the tools we need to build and train the network, whereas `torchvision` contains several Computer Vision oriented utilities, such as shortcuts to standard benchmarks.\n","\n","We are also importing some additional libraries to help us with visualization and debugging."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qwu1ACXL22Es"},"outputs":[],"source":["!pip install scikit-learn -q"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sIZU13zsxAM0"},"outputs":[],"source":["%matplotlib inline\n","\n","import torch\n","from torch.utils.tensorboard import SummaryWriter\n","import torchvision\n","import matplotlib.pyplot as plt\n","import random"]},{"cell_type":"markdown","metadata":{"id":"ykdTcp1M99JE"},"source":["## Step #1: Get the datasets\n","PyTorch provides useful utilities to efficiently load training, testing, and evaluation data, namely the `Dataset` and `Dataloader` modules. The former implements all the functionalities needed to load the dataset in the desired format, while the latter provides the corresponding iteration utilities. PyTorch provides an implemented dataset for [MNIST](https://pytorch.org/vision/stable/generated/torchvision.datasets.MNIST.html#torchvision.datasets.MNIST) and [SVHN](https://pytorch.org/vision/stable/generated/torchvision.datasets.SVHN.html#torchvision.datasets.SVHN); for the Dataloader, we can use the [default implementation](https://pytorch.org/docs/stable/data.html?highlight=dataloader#torch.utils.data.DataLoader)."]},{"cell_type":"markdown","metadata":{"id":"t7XPN6UBO7-1"},"source":["First, let's explore a bit the dataset. To do that, we will initialize an instance of the MNIST dataset and plot a few samples."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7GF6SD7zO2is"},"outputs":[],"source":["# Load the training split of MNIST\n","dataset = torchvision.datasets.MNIST('./data', train=True, transform=None, download=True)\n","\n","# Let's then gather some info about a sample\n","image, label = dataset[0]\n","print(f\"Image has size: {image.size}\")\n","print(f\"Image tensor has shape: {torchvision.transforms.functional.to_tensor(image).shape}\")\n","print(f\"Label is {label}\")\n","\n","# Visualize some samples in a 3x3 matrix\n","fig, axs = plt.subplots(3, 3)\n","fig.suptitle(\"MNIST\")\n","for idx, ax in enumerate(axs.flatten()):\n","    # Let's take one image randomly\n","    image, label = dataset[random.randint(0, len(dataset))]\n","\n","    ax.axis(\"off\")\n","    im = ax.imshow(image, cmap=\"gray\")\n","    ax.set_title(f\"Class: {label}\")\n","    plt.colorbar(im, ax=ax)\n","\n","plt.tight_layout()\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zS6_c7cr22Ev"},"outputs":[],"source":["# Load the training split of SVHN\n","dataset = torchvision.datasets.SVHN('./data', split=\"train\", transform=None, download=True)\n","\n","# Let's then gather some info about a sample\n","image, label = dataset[0]\n","print(f\"Image has size: {image.size}\")\n","print(f\"Image tensor has shape: {torchvision.transforms.functional.to_tensor(image).shape}\")\n","print(f\"Label is {label}\")\n","\n","# Visualize some samples in a 3x3 matrix\n","fig, axs = plt.subplots(3, 3)\n","fig.suptitle(\"SVHN\")\n","for idx, ax in enumerate(axs.flatten()):\n","    # Let's take one image randomly\n","    image, label = dataset[random.randint(0, len(dataset))]\n","\n","    ax.axis(\"off\")\n","    im = ax.imshow(image, cmap=\"gray\")\n","    ax.set_title(f\"Class: {label}\")\n","    plt.colorbar(im, ax=ax)\n","\n","plt.tight_layout()\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"pDWnKm9_22Ev"},"source":["Let's now define a function that allow us to:\n","- (a) download the dataset\n","- (b) split the dataset into chunks, if needed\n","- (c) return the DataLoaders to feed the NN"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jtyBrLjJxNNm"},"outputs":[],"source":["def get_mnist(batch_size, transforms, val_split=0.2):\n","    # Load datasets\n","    full_training_data = torchvision.datasets.MNIST('./data', train=True, transform=transforms, download=True)\n","    test_data = torchvision.datasets.MNIST('./data', train=False, transform=transforms, download=True)\n","\n","    # Create train and validation splits\n","    num_samples = len(full_training_data)\n","    training_samples = int(num_samples * (1 - val_split) + 1)\n","    validation_samples = num_samples - training_samples\n","\n","    training_data, validation_data = torch.utils.data.random_split(full_training_data, [training_samples, validation_samples])\n","\n","    # Print some stats\n","    print(f\"# of training samples: {len(training_data)}\")\n","    print(f\"# of validation samples: {len(validation_data)}\")\n","    print(f\"# of test samples: {len(test_data)}\")\n","\n","    # Initialize dataloaders\n","    train_loader = torch.utils.data.DataLoader(training_data, batch_size, shuffle=True)\n","    val_loader = torch.utils.data.DataLoader(validation_data, batch_size, shuffle=False)\n","    test_loader = torch.utils.data.DataLoader(test_data, batch_size, shuffle=False)\n","\n","    return train_loader, val_loader, test_loader\n","\n","def get_svhn(batch_size, transforms, val_split=0.2):\n","    full_training_data = torchvision.datasets.SVHN('./data', split=\"train\", transform=transforms, download=True)\n","    test_data = torchvision.datasets.SVHN('./data', split=\"test\", transform=transforms, download=True)\n","\n","    # Create train and validation splits\n","    num_samples = len(full_training_data)\n","    training_samples = int(num_samples * (1 - val_split) + 1)\n","    validation_samples = num_samples - training_samples\n","\n","    training_data, validation_data = torch.utils.data.random_split(full_training_data, [training_samples, validation_samples])\n","\n","    # Print some stats\n","    print(f\"# of training samples: {len(training_data)}\")\n","    print(f\"# of validation samples: {len(validation_data)}\")\n","    print(f\"# of test samples: {len(test_data)}\")\n","\n","    # Initialize dataloaders\n","    train_loader = torch.utils.data.DataLoader(training_data, batch_size, shuffle=True)\n","    val_loader = torch.utils.data.DataLoader(validation_data, batch_size, shuffle=False)\n","    test_loader = torch.utils.data.DataLoader(test_data, batch_size, shuffle=False)\n","\n","    return train_loader, val_loader, test_loader"]},{"cell_type":"markdown","metadata":{"id":"obE13C39_2g1"},"source":["## Step #2: Build the neural network\n","We now have to define the architecture of our MLP, which consists of two fully connected linear layers. Luckily, we don't need to implement them by hand like in the first lab, because PyTorch has already done it for us. We also need to include an activation function between the two layers, e.g., Sigmoid (`torch.nn.Sigmoid`)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JO_my8CC0Uhn"},"outputs":[],"source":["class MyFirstNetwork(torch.nn.Module):\n","    def __init__(self, input_dim, hidden_dim, output_dim):\n","        super(MyFirstNetwork, self).__init__()\n","\n","        # Input linear layer\n","        self.input_to_hidden = torch.nn.Linear(input_dim, hidden_dim)\n","\n","        # Output linear layer\n","        self.hidden_to_output = torch.nn.Linear(hidden_dim, output_dim)\n","\n","        # Activation function\n","        self.activation = torch.nn.Sigmoid()\n","\n","    def forward(self, x):\n","        # x.shape is [batch_size, height, width].\n","        # To forward x through the network, we need to flatten the height and width\n","        # NOTE: this is not always required, e.g. CNNs take as input the 2D/3D pixel matrix directly\n","\n","        x = x.reshape(x.shape[0], -1)\n","        x = self.input_to_hidden(x)\n","        x = self.activation(x)\n","        x = self.hidden_to_output(x)\n","\n","        return x"]},{"cell_type":"markdown","metadata":{"id":"qV2v8x0ZBG8y"},"source":["## Step #3: Define the optimization algorithm\n","The optimizer is the tool that actually carries out the optimization of the parameters with respect to the chosen loss function. There are a variety of implemented optimizers in the [`torch.optim`](https://pytorch.org/docs/stable/optim.html) module. Let's define our optimizer, giving as input the network parameters, the learning rate, the weight decay coefficient, and the momentum."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nQmB7Kir8Y56"},"outputs":[],"source":["def get_optimizer(net, lr, wd, momentum):\n","    optimizer = torch.optim.SGD(net.parameters(), lr=lr, weight_decay=wd, momentum=momentum)\n","    return optimizer"]},{"cell_type":"markdown","metadata":{"id":"Az-K8L5oBu3F"},"source":["## Step #4: Define the loss function\n","The loss/cost function expresses the value that you wish to minimize by optimizing the parameters of your network. In other words, it should efficiently express the prediction error. Given that we are addressing a multi-class classification task, a suitable choice is a cross-entropy with softmax. This is available, along with many alternatives, in the [`torch.nn`](https://pytorch.org/docs/stable/nn.html#loss-functions) module. Note that `torch.nn.CrossEntropyLoss` already applies the softmax function, i.e. we don't need to manually define it."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"j1CzNOKhCe3h"},"outputs":[],"source":["def get_loss_function():\n","    loss_function = torch.nn.CrossEntropyLoss()\n","    return loss_function"]},{"cell_type":"markdown","metadata":{"id":"APuCISvfCrP6"},"source":["## Step #5: Define the training and test loops\n","We are ready to define our training and test loops. These should be two separate functions which:\n","1.   **iterate** over a given set of data\n","2.   **forward** the data through the neural network\n","3.   **compare** the network output with the ground truth labels to compute the loss and/or evaluation metrics\n","\n","Additionally, inside the training loop, we need these steps to actually carry out the optimization\n","1.   perform the backward pass (`loss.backward()`) to **compute gradients**\n","2.   call the optimizer to consequently **update the weights** (`optimizer.step()`)\n","3.   **reset** the gradients in order not to accumulate it (`optimizer.zero_grad()`)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ur0qy-JBChFy"},"outputs":[],"source":["def training_step(net, data_loader, optimizer, cost_function, device=\"cuda\"):\n","    samples = 0.0\n","    cumulative_loss = 0.0\n","    cumulative_accuracy = 0.0\n","\n","    # Set the network to training mode\n","    net.train()\n","\n","    # Iterate over the training set\n","    for batch_idx, (inputs, targets) in enumerate(data_loader):\n","        # Load data into GPU\n","        inputs = inputs.to(device)\n","        targets = targets.to(device)\n","\n","        # Forward pass\n","        outputs = net(inputs)\n","\n","        # Loss computation\n","        loss = cost_function(outputs, targets)\n","\n","        # Backward pass\n","        loss.backward()\n","\n","        # Parameters update\n","        optimizer.step()\n","\n","        # Gradients reset\n","        optimizer.zero_grad()\n","\n","        # Fetch prediction and loss value\n","        samples += inputs.shape[0]\n","        cumulative_loss += loss.item()\n","        _, predicted = outputs.max(dim=1) # max() returns (maximum_value, index_of_maximum_value)\n","\n","        # Compute training accuracy\n","        cumulative_accuracy += predicted.eq(targets).sum().item()\n","\n","    return cumulative_loss / samples, cumulative_accuracy / samples * 100\n","\n","def test_step(net, data_loader, cost_function, device=\"cuda\"):\n","    samples = 0.\n","    cumulative_loss = 0.\n","    cumulative_accuracy = 0.\n","\n","    # Set the network to evaluation mode\n","    net.eval()\n","\n","    # Disable gradient computation (we are only testing, we do not want our model to be modified in this step!)\n","    with torch.no_grad():\n","        # Iterate over the test set\n","        for batch_idx, (inputs, targets) in enumerate(data_loader):\n","            # Load data into GPU\n","            inputs = inputs.to(device)\n","            targets = targets.to(device)\n","\n","            # Forward pass\n","            outputs = net(inputs)\n","\n","            # Loss computation\n","            loss = cost_function(outputs, targets)\n","\n","            # Fetch prediction and loss value\n","            samples += inputs.shape[0]\n","            cumulative_loss += loss.item() # Note: the .item() is needed to extract scalars from tensors\n","            _, predicted = outputs.max(1)\n","\n","            # Compute accuracy\n","            cumulative_accuracy += predicted.eq(targets).sum().item()\n","\n","    return cumulative_loss / samples, cumulative_accuracy / samples * 100"]},{"cell_type":"markdown","metadata":{"id":"wGxHI0uxFCI0"},"source":["## Put it all together!\n","We need a compact procedure to apply all the components and functions defined so far into the actual optimization procedure. In particular, we want our model to iterate over the training step and test step for multiple epochs, tracking the partial results."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hWKua57qE79H"},"outputs":[],"source":["# Tensorboard logging utilities\n","def log_values(writer, step, loss, accuracy, prefix):\n","  writer.add_scalar(f\"{prefix}/loss\", loss, step)\n","  writer.add_scalar(f\"{prefix}/accuracy\", accuracy, step)\n","\n","# Main function\n","def main(\n","    batch_size=128,\n","    input_dim=28*28,\n","    hidden_dim=100,\n","    output_dim=10,\n","    device='cuda:0',\n","    learning_rate=0.0001,\n","    weight_decay=0.0000001,\n","    momentum=0.9,\n","    epochs=10,\n","    exp_name=\"exp1\"\n","):\n","    # Create a logger for the experiment\n","    writer = SummaryWriter(log_dir=f\"runs/{exp_name}\")\n","\n","    # Create data transforms\n","    # Images in the dataset are stored as PIL objects. We need to convert it to a tensor.\n","    transforms = torchvision.transforms.Compose([\n","        torchvision.transforms.ToTensor()\n","    ])\n","\n","    # Get dataloaders\n","    train_loader, val_loader, test_loader = get_mnist(batch_size, transforms)\n","    # train_loader, val_loader, test_loader = get_svhn(batch_size, transforms)\n","\n","    # Instantiate the network and move it to the chosen device (GPU)\n","    net = MyFirstNetwork(input_dim, hidden_dim, output_dim).to(device)\n","\n","    # Let's \"print\" the network to view all the modules\n","    print(net)\n","\n","    # Instantiate the optimizer\n","    optimizer = get_optimizer(net, learning_rate, weight_decay, momentum)\n","\n","    # Define the cost function\n","    loss_function = get_loss_function()\n","\n","    # Computes evaluation results before training\n","    print(\"Before training:\")\n","    train_loss, train_accuracy = test_step(net, train_loader, loss_function)\n","    val_loss, val_accuracy = test_step(net, val_loader, loss_function)\n","    test_loss, test_accuracy = test_step(net, test_loader, loss_function)\n","\n","    # Log to TensorBoard\n","    log_values(writer, -1, train_loss, train_accuracy, \"Train\")\n","    log_values(writer, -1, val_loss, val_accuracy, \"Validation\")\n","    log_values(writer, -1, test_loss, test_accuracy, \"Test\")\n","\n","    print(f\"\\tTraining loss {train_loss:.5f}, Training accuracy {train_accuracy:.2f}\")\n","    print(f\"\\tValidation loss {val_loss:.5f}, Validation accuracy {val_accuracy:.2f}\")\n","    print(f\"\\tTest loss {test_loss:.5f}, Test accuracy {test_accuracy:.2f}\")\n","    print(\"-----------------------------------------------------\")\n","\n","    # For each epoch, train the network and then compute evaluation results\n","    for e in range(epochs):\n","        train_loss, train_accuracy = training_step(net, train_loader, optimizer, loss_function)\n","        val_loss, val_accuracy = test_step(net, val_loader, loss_function)\n","\n","        # Logs to TensorBoard\n","        log_values(writer, e, train_loss, train_accuracy, \"Train\")\n","        log_values(writer, e, val_loss, val_accuracy, \"Validation\")\n","\n","        print(f\"Epoch: {e + 1}\")\n","        print(f\"\\tTraining loss {train_loss:.5f}, Training accuracy {train_accuracy:.2f}\")\n","        print(f\"\\tValidation loss {val_loss:.5f}, Validation accuracy {val_accuracy:.2f}\")\n","        print(\"-----------------------------------------------------\")\n","\n","    # Compute final evaluation results\n","    print(\"After training:\")\n","    train_loss, train_accuracy = test_step(net, train_loader, loss_function)\n","    val_loss, val_accuracy = test_step(net, val_loader, loss_function)\n","    test_loss, test_accuracy = test_step(net, test_loader, loss_function)\n","\n","    # Log to TensorBoard\n","    log_values(writer, epochs + 1, train_loss, train_accuracy, \"Train\")\n","    log_values(writer, epochs + 1, val_loss, val_accuracy, \"Validation\")\n","    log_values(writer, epochs + 1, test_loss, test_accuracy, \"Test\")\n","\n","    print(f\"\\tTraining loss {train_loss:.5f}, Training accuracy {train_accuracy:.2f}\")\n","    print(f\"\\tValidation loss {val_loss:.5f}, Validation accuracy {val_accuracy:.2f}\")\n","    print(f\"\\tTest loss {test_loss:.5f}, Test accuracy {test_accuracy:.2f}\")\n","    print(\"-----------------------------------------------------\")\n","\n","    # Closes the logger\n","    writer.close()\n","\n","    # Let's return the net\n","    return net"]},{"cell_type":"markdown","metadata":{"id":"jJzgp_pOF-GM"},"source":["## Run it!\n","Let's run our model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AwVJu7teF5YP"},"outputs":[],"source":["!rm -r runs"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mqJ1opqbGDcY"},"outputs":[],"source":["net = main(exp_name=\"hello_training\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SdfqDsdMr5MS"},"outputs":[],"source":["# Let's check the results on tensorboard\n","%load_ext tensorboard\n","%tensorboard --logdir=runs"]},{"cell_type":"markdown","metadata":{"id":"mf7OFpZh22Ez"},"source":["## Visualize predictions\n","\n","Let us now take the trained model and visualize the predictions w.r.t. ground truth."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QDwcKXcR22Ez"},"outputs":[],"source":["rows, cols = 3, 3\n","device = \"cuda:0\"\n","transforms = torchvision.transforms.Compose([\n","    torchvision.transforms.ToTensor()\n","])\n","_, _, test_loader = get_mnist(rows * cols, transforms)\n","\n","# Load data\n","inputs, targets = next(iter(test_loader))\n","inputs = inputs.to(device)\n","targets = targets.to(device)\n","\n","# Forward pass\n","outputs = net(inputs)\n","preds = outputs.argmax(dim=1)\n","\n","# Visualize some samples in a matrix\n","fig, axs = plt.subplots(rows, cols)\n","fig.suptitle(\"MNIST\")\n","for idx, ax in enumerate(axs.flatten()):\n","    image, label = inputs[idx], targets[idx]\n","    pred = preds[idx]\n","\n","    if pred != label:\n","        ax.title.set_color(\"red\")\n","    else:\n","        ax.title.set_color(\"green\")\n","\n","    ax.axis(\"off\")\n","    im = ax.imshow(torchvision.transforms.functional.to_pil_image(image), cmap=\"gray\")\n","    ax.set_title(f\"Pred: {pred.item()}\")\n","    plt.colorbar(im, ax=ax)\n","\n","plt.tight_layout()\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"7ZkLPmx522E0"},"source":["---"]},{"cell_type":"markdown","metadata":{"id":"1oskjnB722E0"},"source":["# Are MLPs all we need?\n","\n","The neural network we trained achieved good results on digit recognition, but is it actually good? Let's test it out! Let's create a new version of the MNIST dataset where the digits are slightly shifted from the center of the image. What do you think will happen?"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nnwDYd0b22E0"},"outputs":[],"source":["# Let's create function that pads the original input so that the digit is not centered in the image\n","def padding(x):\n","    pad_size = 28\n","    left_padding = torch.randint(low=0, high=pad_size, size=(1,))\n","    top_padding = torch.randint(low=0, high=pad_size, size=(1,))\n","    return torch.nn.functional.pad(x, (left_padding,  pad_size - left_padding,  top_padding,  pad_size - top_padding), \"constant\", 0)\n","\n","transforms_padded = torchvision.transforms.Compose([\n","    torchvision.transforms.ToTensor(),\n","    torchvision.transforms.Lambda(lambda x: padding(x)),\n","])\n","transforms = torchvision.transforms.Compose([\n","    torchvision.transforms.ToTensor(),\n","    torchvision.transforms.Lambda(lambda x: torch.nn.functional.pad(x, (14, 14, 14, 14), \"constant\", 0))\n","])\n","\n","# Let's create both normal and padded datasets, and view their differences\n","rows, cols = 3, 3\n","_, _, test_loader = get_mnist(rows * cols, transforms)\n","_, _, test_loader_padded = get_mnist(rows * cols, transforms_padded)\n","\n","for idx, (inputs, targets) in enumerate([next(iter(test_loader)), next(iter(test_loader_padded))]):\n","    fig, axs = plt.subplots(rows, cols)\n","    fig.suptitle(f\"MNIST {'padded' if idx else ''}\")\n","    for idx, ax in enumerate(axs.flatten()):\n","        image, label = inputs[idx], targets[idx]\n","\n","        ax.axis(\"off\")\n","        im = ax.imshow(torchvision.transforms.functional.to_pil_image(image), cmap=\"gray\")\n","        plt.colorbar(im, ax=ax)\n","\n","    plt.tight_layout()\n","    plt.show()"]},{"cell_type":"markdown","metadata":{"id":"FEcxF0n822E1"},"source":["Let us now train the same MLP of before with the new dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9QZ82Rgc22E1"},"outputs":[],"source":["# Let's slightly change the main function above\n","def main_pad(\n","    batch_size=128,\n","    input_dim=56*56,\n","    hidden_dim=100,\n","    output_dim=10,\n","    device='cuda:0',\n","    learning_rate=0.0001,\n","    weight_decay=0.0000001,\n","    momentum=0.9,\n","    epochs=10,\n","    exp_name=\"exp1_pad\"\n","):\n","    # Create a logger for the experiment\n","    writer = SummaryWriter(log_dir=f\"runs/{exp_name}\")\n","\n","    # Create data transforms\n","    # Images in the dataset are stored as PIL objects. We need to convert it to a tensor.\n","    transforms_padded = torchvision.transforms.Compose([\n","        torchvision.transforms.ToTensor(),\n","        torchvision.transforms.Lambda(lambda x: padding(x)),\n","    ])\n","\n","    # Get dataloaders\n","    train_loader, val_loader, test_loader = get_mnist(batch_size, transforms_padded)\n","\n","    # Instantiate the network and move it to the chosen device (GPU)\n","    net = MyFirstNetwork(input_dim, hidden_dim, output_dim).to(device)\n","\n","    # Let's \"print\" the network to view all the modules\n","    print(net)\n","\n","    # Instantiate the optimizer\n","    optimizer = get_optimizer(net, learning_rate, weight_decay, momentum)\n","\n","    # Define the cost function\n","    loss_function = get_loss_function()\n","\n","    # Computes evaluation results before training\n","    print(\"Before training:\")\n","    train_loss, train_accuracy = test_step(net, train_loader, loss_function)\n","    val_loss, val_accuracy = test_step(net, val_loader, loss_function)\n","    test_loss, test_accuracy = test_step(net, test_loader, loss_function)\n","\n","    # Log to TensorBoard\n","    log_values(writer, -1, train_loss, train_accuracy, \"Train\")\n","    log_values(writer, -1, val_loss, val_accuracy, \"Validation\")\n","    log_values(writer, -1, test_loss, test_accuracy, \"Test\")\n","\n","    print(f\"\\tTraining loss {train_loss:.5f}, Training accuracy {train_accuracy:.2f}\")\n","    print(f\"\\tValidation loss {val_loss:.5f}, Validation accuracy {val_accuracy:.2f}\")\n","    print(f\"\\tTest loss {test_loss:.5f}, Test accuracy {test_accuracy:.2f}\")\n","    print(\"-----------------------------------------------------\")\n","\n","    # For each epoch, train the network and then compute evaluation results\n","    for e in range(epochs):\n","        train_loss, train_accuracy = training_step(net, train_loader, optimizer, loss_function)\n","        val_loss, val_accuracy = test_step(net, val_loader, loss_function)\n","\n","        # Logs to TensorBoard\n","        log_values(writer, e, train_loss, train_accuracy, \"Train\")\n","        log_values(writer, e, val_loss, val_accuracy, \"Validation\")\n","\n","        print(f\"Epoch: {e + 1}\")\n","        print(f\"\\tTraining loss {train_loss:.5f}, Training accuracy {train_accuracy:.2f}\")\n","        print(f\"\\tValidation loss {val_loss:.5f}, Validation accuracy {val_accuracy:.2f}\")\n","        print(\"-----------------------------------------------------\")\n","\n","    # Compute final evaluation results\n","    print(\"After training:\")\n","    train_loss, train_accuracy = test_step(net, train_loader, loss_function)\n","    val_loss, val_accuracy = test_step(net, val_loader, loss_function)\n","    test_loss, test_accuracy = test_step(net, test_loader, loss_function)\n","\n","    # Log to TensorBoard\n","    log_values(writer, epochs + 1, train_loss, train_accuracy, \"Train\")\n","    log_values(writer, epochs + 1, val_loss, val_accuracy, \"Validation\")\n","    log_values(writer, epochs + 1, test_loss, test_accuracy, \"Test\")\n","\n","    print(f\"\\tTraining loss {train_loss:.5f}, Training accuracy {train_accuracy:.2f}\")\n","    print(f\"\\tValidation loss {val_loss:.5f}, Validation accuracy {val_accuracy:.2f}\")\n","    print(f\"\\tTest loss {test_loss:.5f}, Test accuracy {test_accuracy:.2f}\")\n","    print(\"-----------------------------------------------------\")\n","\n","    # Closes the logger\n","    writer.close()\n","\n","    # Let's return the net\n","    return net"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GiFJ2ytF22E1"},"outputs":[],"source":["net_padded = main_pad(exp_name=\"hello_pad\")"]},{"cell_type":"markdown","metadata":{"id":"ra9Yi4pe22E1"},"source":["Finally, let's visualize the predictions of this network"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qKSFJoWZ22E2"},"outputs":[],"source":["rows, cols = 3, 3\n","device = \"cuda:0\"\n","transforms_padded = torchvision.transforms.Compose([\n","    torchvision.transforms.ToTensor(),\n","    torchvision.transforms.Lambda(lambda x: padding(x)),\n","])\n","_, _, test_loader = get_mnist(rows * cols, transforms_padded)\n","\n","# Load data\n","inputs, targets = next(iter(test_loader))\n","inputs = inputs.to(device)\n","targets = targets.to(device)\n","\n","# Forward pass\n","outputs = net_padded(inputs)\n","preds = outputs.argmax(dim=1)\n","\n","# Visualize some samples in a matrix\n","fig, axs = plt.subplots(rows, cols)\n","fig.suptitle(\"MNIST\")\n","for idx, ax in enumerate(axs.flatten()):\n","    image, label = inputs[idx], targets[idx]\n","    pred = preds[idx]\n","\n","    if pred != label:\n","        ax.title.set_color(\"red\")\n","    else:\n","        ax.title.set_color(\"green\")\n","\n","    ax.axis(\"off\")\n","    im = ax.imshow(torchvision.transforms.functional.to_pil_image(image), cmap=\"gray\")\n","    ax.set_title(f\"Pred: {pred.item()}\")\n","    plt.colorbar(im, ax=ax)\n","\n","plt.tight_layout()\n","plt.show()"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.18"}},"nbformat":4,"nbformat_minor":0}