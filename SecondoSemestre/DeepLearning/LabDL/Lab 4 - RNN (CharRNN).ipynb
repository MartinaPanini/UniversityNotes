{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z2_IK6G_iUrZ"
   },
   "source": [
    "# Recurrent Neural Networks\n",
    "\n",
    "Recurrent Neural Networks (RNN) are a family of models designed in order to model sequences of data (e.g. video, text). In this tutorial (adapted from [here](https://pytorch.org/tutorials/intermediate/char_rnn_classification_tutorial.html)), we will see how to **predict the language of a name** using an RNN model, taking single word characters as input.\n",
    "\n",
    "Specifically, we will train the network on a list of surnames from 18 languages of origin, and predict which language a name is from based on the spelling:\n",
    "\n",
    "```\n",
    "$ python predict.py Hinton\n",
    "(0.63) Scottish\n",
    "(0.22) English\n",
    "(0.02) Irish\n",
    "\n",
    "$ python predict.py Schmidhuber\n",
    "(0.83) German\n",
    "(0.08) Czech\n",
    "(0.07) Dutch\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E-W8XGEaNEwM"
   },
   "source": [
    "## Downloading and preparing the data\n",
    "\n",
    "Let's start by downloading the zipped data provided in the original tutorial, and let's then extract it in our environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "28yTDzpPM-RA"
   },
   "outputs": [],
   "source": [
    "!wget https://download.pytorch.org/tutorial/data.zip\n",
    "!unzip data.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_KRCjjpWNYmV"
   },
   "source": [
    "In the folder we can find 18 text files named as \"[Language].txt\". Each file contains a series of names, one name per line. In the following, we will take care of data preprocessing by:\n",
    "\n",
    "* extracting names and numbers of categories from the files\n",
    "* converting each name from Unicode to ASCII encoding\n",
    "* defining a dictionary containing all names (values) of a given language (key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lkZHGCodNTW3"
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import unicodedata\n",
    "import string\n",
    "import random\n",
    "\n",
    "all_filenames = glob.glob(\"data/names/*.txt\")\n",
    "all_letters = string.ascii_letters + \" .,;'\"\n",
    "n_letters = len(all_letters)\n",
    "\n",
    "# Turn a Unicode string to plain ASCII, thanks to http://stackoverflow.com/a/518232/2809427\n",
    "def unicode_to_ascii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize(\"NFD\", s)\n",
    "        if unicodedata.category(c) != \"Mn\"\n",
    "        and c in all_letters\n",
    "    )\n",
    "\n",
    "# Build the category_lines dictionary: keys are the languages, and values are list of names for that language\n",
    "category_lines = {}\n",
    "all_categories = []\n",
    "\n",
    "# Read a file and split into lines\n",
    "def readLines(filename):\n",
    "    lines = open(filename).read().strip().split(\"\\n\")\n",
    "    return [unicode_to_ascii(line) for line in lines]\n",
    "\n",
    "for filename in all_filenames:\n",
    "    # Extract the name of the language\n",
    "    category = filename.split(\"/\")[-1].split(\".\")[0]\n",
    "\n",
    "    # Read the names of that language\n",
    "    lines = readLines(filename)\n",
    "\n",
    "    # Append to the list and add to the dictionary\n",
    "    all_categories.append(category)\n",
    "    category_lines[category] = lines\n",
    "\n",
    "unicode_str = \"Ślusàrski\"\n",
    "print(f\"{unicode_str} => {unicode_to_ascii(unicode_str)}\")\n",
    "    \n",
    "n_categories = len(all_categories)\n",
    "print(f\"Num. of categories (i.e. languages) = {n_categories}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m_kUmnhGOJVs"
   },
   "source": [
    "## Encoding words into Tensors\n",
    "\n",
    "A crucial issue in this task lies in how how to define the input of the network. Since the network treats numbers - and not plain text - we must **convert text to numerical representation**. With this purpose, we represent each letter as a **one-hot vector** of size `(1, n_letters)`. A one-hot vector is filled with 0s, except for a 1 at the index of the current letter, e.g. `\"b\" = <0 1 0 0 0 ...>`.\n",
    "\n",
    "In order to build a word, we join these character representations into a 2D matrix `(line_length, 1, n_letters)`.\n",
    "\n",
    "That extra 1 dimension is due to the fact that PyTorch assumes the input to be divided in batches: we're just using a batch size of 1 here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AEGRVRWROD9l"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Just for demonstration, encode a character into a (1, n_letters) tensor\n",
    "def letter_to_tensor(letter):\n",
    "    tensor = torch.zeros(1, n_letters)\n",
    "    letter_index = all_letters.find(letter)\n",
    "    tensor[0, letter_index] = 1\n",
    "\n",
    "    return tensor\n",
    "\n",
    "# Encode a line into a (line_length, n_letters) one-hot tensor, (or (line_length, 1, n_letters) if the batch dimension is added) a line is a sequence of characters\n",
    "def line_to_tensor(line, add_batch_dimension=True):\n",
    "    tensor = torch.zeros(len(line), n_letters)\n",
    "    for line_index, letter in enumerate(line):\n",
    "        letter_index = all_letters.find(letter)\n",
    "        tensor[line_index, letter_index] = 1\n",
    "\n",
    "    if add_batch_dimension:\n",
    "        tensor = tensor.unsqueeze(1)\n",
    "\n",
    "    return tensor\n",
    "\n",
    "# Create a batch of samples given a list of lines that is, a list of character sequences\n",
    "def create_batch(lines):\n",
    "    tensors = []\n",
    "    for current_line in lines:\n",
    "        # Current_line_tensor is (line_length, n_letters)\n",
    "        current_line_tensor = line_to_tensor(current_line, add_batch_dimension=False)\n",
    "        tensors.append(current_line_tensor)\n",
    "\n",
    "    # Since each line_tensor may have a different line_length, we pad each\n",
    "    # line to the length of the longest sequence\n",
    "    # Padded_tensor is (max_line_length, batch_size, n_letters)\n",
    "    padded_tensor = torch.nn.utils.rnn.pad_sequence(tensors, batch_first=False, padding_value=0)\n",
    "\n",
    "    return padded_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zP6MXSkWPTEP"
   },
   "source": [
    "## Building the Network\n",
    "\n",
    "We want to define a simple recurrent neural network. The newtork should have a recurrent layer followed by a fully connected layer mapping the features of the recurrent unit to the output space (i.e. number of categories).\n",
    "\n",
    "To run a step of this network, we need to provide an input (in our case, the tensor for the current sequence/s) and a previous hidden state (which we initialize as zeros at first). We'll get back the logits (i.e. network activation before the softmax) for each each language.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ksAuhgIHPQNO"
   },
   "outputs": [],
   "source": [
    "# Create a simple recurrent network\n",
    "class SimpleRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(SimpleRNN, self).__init__()\n",
    "\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "\n",
    "        self.i2h = nn.RNN(input_size, hidden_size)\n",
    "        self.i2o = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    # Forward the whole sequence at once\n",
    "    def forward(self, input, hidden=None):\n",
    "        if hidden==None:\n",
    "            hidden = self.init_hidden(input.shape[1])\n",
    "\n",
    "        output, _ = self.i2h(input, hidden)\n",
    "        # only the features extracted at the end of the sequence are used\n",
    "        # to produce the output\n",
    "        output = self.i2o(output[-1])\n",
    "\n",
    "        return output\n",
    "\n",
    "    # Instantiate the hidden state of the first element of the sequence dim: 1 x batch_size x hidden_size)\n",
    "    def init_hidden(self,shape=1):\n",
    "        return torch.zeros(1, shape, self.hidden_size)\n",
    "\n",
    "# Create a simple LSTM network\n",
    "class SimpleLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(SimpleLSTM, self).__init__()\n",
    "\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "\n",
    "        self.i2h = nn.LSTM(input_size, hidden_size)\n",
    "        self.i2o = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, input, hidden=None, cell=None):\n",
    "        if hidden==None:\n",
    "            hidden = self.init_hidden(input.shape[1])\n",
    "\n",
    "        if cell==None:\n",
    "            cell = self.init_hidden(input.shape[1])\n",
    "\n",
    "        output, (_,_)= self.i2h(input, (hidden,cell))\n",
    "        # only the features extracted at the end of the sequence are used\n",
    "        # to produce the output\n",
    "        output = self.i2o(output[-1])\n",
    "\n",
    "        return output\n",
    "\n",
    "    def init_hidden(self,shape=1):\n",
    "        return torch.zeros(1, shape, self.hidden_size)\n",
    "\n",
    "    def init_cell(self,shape=1):\n",
    "        return torch.zeros(1, shape, self.hidden_size)\n",
    "\n",
    "# Implement a simple RNN using cells\n",
    "class SimpleRNNwithCell(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(SimpleRNNwithCell, self).__init__()\n",
    "\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "\n",
    "        self.i2h = nn.RNNCell(input_size, hidden_size)\n",
    "        self.i2o = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, input, hidden=None):\n",
    "\n",
    "        if hidden == None:\n",
    "            hidden = self.init_hidden(input.shape[1])\n",
    "\n",
    "        # Manually feed each sequence element to the RNN cell\n",
    "        for i in range(input.shape[0]):\n",
    "            hidden = self.i2h(input[i],hidden)\n",
    "\n",
    "        output = self.i2o(hidden)\n",
    "\n",
    "        return output\n",
    "\n",
    "    def init_hidden(self,shape=1):\n",
    "        return torch.zeros(shape, self.hidden_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CbsIxX8dP_QD"
   },
   "source": [
    "# Preparing for training\n",
    "\n",
    "Before going into training we should make a few helper functions. The first one should interpret the output of the network, which we know to be a logits of each category. We can use `Tensor.topk` to get the index of the greatest value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yor-aotlP35k"
   },
   "outputs": [],
   "source": [
    "def category_from_output(output):\n",
    "    # Returns top_k_values, top_k_indices\n",
    "    top_values, top_idx = output.data.topk(1)\n",
    "    category_idx = top_idx[0][0]  # Gets the index for the top value of the first batch element\n",
    "    return all_categories[category_idx], category_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Za7e14X3QRoE"
   },
   "source": [
    "We want a quick way to get a training example (a name and its language):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3unsDbhFQO5M"
   },
   "outputs": [],
   "source": [
    "def random_training_pair(bs=1):\n",
    "    lines = []\n",
    "    categories = []\n",
    "\n",
    "    # Each batch element is a random line from a random language\n",
    "    for b in range(bs):\n",
    "        category = random.choice(all_categories)\n",
    "        line = random.choice(category_lines[category])\n",
    "\n",
    "        lines.append(line)\n",
    "        categories.append(category)\n",
    "\n",
    "    # Build the ground truth labels\n",
    "    categories_tensor = torch.LongTensor([all_categories.index(c) for c in categories])\n",
    "\n",
    "    # Use our previous helper function to build the batch from a list of sequences of characters\n",
    "    lines_tensor = create_batch(lines)\n",
    "\n",
    "    return categories_tensor, lines_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yw_GE4eEQzID"
   },
   "source": [
    "## Training the network\n",
    "\n",
    "Now all it takes to train this network is showing it a bunch of examples, have it making guesses, and tell it if it's wrong.\n",
    "\n",
    "Since the output of the networks consists of logits - and the task is classification - we can use a standard cross-entropy loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Nshn6EMTQjpq"
   },
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "01HcYRBIQ_E7"
   },
   "source": [
    "Now we instantiate a standard training loop where we will:\n",
    "\n",
    "*   forward the input to the network\n",
    "*   compute the loss\n",
    "*   perform backpropagation\n",
    "*   make a step with the optimizer\n",
    "*   reset the optimizer/network's grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VGSQsBiuQ84E"
   },
   "outputs": [],
   "source": [
    "def train(rnn, optimizer, categories_tensor, lines_tensor):\n",
    "    optimizer.zero_grad()\n",
    "    output = rnn(lines_tensor)\n",
    "\n",
    "    loss = criterion(output, categories_tensor)\n",
    "    loss.backward()\n",
    "\n",
    "    optimizer.step()\n",
    "\n",
    "    return output, loss.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FQyKSa1TRPuE"
   },
   "source": [
    "Now we just have to:\n",
    "* instatiate the network\n",
    "* instatiate the optimizer\n",
    "* run the training step for a given number of iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "um1msplGRN5Z"
   },
   "outputs": [],
   "source": [
    "# Initialize the network:\n",
    "n_hidden = 128\n",
    "rnn = SimpleRNN(n_letters, n_hidden, n_categories)\n",
    "\n",
    "# Initialize the optimizer\n",
    "learning_rate = 0.005 # Example: different LR could work better\n",
    "optimizer = torch.optim.SGD(rnn.parameters(), lr=learning_rate)\n",
    "\n",
    "# Initialize the training loop\n",
    "batch_size = 2\n",
    "n_iterations = 100000\n",
    "print_every = 5000\n",
    "\n",
    "# Keep track of the loss\n",
    "current_loss = 0\n",
    "\n",
    "for i in range(1, n_iterations + 1):\n",
    "    # Get a random training input and target\n",
    "    category_tensor, line_tensor = random_training_pair(bs=batch_size)\n",
    "\n",
    "    # Perform the training step\n",
    "    output, loss = train(rnn, optimizer, category_tensor, line_tensor)\n",
    "\n",
    "    # Accumulate loss for printing\n",
    "    current_loss += loss\n",
    "\n",
    "    # Print iteration number and loss\n",
    "    if i % print_every == 0:\n",
    "        print(f\"Iter {i}/{n_iterations + 1} ({i / n_iterations * 100:.0f}%) - Loss: {current_loss / print_every:.4f}\")\n",
    "        current_loss = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rfwWdBVfSUv6"
   },
   "source": [
    "## Trying it out\n",
    "\n",
    "Finally, following the original tutorial [in the Practical PyTorch repo](https://github.com/spro/practical-pytorch/tree/master/char-rnn-classification) we instantiate a prediction function and test on some user defined inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F673kuLHRkx8"
   },
   "outputs": [],
   "source": [
    "normalizer = torch.nn.Softmax(dim=-1)\n",
    "\n",
    "def predict(input_line, n_predictions=3):\n",
    "    print(f\"\\n> {input_line}\")\n",
    "    output = rnn(line_to_tensor(input_line))\n",
    "    output = normalizer(output)\n",
    "    # get top N categories\n",
    "    top_values, top_index = output.data.topk(n_predictions, 1, True)\n",
    "    predictions = []\n",
    "\n",
    "    for i in range(n_predictions):\n",
    "        value = top_values[0][i]  # 0 indexes the first batch element\n",
    "        category_index = top_index[0][i]\n",
    "        print(f\"({value:.2f}) {all_categories[category_index]}\")\n",
    "        predictions.append([value, all_categories[category_index]])\n",
    "\n",
    "predict(\"Dovesky\")\n",
    "predict(\"Jackson\")\n",
    "predict(\"Satoshi\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
