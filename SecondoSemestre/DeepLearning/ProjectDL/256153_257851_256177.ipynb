{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0781d465",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "  <h2><u><strong>Deep Learning 2025 - Project Assignment</strong></u></h2>\n",
    "  <p>\n",
    "    Elia Avanzolini<br>\n",
    "    Matricola: 256153\n",
    "  </p>\n",
    "  <p>\n",
    "    Martina Panini<br>\n",
    "    Matricola: 257851\n",
    "  </p>\n",
    "  <p>\n",
    "    Jie Chen<br>\n",
    "    Matricola: 256177\n",
    "  </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4226ef5f",
   "metadata": {},
   "source": [
    "## Abstract\n",
    "This project addresses the challenge of few-shot adaptation in vision-language models, focusing on improving CLIP's performance on base classes while preserving its zero-shot generalization to novel categories. We propose a hybrid approach combining LoRA-based vision encoder adaptation with Tip-Adapter-F, a cache-based approach that stores features from the few-shot examples to enhance base-class predictions. An adaptive ensemble integrates predictions from the specialized LoRA-finetuned model and the original pre-trained model, dynamically balancing confidence between base and novel categories. Evaluated on the Oxford Flowers-102 dataset, our method demonstrates that LoRA specialization boosts base-class accuracy but reduces novel-class performance, while Tip-Adapter-F maintains transferability. The ensemble strategy effectively balances these trade-offs, achieving a 24.67% improvement in base-class accuracy (from 71.33% to 96.00%) and preserving novel-class accuracy (77.64%), yielding the highest harmonic mean (85.85%). Our results highlight the importance of integrating parameter-efficient adaptation with memory-based methods for robust few-shot learning in vision-language systems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b31b4417",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a75857b",
   "metadata": {},
   "source": [
    "Large-scale vision-language models such as CLIP have demonstrated remarkable generalization capabilities in zero-shot image classification by leveraging contrastive learning on massive image-text pairs. However, their performance tends to decline in fine-grained tasks and few-shot scenarios, particularly when strong accuracy on base classes is required without degrading generalization to novel ones.\n",
    "\n",
    "In this project, we tackle the **Few-Shot Adaptation** problem as define in project assignment. Specifically, we aim to improve classification accuracy on base categories from the Oxford Flowers-102 dataset using only the 10 labeled examples provided per class, while trying to preserve the modelâ€™s original zero-shot performance on novel categories. Therefore, our aim is to develop a final model that balance high accuracy on base classes and maintain strong generalization to novel classes, measured via their **harmonic mean**.\n",
    "\n",
    "To address this challenge, we propose a method that combines three complementary components:\n",
    "\n",
    "- **LoRA-based adaptation of the CLIP vision encoder**, enabling efficient and targeted updates to class-level vision embeddings while keeping the rest of the model frozen.\n",
    "- **Tip-Adapter-F**, a parameter-efficient fine-tuning approach that trains a lightweight MLP adapter on top of frozen CLIP image features to better align support image embeddings with their corresponding class prototypes.\n",
    "- **Ensemble scoring**, where we combine predictions from previous methods, yielding a final score that benefits from both. \n",
    "\n",
    "We evaluate our approach using three key metrics:\n",
    "1. CLIP zero-shot accuracy on **base classes**,\n",
    "2. CLIP zero-shot accuracy on **novel classes**, and\n",
    "3. The **harmonic mean** of the two, which reflects the trade-off between specialization and generalization.\n",
    "\n",
    "These metrics form our baseline for comparison. The central objective of our work is to **maximize base class accuracy using only the provided few-shot data**, while **minimizing any drop in performance on the novel classes**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "73c88146",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-23T11:07:18.747660Z",
     "iopub.status.busy": "2025-08-23T11:07:18.747330Z",
     "iopub.status.idle": "2025-08-23T11:07:20.257028Z",
     "shell.execute_reply": "2025-08-23T11:07:20.256266Z",
     "shell.execute_reply.started": "2025-08-23T11:07:18.747637Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai_clip in /opt/conda/lib/python3.12/site-packages (1.0.1)\n",
      "Requirement already satisfied: ftfy in /opt/conda/lib/python3.12/site-packages (from openai_clip) (6.3.1)\n",
      "Requirement already satisfied: regex in /opt/conda/lib/python3.12/site-packages (from openai_clip) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.12/site-packages (from openai_clip) (4.67.1)\n",
      "Requirement already satisfied: wcwidth in /opt/conda/lib/python3.12/site-packages (from ftfy->openai_clip) (0.2.13)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# we need to install clip as it is not pre-installed\n",
    "# https://github.com/mlfoundations/open_clip\n",
    "%pip install openai_clip\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4986667c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-23T11:07:20.258539Z",
     "iopub.status.busy": "2025-08-23T11:07:20.258218Z",
     "iopub.status.idle": "2025-08-23T11:07:20.263041Z",
     "shell.execute_reply": "2025-08-23T11:07:20.262538Z",
     "shell.execute_reply.started": "2025-08-23T11:07:20.258514Z"
    }
   },
   "outputs": [],
   "source": [
    "#Here we import the necessary libraries\n",
    "\n",
    "import clip\n",
    "\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from typing import Optional, List, Tuple, Union, Callable, Dict\n",
    "from PIL import Image\n",
    "from collections import OrderedDict\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.init import xavier_uniform_\n",
    "from torch.nn.init import constant_\n",
    "from torch.nn.init import xavier_normal_\n",
    "\n",
    "import warnings\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "72d7b910",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-23T11:07:20.263978Z",
     "iopub.status.busy": "2025-08-23T11:07:20.263740Z",
     "iopub.status.idle": "2025-08-23T11:07:20.400899Z",
     "shell.execute_reply": "2025-08-23T11:07:20.400346Z",
     "shell.execute_reply.started": "2025-08-23T11:07:20.263960Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Set up the device (GPU if available, otherwise CPU)\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "    torch.cuda.empty_cache()\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "224cf89a",
   "metadata": {},
   "source": [
    "## Methodology"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14dfacb0",
   "metadata": {},
   "source": [
    "We conduct our experiments on the **Oxford Flowers-102** dataset [1], a widely used benchmark for fine-grained image classification. The dataset contains 102 flower categories, selected from species commonly found in the United Kingdom. Each class includes between 40 and 258 images, with substantial variation in scale, pose, and lighting conditions. Additionally, several categories exhibit high intra-class variation and inter-class similarity, making the classification task particularly challenging.\n",
    "\n",
    "For this project, we use the official split and helper code provided with the assignment to download, load, and partition the dataset. As the training set contains exactly 10 labeled images per class, we adopt a **10-shot** setting (i.e., *k = 10*) for all our few-shot learning experiments.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5eafc452",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-23T11:07:20.402747Z",
     "iopub.status.busy": "2025-08-23T11:07:20.402266Z",
     "iopub.status.idle": "2025-08-23T11:07:20.407254Z",
     "shell.execute_reply": "2025-08-23T11:07:20.406741Z",
     "shell.execute_reply.started": "2025-08-23T11:07:20.402725Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define list of class names\n",
    "CLASS_NAMES = [\"pink primrose\", \"hard-leaved pocket orchid\", \"canterbury bells\", \"sweet pea\", \"english marigold\", \"tiger lily\", \"moon orchid\", \"bird of paradise\", \"monkshood\", \"globe thistle\", \"snapdragon\", \"colt's foot\", \"king protea\", \"spear thistle\", \"yellow iris\", \"globe-flower\", \"purple coneflower\", \"peruvian lily\", \"balloon flower\", \"giant white arum lily\", \"fire lily\", \"pincushion flower\", \"fritillary\", \"red ginger\", \"grape hyacinth\", \"corn poppy\", \"prince of wales feathers\", \"stemless gentian\", \"artichoke\", \"sweet william\", \"carnation\", \"garden phlox\", \"love in the mist\", \"mexican aster\", \"alpine sea holly\", \"ruby-lipped cattleya\", \"cape flower\", \"great masterwort\", \"siam tulip\", \"lenten rose\", \"barbeton daisy\", \"daffodil\", \"sword lily\", \"poinsettia\", \"bolero deep blue\", \"wallflower\", \"marigold\", \"buttercup\", \"oxeye daisy\", \"common dandelion\", \"petunia\", \"wild pansy\", \"primula\", \"sunflower\", \"pelargonium\", \"bishop of llandaff\", \"gaura\", \"geranium\", \"orange dahlia\", \"pink-yellow dahlia?\", \"cautleya spicata\", \"japanese anemone\", \"black-eyed susan\", \"silverbush\", \"californian poppy\", \"osteospermum\", \"spring crocus\", \"bearded iris\", \"windflower\", \"tree poppy\", \"gazania\", \"azalea\", \"water lily\", \"rose\", \"thorn apple\", \"morning glory\", \"passion flower\", \"lotus\", \"toad lily\", \"anthurium\", \"frangipani\", \"clematis\", \"hibiscus\", \"columbine\", \"desert-rose\", \"tree mallow\", \"magnolia\", \"cyclamen\", \"watercress\", \"canna lily\", \"hippeastrum\", \"bee balm\", \"ball moss\", \"foxglove\", \"bougainvillea\", \"camellia\", \"mallow\", \"mexican petunia\", \"bromelia\", \"blanket flower\", \"trumpet creeper\", \"blackberry lily\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "40584701",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-23T11:07:20.408165Z",
     "iopub.status.busy": "2025-08-23T11:07:20.407908Z",
     "iopub.status.idle": "2025-08-23T11:07:20.418696Z",
     "shell.execute_reply": "2025-08-23T11:07:20.418185Z",
     "shell.execute_reply.started": "2025-08-23T11:07:20.408146Z"
    }
   },
   "outputs": [],
   "source": [
    "# Same function provided in the class notebook \n",
    "def get_data(data_dir=\"./data\", transform=None):\n",
    "    \"\"\"Loads Flowers102 train, validation, and test sets.\"\"\"\n",
    "    train = torchvision.datasets.Flowers102(root=data_dir, split=\"train\", download=True, transform=transform)\n",
    "    val = torchvision.datasets.Flowers102(root=data_dir, split=\"val\", download=True, transform=transform)\n",
    "    test = torchvision.datasets.Flowers102(root=data_dir, split=\"test\", download=True, transform=transform)\n",
    "    return train, val, test\n",
    "\n",
    "def base_novel_categories(dataset):\n",
    "    \"\"\"Splits dataset categories into base and novel classes.\"\"\"\n",
    "    all_classes = set(dataset._labels)\n",
    "    num_classes = len(all_classes)\n",
    "    base_classes = list(range(num_classes))[:num_classes//2]\n",
    "    novel_classes = list(range(num_classes))[num_classes//2:]\n",
    "    return base_classes, novel_classes\n",
    "\n",
    "def split_data(dataset, base_classes):\n",
    "    \"\"\"Divides a dataset into subsets based on base and novel categories.\"\"\"\n",
    "    base_categories_samples = []\n",
    "    novel_categories_samples = []\n",
    "    base_set = set(base_classes)\n",
    "    for sample_id, label in enumerate(dataset._labels):\n",
    "        if label in base_set:\n",
    "            base_categories_samples.append(sample_id)\n",
    "        else:\n",
    "            novel_categories_samples.append(sample_id)\n",
    "    base_dataset = torch.utils.data.Subset(dataset, base_categories_samples)\n",
    "    novel_dataset = torch.utils.data.Subset(dataset, novel_categories_samples)\n",
    "    return base_dataset, novel_dataset\n",
    "\n",
    "def harmonic_mean(base_accuracy, novel_accuracy):\n",
    "    \"\"\"Calculates the harmonic mean of two accuracies.\"\"\"\n",
    "    numerator = 2\n",
    "    denominator = 1 / base_accuracy + 1 / novel_accuracy\n",
    "    return numerator / denominator\n",
    "\n",
    "# Evaluation function from the provided notebook\n",
    "@torch.no_grad()\n",
    "def eval_zeroshot(model, dataset, categories, batch_size, device, label=\"\"):\n",
    "    model.eval()\n",
    "    contig_cat2idx = {cat: idx for idx, cat in enumerate(categories)}\n",
    "    text_inputs = clip.tokenize(\n",
    "        [f\"a photo of a {CLASS_NAMES[c]}, a type of flower.\" for c in categories]\n",
    "    ).to(device)\n",
    "    text_features = model.encode_text(text_inputs)\n",
    "    text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "    correct_predictions = 0\n",
    "    for image, target in tqdm(dataloader, desc=label):\n",
    "        target = torch.Tensor([contig_cat2idx[t.item()] for t in target]).long()\n",
    "        image = image.to(device)\n",
    "        target = target.to(device)\n",
    "        image_features = model.encode_image(image)\n",
    "        image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "        predicted_class = (image_features @ text_features.T).argmax(dim=-1)\n",
    "        correct_predictions += (predicted_class == target).sum().item()\n",
    "    accuracy = correct_predictions / len(dataset)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "701db1f5",
   "metadata": {},
   "source": [
    "### Compute Baseline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c1ef841",
   "metadata": {},
   "source": [
    "We begin our experiments by evaluating the zero-shot performance of the original CLIP model on both the base and novel classes of the Oxford Flowers-102 dataset. Specifically, we compute the top-1 classification accuracy separately for base and novel categories. From these two metrics, we also derive their **harmonic mean**, which provides a balanced measure of performance across both domains.\n",
    "\n",
    "These three values â€” base accuracy, novel accuracy, and their harmonic mean â€” serve as our **baseline** for all subsequent comparisons. They allow us to quantify the impact of each adaptation method not only on the accuracy of base classes (where fine-tuning occurs), but also on the model's ability to retain generalization to unseen (novel) classes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "68b58fbe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-23T11:07:20.419538Z",
     "iopub.status.busy": "2025-08-23T11:07:20.419343Z",
     "iopub.status.idle": "2025-08-23T11:07:51.516931Z",
     "shell.execute_reply": "2025-08-23T11:07:51.516240Z",
     "shell.execute_reply.started": "2025-08-23T11:07:20.419521Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zero-Shot CLIP Baseline\n",
      "Loading original CLIP model and preparing dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ðŸ§  Zero-shot evaluation on Base Classes: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:11<00:00,  1.79it/s]\n",
      "ðŸ§  Zero-shot evaluation on Novel Classes: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [00:16<00:00,  1.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset prepared.\n",
      "Number of base training samples: 510\n",
      "Number of novel test samples: 3676\n",
      "Base classes accuracy: 71.33%\n",
      "Novel classes accuracy: 78.24%\n",
      "Harmonic Mean: 74.62%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Zero-Shot CLIP Baseline\")\n",
    "print(\"Loading original CLIP model and preparing dataset...\")\n",
    "\n",
    "# Load the original CLIP model and its pre-processing function\n",
    "clip_model, clip_preprocess = clip.load(\"ViT-B/16\", device=device)\n",
    "\n",
    "# Get the Flowers102 datasets\n",
    "train_set, val_set, test_set = get_data(transform=clip_preprocess)\n",
    "\n",
    "# Split classes and datasets into base and novel categories\n",
    "base_classes, novel_classes = base_novel_categories(train_set)\n",
    "train_base, _ = split_data(train_set, base_classes)\n",
    "val_base, _ = split_data(val_set, base_classes)\n",
    "test_base, test_novel = split_data(test_set, base_classes)\n",
    "\n",
    "\n",
    "base_accuracy = eval_zeroshot(model=clip_model, dataset=test_base, categories=base_classes, batch_size=128, device=device, label=\"ðŸ§  Zero-shot evaluation on Base Classes\")\n",
    "novel_accuracy = eval_zeroshot(model=clip_model, dataset=test_novel, categories=novel_classes, batch_size=128, device=device, label=\"ðŸ§  Zero-shot evaluation on Novel Classes\")\n",
    "\n",
    "print(\"Dataset prepared.\")\n",
    "print(f\"Number of base training samples: {len(train_base)}\")\n",
    "print(f\"Number of novel test samples: {len(test_novel)}\")\n",
    "print(f\"Base classes accuracy: {base_accuracy*100:.2f}%\")\n",
    "print(f\"Novel classes accuracy: {novel_accuracy*100:.2f}%\")\n",
    "print(f\"Harmonic Mean: {harmonic_mean(base_accuracy, novel_accuracy)*100:.2f}%\")\n",
    "\n",
    "# Store baseline scores for plotting\n",
    "base_accuracy_zeroshot = base_accuracy\n",
    "novel_accuracy_zeroshot = novel_accuracy\n",
    "hm_zeroshot = harmonic_mean(base_accuracy_zeroshot, novel_accuracy_zeroshot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c8c4fc97",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-23T11:07:51.518717Z",
     "iopub.status.busy": "2025-08-23T11:07:51.518425Z",
     "iopub.status.idle": "2025-08-23T11:07:51.687556Z",
     "shell.execute_reply": "2025-08-23T11:07:51.686965Z",
     "shell.execute_reply.started": "2025-08-23T11:07:51.518694Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABKUAAAKyCAYAAAAEvm1SAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjMsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvZiW1igAAAAlwSFlzAAAPYQAAD2EBqD+naQAAbY5JREFUeJzt3XlUFfX/x/HXZUcEVJRNEXBfEE2wUnNLkdzSvplZ5m5Kaq5ZLrmbmJlalpalYmnpTy3za+aWa1mmpuVCWmbigvu+BAjz+8PD/XZlERTnGj4f59xzup/5zMx7hhvjffGZz1gMwzAEAAAAAAAAmMjB3gUAAAAAAADgwUMoBQAAAAAAANMRSgEAAAAAAMB0hFIAAAAAAAAwHaEUAAAAAAAATEcoBQAAAAAAANMRSgEAAAAAAMB0hFIAAAAAAAAwHaEUAAAAAAAATEcoBQDItywWy21fo0aNsneZNlJSUvThhx+qRo0aKlKkiAoUKKDg4GC1bNlSX375pbXfhg0bZLFYtHjx4jzb94oVK+7ofPz3v/9VixYt5OfnJxcXFxUpUkQNGzbU/PnzlZKSYu1nsVjUu3fvbLdVv359hYWF2bSFhITY/MwKFiyoRx55RJ988sltaxs1apTNui4uLgoNDVXfvn114cKFXB9rds6dO6e2bdvK19dXFotFrVq1ytPt496oX79+lr8f9uzZI+l//79t2LDBvsXeA8uXL1eHDh1UpUoVOTs7y2KxZNk3JSVFo0ePVkhIiFxdXVWhQgVNmzYtQ7+PP/5YrVq1UkhIiNzd3VWmTBm99NJLSkxMvJeHAgD4F3KydwEAANwrP/zwQ6btN27cUIcOHXTs2DE1bdrU5Kqy1759e33xxRfq16+fRo8eLVdXV/35559auXKlVq1apaeeeuqe7XvFihV6//33cxxMGYahLl26KC4uTk2bNtXkyZMVFBSkixcvav369erZs6fOnDmjvn373nVttWvX1qRJkyRJR48e1aRJk9SxY0ddvXpVL7300m3XX7lypby9vXX58mWtWLFC77zzjn766Sdt2bIl2y/huTF27Fh9+eWXmj17tkqXLq0iRYrkyXZx75UqVUrz58/P0F66dGk7VGOuL7/8Uj/++KMeeughubq6aseOHVn27dmzpz799FONHTtWNWrU0KpVq9S3b19dvnxZQ4cOtfYbOXKkGjRooPHjx6t48eLav3+/xo4dq6+++ko7d+6Un5+fGYcGAPgXIJQCAORbjz76aKbtffr00aFDh/Thhx/q4YcfzpN9Xb9+Xe7u7ne1jUOHDmnhwoUaMWKERo8ebW1v2LChXnzxRaWlpd1tmXnqrbfeUlxcnEaPHq0RI0bYLGvRooVeffVV/fHHH3myr0KFCtn8PBs1aqTg4GBNnjw5R6FURESEihYtKkmKiorS2bNn9emnn2rLli2qXbv2XdWW/rPfs2ePSpcurXbt2t3V9tIZhqG///77rj9XuD13d/csf1/8G127dk0FChTIUd+PPvpIDg43b57o3bt3lqHU3r17NWvWLL3xxhsaNGiQpJujzM6ePatx48YpJibGGsTu3LlTvr6+1nXr1aun6tWrq0aNGvroo4/0+uuv383hAQDyEW7fAwA8UD799FNNmzZNXbt2Vffu3W2WJScna9y4capQoYJcXV1VrFgxde7cWadPn7bpFxISoubNm+uLL77QQw89JDc3N2uItGfPHrVs2VKFCxeWm5ubqlWrprlz5+aotrNnz0qSAgICMl2e/sXxn1JSUjRs2DAFBgbKy8tLjRo10v79+zP0mz17tqpWrSo3NzcVKVJETz31lOLj463LO3XqpPfff1+S7W2Pf/31V6a1pKSk6M0331SFChU0fPjwTPv4+/vrsccey/aY71ShQoVUvnx5HT58+I7WTw8g0te/m599586dZbFYtHbtWsXHx1vPXfqtXufOnVPPnj1VvHhxubi4qFSpUho2bJiSkpJstp1+e+MHH3ygihUrytXVVXPnzlVcXJwsFovWrVunF198UT4+PvLy8lKHDh109epVnThxQm3atFGhQoUUEBCgV155xea2SUkaPXq0HnnkERUpUkReXl6qXr26Zs2aJcMwMj2+lStXqnr16nJ3d1eFChU0e/bsDOfw2LFj6t69u4KCguTi4qLAwEC1bt1aJ0+etPa5dOmSXnnlFYWGhsrFxUXFixdXv379dPXq1Rz9nG73uZVufnYLFiyoP/74Q02bNlXBggUVFBSkgQMHZjjHeW3ZsmWqWbOmChQoIE9PT0VFRdmM0Ny7d68sFosWLVpkbduxY4csFosqV65ss60nn3xSERERNm0LFy5UzZo15eHhoYIFCyo6Olo7d+606ZN+/Lt371bjxo3l6emphg0bSroZDjVv3ly+vr5ydXVVYGCgmjVrpqNHj1rXz+z3SmaWLl0qwzDUuXNnm/bOnTvr+vXrWrlypbXtn4FUuoiICDk6OurIkSM52h8A4MHASCkAwANj586d6tGjh2rUqGENYNKlpaWpZcuW2rx5s1599VXVqlVLhw8f1siRI1W/fn1t377dZsTKzz//rPj4eL3++usKDQ2Vh4eH9u/fr1q1asnX11fvvvuufHx8NG/ePHXq1EknT57Uq6++mm19FStWVKFChTR69Gg5ODiocePGCgkJyXadoUOHqnbt2vr444916dIlvfbaa2rRooXi4+Pl6OgoSYqNjdXQoUP13HPPKTY2VmfPntWoUaNUs2ZNbdu2TWXLltXw4cN19epVLV682OZLdVYB2fbt23Xu3Dm9+OKLeXb7W26kpKTo8OHDKlas2B2tnz6Cq1ixYnf9s3d3d1e/fv3Us2dPXbx40XobWKVKlfT333+rQYMGOnjwoEaPHq3w8HBt3rxZsbGx2rVrl77++mubupYuXarNmzdrxIgR8vf3l6+vr7Zt2yZJ6tatm/7zn/9owYIF2rlzp4YOHaobN25o//79+s9//qPu3btr7dq1evPNNxUYGKgBAwZYt/vXX3+pR48eKlmypCTpxx9/1Msvv6xjx45lGOX2yy+/aODAgRo8eLD8/Pz08ccfq2vXripTpozq1q0r6WYgVaNGDaWkpGjo0KEKDw/X2bNntWrVKp0/f15+fn66du2a6tWrp6NHj1r77N27VyNGjNDu3bu1du3abD87OfncpktJSdGTTz6prl27auDAgdq0aZPGjh0rb2/vDMeXlRs3bti8d3BwyDaw+eyzz9SuXTs1btxYn3/+uZKSkjRx4kTVr19f3377rR577DFVrlxZAQEBWrt2rZ555hlJ0tq1a+Xu7q59+/bp+PHjCgwM1I0bN7Rx40bFxMRYtz9+/Hi9/vrr6ty5s15//XUlJyfrrbfeUp06dfTTTz+pUqVK1r7Jycl68skn1aNHDw0ePFg3btzQ1atXFRUVpdDQUL3//vvy8/PTiRMntH79el2+fDlH5+Sf9uzZo2LFisnf39+mPTw83Lo8Oxs3blRqamqGMA4A8IAzAAB4AJw+fdoIDg42ihUrZiQkJGRY/vnnnxuSjCVLlti0b9u2zZBkTJ8+3doWHBxsODo6Gvv377fp27ZtW8PV1TXD9ps0aWIUKFDAuHDhwm3r/Prrr42iRYsakgxJho+Pj/HMM88Yy5Yts+m3fv16Q5LRtGlTm/b/+7//MyQZP/zwg2EYhnH+/HnD3d09Q7+EhATD1dXVeP75561tvXr1MnL6T4MFCxYYkowPPvggR/0NwzAkGb169cq2T7169YzKlSvbtAUHBxtNmzY1UlJSjJSUFOPQoUNGx44dDUnGoEGDst3eyJEjDUnGiRMnjJSUFOP8+fPGvHnzDHd3dyMoKMi4fv16nvzss6r9gw8+MCQZ//d//2fT/uabbxqSjNWrV9ucH29vb+PcuXM2fefMmWNIMl5++WWb9latWhmSjMmTJ9u0V6tWzahevXqW5yQ1NdVISUkxxowZY/j4+BhpaWk2x+fm5mYcPnzY2nb9+nWjSJEiRo8ePaxtXbp0MZydnY19+/ZluZ/Y2FjDwcHB2LZtm0374sWLDUnGihUrslw3N5/b9M/Cree4adOmRvny5bPcR7p69epZ/3/756tdu3bWPun/v61fv94wjJvnMDAw0KhSpYqRmppq7Xf58mXD19fXqFWrlrXthRdeMEqVKmV936hRI+PFF180ChcubMydO9cwDMP4/vvvbT4PCQkJhpOTU4af+eXLlw1/f3+jTZs2GY5/9uzZNn23b99uSDKWLl1623OQLrvfAVFRUVmeTxcXF6N79+5ZbvfSpUtGxYoVjaCgIOPy5cs5rgcAkP9x+x4AIN9LTU1V27ZtdfToUS1cuFBBQUEZ+ixfvlyFChVSixYtdOPGDeurWrVq8vf3z/DUrfDwcJUrV86mbd26dWrYsGGG7Xfq1EnXrl2zjkBKS0uz2Udqaqq1b9OmTZWQkKAvv/xSr7zyiipXrqylS5fqySefzPTJdU8++WSGuqT/3Zb2ww8/6Pr16+rUqZNNv6CgID3++OP69ttvszlz948VK1bI2dlZzs7OCg0N1f/93//p5Zdf1rhx43K0vr+/v5ydnVW4cGG98MILql69ulauXCk3N7c8+dlnZd26dfLw8FDr1q1t2tN/Hree/8cff1yFCxfOdFvNmze3eV+xYkVJUrNmzTK033pb47p169SoUSN5e3vL0dFRzs7OGjFihM6ePatTp07Z9K1WrZp1RJUkubm5qVy5cjbb/Oabb9SgQQNrDZlZvny5wsLCVK1aNZvzGh0dfdsn2eX2c2uxWNSiRQubtvDw8Bzf3lm6dGlt27bN5jV27Ngs++/fv1/Hjx9X+/btbUZTFSxYUE8//bR+/PFHXbt2TdLNOeH+/PNPHTp0SH///be+++47PfHEE2rQoIHWrFkj6eboKVdXV+vtrqtWrbI+kOGf587NzU316tXL9Nw9/fTTNu/LlCmjwoUL67XXXtMHH3ygffv25ehcZCe7kW1ZLfv777/1n//8R4cPH9aiRYtUsGDBu64DAJB/cPseACDfe/XVV/Xtt99q0qRJatCgQaZ9Tp48qQsXLsjFxSXT5WfOnLF5n9ltbWfPns20PTAw0Lpckrp06WIzz9StXzLd3d3VqlUrtWrVSpKUkJCgJk2a6P3339dLL71kc/uLj4+Pzb5cXV0l3Zx8+5/7zKqu9C/FuZUeWhw6dOiO1s+txx57TFOmTJHFYlGBAgVUunTpLH9WmVm7dq28vb3l7OysEiVK2Jy3vPjZZ+Xs2bPy9/fP8IXd19dXTk5O1p9PTrZ969P80uvNrP3vv/+2vv/pp5/UuHFj1a9fXx999JFKlCghFxcXLV26VG+88Yb1s5Lu1s+UdPNz9c9+p0+fVokSJbKsVbp5Xv/44w85OztnuvzW8/pPuf3cFihQQG5ubhlq/ud5yI6bm5siIyNz1Dcn9aWlpen8+fMqUKCAGjVqJOnmZzA0NFQpKSl6/PHHdfLkSWvwtXbtWtWuXdt6m2j6vFw1atTIdP+33lZYoEABeXl52bR5e3tr48aNeuONNzR06FCdP39eAQEBevHFF/X6669n+XPJio+Pj3bt2pWh/erVq0pOTs70aZNJSUl66qmn9N1332n58uV65JFHcrVPAED+RygFAMjXPv/8c02ePFnPPvusBg4cmGW/okWLysfHx2ay3n/y9PS0eZ/ZqAAfHx8lJiZmaD9+/Lh1H5I0atQom1FPt277ViVLllT37t3Vr18/7d27N1dzsqQHDFnVlV5TbkVGRqpIkSL66quvFBsbe8/nlfL29s5VaHCrqlWrZnmsefGzz4qPj4+2bt0qwzBs1jt16pRu3LiRoaZ7cR4XLFggZ2dnLV++3Ca4Wbp06R1vs1ixYjaTZWemaNGicnd3z3SS9PTlWblXn9u8crv6HBwcrCPeSpQooXLlymnt2rUKCQlRZGSkChUqpIYNG6pnz57aunWrfvzxR5snbqYf3+LFixUcHHzberL63FSpUkULFiyQYRj69ddfFRcXpzFjxsjd3V2DBw/O1TGnb+vEiRM280rt3r1bkhQWFmbTPykpSa1atdL69ev11VdfWSdfBwDgn7h9DwCQb/3666/q1q2bwsLCNGvWrGz7Nm/eXGfPnlVqaqoiIyMzvMqXL3/b/TVs2FDr1q2zhlDpPvnkExUoUMD6xLf0L6a3bvvy5cu6cuVKpttOf+JY+qirnKpZs6bc3d01b948m/ajR49abzdMd+soq+w4Ozvrtdde02+//ZblbU6nTp3S999/n6t67SEvfvZZadiwoa5cuZIhAPrkk0+sy+81i8UiJycn68T30s2f8aeffnrH22zSpInWr1+f6ZMe0zVv3lwHDx6Uj49Ppuc1u0n8c/O5tYfy5curePHi+uyzz2yeYHj16lUtWbLE+kS+dI0aNdK6deu0Zs0aRUVFSZLKlSunkiVLasSIEUpJSbGOqJKk6OhoOTk56eDBg5meu9wGtBaLRVWrVtWUKVNUqFAh/fzzz7k+5pYtW8pisWR4mmhcXJzc3d31xBNPWNvSR0itW7dOS5YsUXR0dK73BwB4MDBSCgCQL50/f16tWrVSUlKSXnvtNetf829VrFgxlS5dWm3bttX8+fPVtGlT9e3bVw8//LCcnZ119OhRrV+/Xi1bttRTTz2V7T5Hjhyp5cuXq0GDBhoxYoSKFCmi+fPn6+uvv9bEiRPl7e2d7fr79+9XdHS02rZtq3r16ikgIEDnz5/X119/rZkzZ6p+/fqqVatWrs5DoUKFNHz4cA0dOlQdOnTQc889p7Nnz2r06NFyc3PTyJEjrX2rVKkiSXrzzTfVpEkTOTo6Kjw8PMvb2gYNGqT4+HiNHDlSP/30k55//nkFBQXp4sWL2rRpk2bOnKnRo0erdu3a1nUOHjyoxYsXZ9hWpUqVbJ4mZqa8+NlnpUOHDnr//ffVsWNH/fXXX6pSpYq+++47jR8/Xk2bNrUJIu6VZs2aafLkyXr++efVvXt3nT17VpMmTbKGkHdizJgx+uabb1S3bl0NHTpUVapU0YULF7Ry5UoNGDBAFSpUUL9+/bRkyRLVrVtX/fv3V3h4uNLS0pSQkKDVq1dr4MCBWd7OlZvPrT04ODho4sSJateunZo3b64ePXooKSlJb731li5cuKAJEybY9G/YsKGmT5+uM2fOaOrUqTbtc+bMUeHChRUREWFtDwkJ0ZgxYzRs2DD9+eefeuKJJ1S4cGGdPHlSP/30kzw8PGxGVmVm+fLlmj59ulq1aqVSpUrJMAx98cUXunDhgjUYk27OP5f+hMeDBw9KkvX/0fQAXZIqV66srl27auTIkXJ0dFSNGjW0evVqzZw5U+PGjbO5fa9169b65ptvNGzYMPn4+OjHH3+0LvPy8rLb/+sAgPuQfedZBwDg3kh/WtbtXh07drSuk5KSYkyaNMmoWrWq4ebmZhQsWNCoUKGC0aNHD+P333+39gsODjaaNWuW6X53795ttGjRwvD29jZcXFyMqlWrGnPmzMlRzefPnzfGjRtnPP7440bx4sUNFxcXw8PDw6hWrZoxbtw449q1axmOb9GiRTbbOHTokCEpwz4//vhjIzw83HBxcTG8vb2Nli1bGnv37rXpk5SUZHTr1s0oVqyYYbFYDEnGoUOHblv3V199ZTRr1swoVqyY4eTkZBQuXNho0KCB8cEHHxhJSUnWftn9HEaOHGkYRtZP38vqfN9O+tP3Tp8+nW2/vPjZZ1a7YRjG2bNnjZiYGCMgIMBwcnIygoODjSFDhhh///23TT9l8XTC9Kfv3foUu6yOrWPHjoaHh4dN2+zZs43y5csbrq6uRqlSpYzY2Fhj1qxZGX7GWR1fvXr1jHr16tm0HTlyxOjSpYvh7+9vODs7G4GBgUabNm2MkydPWvtcuXLFeP31143y5ctbP3tVqlQx+vfvb5w4cSLDfm6Vk89tZsf7z/NzO1n93P7p1qfvpVu6dKnxyCOPGG5uboaHh4fRsGFD4/vvv8+w/vnz5w0HBwfDw8PDSE5OtrbPnz/fkGT85z//yXS/S5cuNRo0aGB4eXkZrq6uRnBwsNG6dWtj7dq11j5ZHf9vv/1mPPfcc0bp0qUNd3d3w9vb23j44YeNuLg4m37pn6/b/X40DMNITk42Ro4caZQsWdJwcXExypUrZ7z77rsZ9p3d/+u3fo4AAA82i2H8Y8wxAAAAAAAAYALmlAIAAAAAAIDpCKUAAAAAAABgOkIpAAAAAAAAmM6uodSmTZvUokULBQYGymKxZHhccmY2btyoiIgIubm5qVSpUvrggw/ufaEAAAAAAADIU3YNpa5evaqqVavqvffey1H/Q4cOqWnTpqpTp4527typoUOHqk+fPlqyZMk9rhQAAAAAAAB56b55+p7FYtGXX36pVq1aZdnntdde07JlyxQfH29ti4mJ0S+//KIffvjBhCoBAAAAAACQF5zsXUBu/PDDD2rcuLFNW3R0tGbNmqWUlBQ5OztnWCcpKUlJSUnW92lpaTp37px8fHxksVjuec0AAAAAAAAPEsMwdPnyZQUGBsrBIeub9P5VodSJEyfk5+dn0+bn56cbN27ozJkzCggIyLBObGysRo8ebVaJAAAAAAAAkHTkyBGVKFEiy+X/qlBKUobRTel3H2Y16mnIkCEaMGCA9f3FixdVsmRJHTlyRF5eXveuUAAAAAAAgAfQpUuXFBQUJE9Pz2z7/atCKX9/f504ccKm7dSpU3JycpKPj0+m67i6usrV1TVDu5eXF6EUAAAAAADAPXK7aZPs+vS93KpZs6bWrFlj07Z69WpFRkZmOp8UAAAAAAAA7k92DaWuXLmiXbt2adeuXZKkQ4cOadeuXUpISJB089a7Dh06WPvHxMTo8OHDGjBggOLj4zV79mzNmjVLr7zyij3KBwAAAAAAwB2y6+1727dvV4MGDazv0+d+6tixo+Li4pSYmGgNqCQpNDRUK1asUP/+/fX+++8rMDBQ7777rp5++mnTawcAAAAAAMCdsxjpM4U/IC5duiRvb29dvHiROaUAAAAAAPleamqqUlJS7F0G8hFnZ2c5OjpmuTyn2cu/aqJzAAAAAACQM4Zh6MSJE7pw4YK9S0E+VKhQIfn7+992MvPsEEoBAAAAAJAPpQdSvr6+KlCgwF2FB0A6wzB07do1nTp1SpIUEBBwx9silAIAAAAAIJ9JTU21BlI+Pj72Lgf5jLu7uyTp1KlT8vX1zfZWvuzY9el7AAAAAAAg76XPIVWgQAE7V4L8Kv2zdTfzlRFKAQAAAACQT3HLHu6VvPhsEUoBAAAAAADAdIRSAAAAAAAAdykkJERTp061dxn/Kkx0DgAAAADAA2TCzjOm7WvwQ0Vz1X/Dhg1q0KBBlsvr16+v9evX321ZuXb16lWNGTNGixYt0vHjx+Xp6anKlSvrlVdeUfPmzfNsP506ddKFCxe0dOnS2/Y9ceKE3njjDX399dc6duyYfH19Va1aNfXr108NGzaUdDMo69evn/r165dh/b/++kuhoaHauXOnqlWrZn2frlChQqpSpYrGjh2revXq5dUh2iCUAgAAAAAA94VatWopMTExQ/uyZcsUExOjnj173vG2k5OT5eLickfrxsTE6KefftJ7772nSpUq6ezZs9qyZYvOnj17x/Xcjb/++ku1a9dWoUKFNHHiRIWHhyslJUWrVq1Sr1699Ntvv93xtteuXavKlSvr1KlTGjp0qJo2bao9e/bYBFZ5hdv3AAAAAADAfcHFxUX+/v42r/Pnz2vQoEEaOnSonnnmGWvfffv2qWnTpipYsKD8/PzUvn17nTnzv1Fg9evXV+/evTVgwAAVLVpUUVFRkqSNGzfq4YcflqurqwICAjR48GDduHEj27r++9//WgOakJAQRURE6OWXX1bHjh1t+l27dk1dunSRp6enSpYsqZkzZ9os3717tx5//HG5u7vLx8dH3bt315UrVyRJo0aN0ty5c/XVV1/JYrHIYrFow4YNmdbTs2dPWSwW/fTTT2rdurXKlSunypUra8CAAfrxxx9zfL4z4+PjI39/f4WHh+vDDz/UtWvXtHr16rvaZlYIpQAAAAAAwH3pwoULatWqlerVq6exY8da2xMTE1WvXj1Vq1ZN27dv18qVK3Xy5Em1adPGZv25c+fKyclJ33//vT788EMdO3ZMTZs2VY0aNfTLL79oxowZmjVrlsaNG5dtHf7+/lqxYoUuX76cbb+3335bkZGR2rlzp3r27KmXXnrJOmrp2rVreuKJJ1S4cGFt27ZNixYt0tq1a9W7d29J0iuvvKI2bdroiSeeUGJiohITE1WrVq0M+zh37pxWrlypXr16ycPDI8PyQoUKZVtjbhQoUECSlJKSkmfb/Cdu3wMAAAAAAPedtLQ0Pf/883J0dNS8efNksVisy2bMmKHq1atr/Pjx1rbZs2crKChIBw4cULly5SRJZcqU0cSJE619hg0bpqCgIL333nuyWCyqUKGCjh8/rtdee00jRoyQg0PmY3dmzpypdu3aycfHR1WrVtVjjz2m1q1bq3bt2jb9mjZtar3F8LXXXtOUKVO0YcMGVahQQfPnz9f169f1ySefWMOk9957Ty1atNCbb74pPz8/ubu7KykpSf7+/lmelz/++EOGYahChQq5PKO5c/XqVQ0ZMkSOjo73bE4pRkoBAAAAAID7ztChQ/XDDz/oq6++kpeXl82yHTt2aP369SpYsKD1lR7SHDx40NovMjLSZr34+HjVrFnTJuCqXbu2rly5oqNHjyohIcFmm+mhV926dfXnn3/q22+/1dNPP629e/eqTp06NqO3JCk8PNz63xaLRf7+/jp16pR131WrVrUZ3VS7dm2lpaVp//79OT4vhmFYt38v1KpVSwULFpSnp6f++9//Ki4uTlWqVLkn+2KkFAAAAAAAuK8sXLhQkyZN0tdff62yZctmWJ6WlmYdYXSrgIAA63/fenubYRgZwpx/hjwBAQHatWuXdVmRIkWs/+3s7Kw6deqoTp06Gjx4sMaNG6cxY8botddes06g7uzsbLNti8WitLS0LPf9z345VbZsWVksFsXHx6tVq1Y5Xi+nFi5cqEqVKqlQoULy8fHJ8+3/E6EUAAAAAAC4b+zatUtdunTRhAkTFB0dnWmf6tWra8mSJQoJCZGTU86jjUqVKmnJkiU2AdGWLVvk6emp4sWLy8HBQWXKlMnxtm7cuKG///47R0/1q1SpkubOnaurV69aw7Lvv/9eDg4O1tsNXVxclJqamu12ihQpoujoaL3//vvq06dPhuDtwoULdzWvVFBQkEqXLn3H6+cGt+8BAAAAAID7wpkzZ9SqVSvVr19fL7zwgk6cOGHzOn36tCSpV69eOnfunJ577jn99NNP+vPPP7V69Wp16dIl21CnZ8+eOnLkiF5++WX99ttv+uqrrzRy5EgNGDAgy/mkpJtP8vvwww+1Y8cO/fXXX1qxYoWGDh2qBg0aZLi1MCvt2rWTm5ubOnbsqD179mj9+vV6+eWX1b59e/n5+UmSQkJC9Ouvv2r//v06c+ZMlhOMT58+XampqXr44Ye1ZMkS/f7774qPj9e7776rmjVr2vQ9duyYdu3aZfM6d+5cjmq+1xgpBQAAAAAA7gtff/21Dh8+rMOHD9vchpcuODhYf/31lwIDA/X999/rtddeU3R0tJKSkhQcHKwnnngi23CpePHiWrFihQYNGqSqVauqSJEi6tq1q15//fVs64qOjtbcuXM1dOhQXbt2TYGBgWrevLlGjBiR42MrUKCAVq1apb59+6pGjRoqUKCAnn76aU2ePNna58UXX9SGDRsUGRmpK1euaP369apfv36GbYWGhurnn3/WG2+8oYEDByoxMVHFihVTRESEZsyYYdN30qRJmjRpkk3bnDlzMt2u2SxG+s2TD4hLly7J29tbFy9ezHGaCQAAAADAv8nff/+tQ4cOKTQ0VG5ubvYuB/lQdp+xnGYv3L4HAAAAAAAA0xFKAQAAAAAAwHSEUgAAAAAAADAdoRQAAAAAAABMRygFAAAAAAAA0xFKAQAAAACQT6Wlpdm7BORTefHZcsqDOgAAAAAAwH3ExcVFDg4OOn78uIoVKyYXFxdZLBZ7l4V8wDAMJScn6/Tp03JwcJCLi8sdb4tQCgAAAACAfMbBwUGhoaFKTEzU8ePH7V0O8qECBQqoZMmScnC485vwCKUAAAAAAMiHXFxcVLJkSd24cUOpqan2Lgf5iKOjo5ycnO569B2hFAAAAAAA+ZTFYpGzs7OcnZ3tXQqQAROdAwAAAAAAwHSEUgAAAAAAADAdoRQAAAAAAABMRygFAAAAAAAA0xFKAQAAAAAAwHSEUgAAAAAAADAdoRQAAAAAAABMRygFAAAAAAAA0xFKAQAAAAAAwHSEUgAAAAAAADAdoRQAAAAAAABMRygFAAAAAAAA0xFKAQAAAAAAwHSEUgAAAAAAADAdoRQAAAAAAABMRygFAAAAAAAA0xFKAQAAAAAAwHSEUgAAAAAAADAdoRQAAAAAAABMRygFAAAAAAAA0xFKAQAAAAAAwHSEUgAAAAAAADAdoRQAAAAAAABMRygFAAAAAAAA0xFKAQAAAAAAwHSEUgAAAAAAADAdoRQAAAAAAABMRygFAAAAAAAA0xFKAQAAAAAAwHSEUgAAAAAAADAdoRQAAAAAAABMRygFAAAAAAAA0xFKAQAAAAAAwHSEUgAAAAAAADAdoRQAAAAAAABMRygFAAAAAAAA0xFKAQAAAAAAwHSEUgAAAAAAADAdoRQAAAAAAABMRygFAAAAAAAA0xFKAQAAAAAAwHSEUgAAAAAAADAdoRQAAAAAAABMRygFAAAAAAAA0xFKAQAAAAAAwHSEUgAAAAAAADAdoRQAAAAAAABMRygFAAAAAAAA0xFKAQAAAAAAwHSEUgAAAAAAADAdoRQAAAAAAABMRygFAAAAAAAA0xFKAQAAAAAAwHSEUgAAAAAAADAdoRQAAAAAAABMRygFAAAAAAAA0xFKAQAAAAAAwHSEUgAAAAAAADAdoRQAAAAAAABMRygFAAAAAAAA0xFKAQAAAAAAwHSEUgAAAAAAADAdoRQAAAAAAABMRygFAAAAAAAA0xFKAQAAAAAAwHSEUgAAAAAAADAdoRQAAAAAAABMRygFAAAAAAAA0xFKAQAAAAAAwHSEUgAAAAAAADAdoRQAAAAAAABMRygFAAAAAAAA0xFKAQAAAAAAwHSEUgAAAAAAADAdoRQAAAAAAABMRygFAAAAAAAA0xFKAQAAAAAAwHSEUgAAAAAAADAdoRQAAAAAAABMRygFAAAAAAAA0xFKAQAAAAAAwHSEUgAAAAAAADAdoRQAAAAAAABMRygFAAAAAAAA0xFKAQAA3IWQkBBZLJYMr169ekmSrly5ot69e6tEiRJyd3dXxYoVNWPGjGy3+dFHH6lOnToqXLiwChcurEaNGumnn37Ksn9sbKwsFov69etn0z5p0iT5+fnJz89PU6ZMsVm2detWRUREKDU19c4OHAAA4C452bsAAACAf7Nt27bZBDt79uxRVFSUnnnmGUlS//79tX79es2bN08hISFavXq1evbsqcDAQLVs2TLTbW7YsEHPPfecatWqJTc3N02cOFGNGzfW3r17Vbx48Qz7nzlzpsLDw23ad+/erREjRmj58uUyDEPNmzdXVFSUwsLClJKSopiYGM2cOVOOjo55fEYAAAByxu4jpaZPn67Q0FC5ubkpIiJCmzdvzrb//PnzVbVqVRUoUEABAQHq3Lmzzp49a1K1AAAAtooVKyZ/f3/ra/ny5SpdurTq1asnSfrhhx/UsWNH1a9fXyEhIerevbuqVq2q7du3Z7nN+fPnq2fPnqpWrZoqVKigjz76SGlpafr2229t+l25ckXt2rXTRx99pMKFC9ssi4+PV3h4uB5//HE1bNhQ4eHhio+PlyS99dZbqlu3rmrUqJHHZwMAACDn7BpKLVy4UP369dOwYcO0c+dO1alTR02aNFFCQkKm/b/77jt16NBBXbt21d69e7Vo0SJt27ZN3bp1M7lyAACAjJKTkzVv3jx16dJFFotFkvTYY49p2bJlOnbsmAzD0Pr163XgwAFFR0fneLvXrl1TSkqKihQpYtPeq1cvNWvWTI0aNcqwTpUqVXTgwAElJCTo8OHDOnDggMLCwvTHH38oLi5O48aNu7uDBQAAuEt2DaUmT56srl27qlu3bqpYsaKmTp2qoKCgLOdZ+PHHHxUSEqI+ffooNDRUjz32mHr06JHtXxoBAADMsnTpUl24cEGdOnWytr377ruqVKmSSpQoIRcXFz3xxBOaPn26HnvssRxvd/DgwSpevLhN+LRgwQL9/PPPio2NzXSdihUravz48YqKilLjxo0VGxurihUrKiYmRhMnTtSqVasUFhamhx56SJs2bbrjYwYAALhTdptTKjk5WTt27NDgwYNt2hs3bqwtW7Zkuk6tWrU0bNgwrVixQk2aNNGpU6e0ePFiNWvWzIySAQAAsjVr1iw1adJEgYGB1rZ3331XP/74o5YtW6bg4GBt2rRJPXv2VEBAQKYjnG41ceJEff7559qwYYPc3NwkSUeOHFHfvn21evVqa1tmYmJiFBMTY30fFxcnT09P1axZU+XLl9e2bdt09OhRtW3bVocOHZKrq+tdHD0AAEDu2C2UOnPmjFJTU+Xn52fT7ufnpxMnTmS6Tq1atTR//nw9++yz+vvvv3Xjxg09+eSTmjZtWpb7SUpKUlJSkvX9pUuX8uYAAAAA/uHw4cNau3atvvjiC2vb9evXNXToUH355ZfWP6KFh4dr165dmjRp0m1DqUmTJmn8+PFau3atzUTmO3bs0KlTpxQREWFtS01N1aZNm/Tee+8pKSkpwwTmZ86c0ZgxY7Rp0yZt3bpV5cqVU9myZVW2bFmlpKTowIEDqlKlSl6cCgAAgByx+0Tn6fMtpDMMI0Nbun379qlPnz4aMWKEduzYoZUrV+rQoUM2fwG8VWxsrLy9va2voKCgPK0fAABAkubMmSNfX1+bEdwpKSlKSUmRg4PtP7kcHR2VlpaW7fbeeustjR07VitXrlRkZKTNsoYNG2r37t3atWuX9RUZGal27dpp165dmT5Rr1+/furfv79KlCih1NRUpaSkWJfduHHD5gmCAAAAZrDbSKmiRYvK0dExw6ioU6dOZRg9lS42Nla1a9fWoEGDJN38S6OHh4fq1KmjcePGKSAgIMM6Q4YM0YABA6zvL126RDAFAADyVFpamubMmaOOHTvKyel//7zy8vJSvXr1NGjQILm7uys4OFgbN27UJ598osmTJ1v7dejQQcWLF7fODzVx4kQNHz5cn332mUJCQqz/XipYsKAKFiwoT09PhYWF2dTg4eEhHx+fDO2StGbNGv3+++/65JNPJEkPP/ywfvvtN33zzTc6cuSIHB0dVb58+Tw/LwAAANmxWyjl4uKiiIgIrVmzRk899ZS1fc2aNWrZsmWm61y7ds3mH3qSrH8JNAwj03VcXV2ZHwEAANxTa9euVUJCgrp06ZJh2YIFCzRkyBC1a9dO586dU3BwsN544w2bkd4JCQk2o6mmT5+u5ORktW7d2mZbI0eO1KhRo3JV2/Xr19W7d28tXLjQuo/ixYtr2rRp6ty5s1xdXTV37ly5u7vnarsAAAB3y2JkleaYYOHChWrfvr0++OAD1axZUzNnztRHH32kvXv3Kjg4WEOGDNGxY8esf9WLi4vTiy++qHfffVfR0dFKTExUv3795ODgoK1bt+Zon5cuXZK3t7cuXrwoLy+ve3l4AAAAAAAAD5ycZi92GyklSc8++6zOnj2rMWPGKDExUWFhYVqxYoWCg4MlSYmJiUpISLD279Spky5fvqz33ntPAwcOVKFChfT444/rzTfftNchAAAAAAAA4A7YdaSUPTBSCgAAAAAA4N7JafZi96fvAQAAAAAA4MFDKAUAAAAAAADTEUoBAAAAAADAdIRSAAAAAAAAMB2hFAAAAAAAAEznZO8CAADAg2nCzjP2LgH51OCHitq7BAAAkAOMlAIAAAAAAIDpCKUAAAAAAABgOkIpAAAAAAAAmI5QCgAAAAAAAKYjlAIAAAAA4AEUEhIii8WS4dWrV68MfXv06CGLxaKpU6fedrsXLlxQr169FBAQIDc3N1WsWFErVqywLo+NjVWNGjXk6ekpX19ftWrVSvv377fZxqRJk+Tn5yc/Pz9NmTLFZtnWrVsVERGh1NTUOztw3Dd4+h4AAAAAAA+gbdu22QQ7e/bsUVRUlJ555hmbfkuXLtXWrVsVGBh4220mJycrKipKvr6+Wrx4sUqUKKEjR47I09PT2mfjxo3q1auXatSooRs3bmjYsGFq3Lix9u3bJw8PD+3evVsjRozQ8uXLZRiGmjdvrqioKIWFhSklJUUxMTGaOXOmHB0d8+5kwC4IpQAAAAAAeAAVK1bM5v2ECRNUunRp1atXz9p27Ngx9e7dW6tWrVKzZs1uu83Zs2fr3Llz2rJli5ydnSVJwcHBNn1Wrlxp837OnDny9fXVjh07VLduXcXHxys8PFyPP/64JCk8PFzx8fEKCwvTW2+9pbp166pGjRp3dMy4v3D7HgAAAAAAD7jk5GTNmzdPXbp0kcVikSSlpaWpffv2GjRokCpXrpyj7Sxbtkw1a9ZUr1695Ofnp7CwMI0fPz7bW+0uXrwoSSpSpIgkqUqVKjpw4IASEhJ0+PBhHThwQGFhYfrjjz8UFxencePG3eXR4n5BKAUAAAAAwANu6dKlunDhgjp16mRte/PNN+Xk5KQ+ffrkeDt//vmnFi9erNTUVK1YsUKvv/663n77bb3xxhuZ9jcMQwMGDNBjjz2msLAwSVLFihU1fvx4RUVFqXHjxoqNjVXFihUVExOjiRMnatWqVQoLC9NDDz2kTZs23dVxw764fQ8AAAAAgAfcrFmz1KRJE+u8UTt27NA777yjn3/+2TpyKifS0tLk6+trnfMpIiJCx48f11tvvaURI0Zk6N+7d2/9+uuv+u6772zaY2JiFBMTY30fFxcnT09P1axZU+XLl9e2bdt09OhRtW3bVocOHZKrq+sdHjnsiVAKAAAAAIAH2OHDh7V27Vp98cUX1rbNmzfr1KlTKlmypLUtNTVVAwcO1NSpU/XXX39luq2AgAA5OzvbTEJesWJFnThxQsnJyXJxcbG2v/zyy1q2bJk2bdqkEiVKZFnfmTNnNGbMGG3atElbt25VuXLlVLZsWZUtW1YpKSk6cOCAqlSpchdnAPbC7XsAAAAAADzA0ica/+dE5u3bt9evv/6qXbt2WV+BgYEaNGiQVq1aleW2ateurT/++ENpaWnWtgMHDiggIMAaSBmGod69e+uLL77QunXrFBoamm19/fr1U//+/VWiRAmlpqYqJSXFuuzGjRvZzleF+xsjpQAAAAAAeEClpaVpzpw56tixo5yc/hcR+Pj4yMfHx6avs7Oz/P39Vb58eWtbhw4dVLx4ccXGxkqSXnrpJU2bNk19+/bVyy+/rN9//13jx4+3mZeqV69e+uyzz/TVV1/J09NTJ06ckCR5e3vL3d3dZp9r1qzR77//rk8++USS9PDDD+u3337TN998oyNHjsjR0dGmHvy7EEoBAAAAAPCAWrt2rRISEtSlS5c7Wj8hIUEODv+7CSsoKEirV69W//79FR4eruLFi6tv37567bXXrH1mzJghSapfv77NtubMmWMz0fr169fVu3dvLVy40LqP4sWLa9q0aercubNcXV01d+7cDEEW/j0shmEY9i7CTJcuXZK3t7cuXrwoLy8ve5cDAMADa8LOM/YuAfnU4IeK2rsEAAAeaDnNXphTCgAAAAAAAKYjlAIAAAAAAIDpCKUAAAAAAABgOkIpAAAAAAAAmI5QCgAAAAAAAKYjlAIAAAAAAIDpnOxdAAAAAAAA/yYTdp6xdwnIpwY/VNTeJZiKkVIAAAAAAAAwHaEUAAAAAAAATEcoBQAAAAAAANMRSuG+ERISIovFkuHVq1cvSdIXX3yh6OhoFS1aVBaLRbt27brtNr/44gtFRkaqUKFC8vDwULVq1fTpp5/a9JkxY4bCw8Pl5eUlLy8v1axZU998841Nn0mTJsnPz09+fn6aMmWKzbKtW7cqIiJCqampd3cCAAAAAAB4gDDROe4b27Ztswl29uzZo6ioKD3zzDOSpKtXr6p27dp65pln9OKLL+Zom0WKFNGwYcNUoUIFubi4aPny5ercubN8fX0VHR0tSSpRooQmTJigMmXKSJLmzp2rli1baufOnapcubJ2796tESNGaPny5TIMQ82bN1dUVJTCwsKUkpKimJgYzZw5U46Ojnl8RgAAAAAAyL8IpXDfKFasmM37CRMmqHTp0qpXr54kqX379pKkv/76K8fbrF+/vs37vn37au7cufruu++soVSLFi1s+rzxxhuaMWOGfvzxR1WuXFnx8fEKDw/X448/LkkKDw9XfHy8wsLC9NZbb6lu3bqqUaNGbg4VAAAAAIAHHqEU7kvJycmaN2+eBgwYIIvFkifbNAxD69at0/79+/Xmm29m2ic1NVWLFi3S1atXVbNmTUlSlSpVdODAASUkJMgwDB04cEBhYWH6448/FBcXpx07duRJfQAAAAAAPEgIpXBfWrp0qS5cuKBOnTrd9bYuXryo4sWLKykpSY6Ojpo+fbqioqJs+uzevVs1a9bU33//rYIFC+rLL79UpUqVJEkVK1bU+PHjrevExsaqYsWKatSokSZOnKhVq1Zp1KhRcnZ21jvvvKO6devedc0AAAAAAOR3hFK4L82aNUtNmjRRYGDgXW/L09NTu3bt0pUrV/Ttt99qwIABKlWqlM2tfeXLl9euXbt04cIFLVmyRB07dtTGjRutwVRMTIxiYmKs/ePi4uTp6amaNWuqfPny2rZtm44ePaq2bdvq0KFDcnV1veu6AQAAAADIzwilcN85fPiw1q5dqy+++CJPtufg4GCdxLxatWqKj49XbGysTSjl4uJi7RMZGalt27bpnXfe0Ycffphhe2fOnNGYMWO0adMmbd26VeXKlVPZsmVVtmxZpaSk6MCBA6pSpUqe1A4AAAAAQH7lYO8CgFvNmTNHvr6+atas2T3ZvmEYSkpKuuM+/fr1U//+/VWiRAmlpqYqJSXFuuzGjRs2TxAEAAAAAACZY6QU7itpaWmaM2eOOnbsKCcn24/nuXPnlJCQoOPHj0uS9u/fL0ny9/eXv7+/JKlDhw4qXry4YmNjJd2c/ykyMlKlS5dWcnKyVqxYoU8++UQzZsywbnfo0KFq0qSJgoKCdPnyZS1YsEAbNmzQypUrM9S3Zs0a/f777/rkk08kSQ8//LB+++03ffPNNzpy5IgcHR1Vvnz5vD8xAAAAAADkM4RSuK+sXbtWCQkJ6tKlS4Zly5YtU+fOna3v27ZtK0kaOXKkRo0aJUlKSEiQg8P/BgBevXpVPXv21NGjR+Xu7q4KFSpo3rx5evbZZ619Tp48qfbt2ysxMVHe3t4KDw/XypUrM0yGfv36dfXu3VsLFy607qN48eKaNm2aOnfuLFdXV82dO1fu7u55dj4AAAAAAMivLIZhGPYuwkyXLl2St7e3Ll68KC8vL3uXAwDAA2vCzjP2LgH51OCHitq7BAD5HNcw3Cv55RqW0+yFOaUAAAAAAABgOkIpAAAAAAAAmI5QCgAAAAAAAKYjlAIAAAAAAIDpCKUAAAAAAABgOkIpAAAAAAAAmM7J3gXg7vAoUtxL+eVxpAAAAACA+w8jpQAAAAAAAGA6QikAAAAAAACYjlAKAAAAAAAApiOUAgAAAAAAgOkIpQAAAAAAAGA6QikAAAAAAACYjlAKAAAAAAAApiOUAgAAAAAAgOkIpQAAAAAAAGA6QikAAAAAAACYjlAKAAAAAAAApiOUAgAAAAAAgOkIpQAAAAAAAGA6QikAAAAAAACYjlAKAAAAAAAApiOUAgAAAAAAgOkIpQAAAAAAAGA6QikAAAAAAACYjlAKAAAAAAAApiOUAgAAAAAAgOkIpQAAAAAAAGA6QikAAAAAAACYjlAKAAAAAAAApiOUAgAAAAAAgOkIpQAAAAAAAGA6QikAAAAAAACYjlAKAAAAAAAApiOUAgAAAAAAgOkIpQAAAAAAAGA6QikAAAAAAACYjlAKAAAAAAAApiOUAgAAAAAAgOkIpQAAAAAAAGA6QikAAAAAAACYjlAKAAAAAAAApiOUAgAAAAAAgOkIpQAAAAAAAGA6QikAAAAAAACYjlAKAAAAAAAApiOUAgAAAAAAgOkIpQAAAAAAAGA6QikAAAAAAACYjlAKAAAAAAAApiOUAgAAAAAAgOkIpQAAAAAAAGA6QikAAAAAAACYjlAKAAAAAAAApiOUAgAAAAAAgOkIpQAAAAAAAGA6QikAAAAAAACYjlAKAAAAAAAApiOUAgAAAAAAgOkIpQAAAAAAAGA6QikAAAAAAACYjlAKAAAAAAAApiOUAgAAAAAAgOkIpQAAAAAAAGA6QikAAAAAAACYjlAKAAAAAAAApiOUAgAAAAAAgOkIpQAAAAAAAGA6QikAAAAAAACYjlAKAAAAAAAApiOUAgAAAAAAgOkIpQAAAAAAAGA6u4dS06dPV2hoqNzc3BQREaHNmzdn2z8pKUnDhg1TcHCwXF1dVbp0ac2ePdukagEAAAAAAJAXnOy584ULF6pfv36aPn26ateurQ8//FBNmjTRvn37VLJkyUzXadOmjU6ePKlZs2apTJkyOnXqlG7cuGFy5QAAAAAAALgbdg2lJk+erK5du6pbt26SpKlTp2rVqlWaMWOGYmNjM/RfuXKlNm7cqD///FNFihSRJIWEhJhZMgAAAAAAAPKA3W7fS05O1o4dO9S4cWOb9saNG2vLli2ZrrNs2TJFRkZq4sSJKl68uMqVK6dXXnlF169fN6NkAAAAAAAA5BG7jZQ6c+aMUlNT5efnZ9Pu5+enEydOZLrOn3/+qe+++05ubm768ssvdebMGfXs2VPnzp3Lcl6ppKQkJSUlWd9funQp7w4CAAAAAAAAd8TuE51bLBab94ZhZGhLl5aWJovFovnz5+vhhx9W06ZNNXnyZMXFxWU5Wio2Nlbe3t7WV1BQUJ4fAwAAAAAAAHLHbqFU0aJF5ejomGFU1KlTpzKMnkoXEBCg4sWLy9vb29pWsWJFGYaho0ePZrrOkCFDdPHiRevryJEjeXcQAAAAAAAAuCN2C6VcXFwUERGhNWvW2LSvWbNGtWrVynSd2rVr6/jx47py5Yq17cCBA3JwcFCJEiUyXcfV1VVeXl42LwAAAAAAANiXXW/fGzBggD7++GPNnj1b8fHx6t+/vxISEhQTEyPp5iinDh06WPs///zz8vHxUefOnbVv3z5t2rRJgwYNUpcuXeTu7m6vwwAAAAAAAEAu2W2ic0l69tlndfbsWY0ZM0aJiYkKCwvTihUrFBwcLElKTExUQkKCtX/BggW1Zs0avfzyy4qMjJSPj4/atGmjcePG2esQAAAAAAAAcAfsGkpJUs+ePdWzZ89Ml8XFxWVoq1ChQoZb/gAAAAAAAPDvYven7wEAAAAAAODBQygFAAAAAAAA0xFKAQAAAAAAwHSEUgAAAAAAADAdoRQAAAAAAABMRygFAAAAAAAA0xFKAQAAAAAAwHSEUgAAAAAAADAdoRQAAAAAAABMRygFAAAAAAAA0xFKAQAAAAAAwHSEUgAAAAAAADAdoRQAAAAAAABMRygFAAAAAAAA0xFKAQAAAAAAwHSEUgAAAAAAADAdoRQAAAAAAABMRygFAAAAAAAA0xFKAQAAAAAAwHSEUgAAAAAAADAdoRQAAAAAAABMRygFAAAAAAAA0xFKAQAAAAAAwHSEUgAAAAAAADAdoRQAAAAAAABMRygFAAAAAAAA0xFKAQAAAAAAwHS5DqVCQkI0ZswYJSQk3It6AAAAAAAA8ADIdSg1cOBAffXVVypVqpSioqK0YMECJSUl3YvaAAAAAAAAkE/lOpR6+eWXtWPHDu3YsUOVKlVSnz59FBAQoN69e+vnn3++FzUCAAAAAAAgn7njOaWqVq2qd955R8eOHdPIkSP18ccfq0aNGqpatapmz54twzDysk4AAAAAAADkI053umJKSoq+/PJLzZkzR2vWrNGjjz6qrl276vjx4xo2bJjWrl2rzz77LC9rBQAAAAAAQD6R61Dq559/1pw5c/T555/L0dFR7du315QpU1ShQgVrn8aNG6tu3bp5WigAAAAAAADyj1yHUjVq1FBUVJRmzJihVq1aydnZOUOfSpUqqW3btnlSIAAAAAAAAPKfXIdSf/75p4KDg7Pt4+HhoTlz5txxUQAAAAAAAMjfcj3R+alTp7R169YM7Vu3btX27dvzpCgAAAAAAADkb7kOpXr16qUjR45kaD927Jh69eqVJ0UBAAAAAAAgf8t1KLVv3z5Vr149Q/tDDz2kffv25UlRAAAAAAAAyN9yHUq5urrq5MmTGdoTExPl5JTrKaoAAAAAAADwAMp1KBUVFaUhQ4bo4sWL1rYLFy5o6NChioqKytPiAAAAAAAAkD/lemjT22+/rbp16yo4OFgPPfSQJGnXrl3y8/PTp59+mucFAgAAAAAAIP/JdShVvHhx/frrr5o/f75++eUXubu7q3Pnznruuefk7Ox8L2oEAAAAAABAPnNHk0B5eHioe/fueV0LAAAAAAAAHhB3PDP5vn37lJCQoOTkZJv2J5988q6LAgAAAAAAQP6W61Dqzz//1FNPPaXdu3fLYrHIMAxJksVikSSlpqbmbYUAAAAAAADId3L99L2+ffsqNDRUJ0+eVIECBbR3715t2rRJkZGR2rBhwz0oEQAAAAAAAPlNrkdK/fDDD1q3bp2KFSsmBwcHOTg46LHHHlNsbKz69OmjnTt33os6AQAAAAAAkI/keqRUamqqChYsKEkqWrSojh8/LkkKDg7W/v3787Y6AAAAAAAA5Eu5HikVFhamX3/9VaVKldIjjzyiiRMnysXFRTNnzlSpUqXuRY0AAAAAAADIZ3IdSr3++uu6evWqJGncuHFq3ry56tSpIx8fHy1cuDDPCwQAAAAAAED+k+tQKjo62vrfpUqV0r59+3Tu3DkVLlzY+gQ+AAAAAAAAIDu5mlPqxo0bcnJy0p49e2zaixQpQiAFAAAAAACAHMtVKOXk5KTg4GClpqbeq3oAAAAAAADwAMj10/def/11DRkyROfOnbsX9QAAAAAAAOABkOs5pd5991398ccfCgwMVHBwsDw8PGyW//zzz3lWHAAAAAAAAPKnXIdSrVq1ugdlAAAAAAAA4EGS61Bq5MiR96IOAAAAAAAAPEByPacUAAAAAAAAcLdyPVLKwcFBFosly+U8mQ8AAAAAAAC3k+tQ6ssvv7R5n5KSop07d2ru3LkaPXp0nhUGAAAAAACA/CvXoVTLli0ztLVu3VqVK1fWwoUL1bVr1zwpDAAAAAAAAPlXns0p9cgjj2jt2rV5tTkAAAAAAADkY3kSSl2/fl3Tpk1TiRIl8mJzAAAAAAAAyOdyffte4cKFbSY6NwxDly9fVoECBTRv3rw8LQ4AAAAAAAD5U65DqSlTptiEUg4ODipWrJgeeeQRFS5cOE+LAwAAAAAAQP6U61CqU6dO96AMAAAAAAAAPEhyPafUnDlztGjRogztixYt0ty5c/OkKAAAAAAAAORvuQ6lJkyYoKJFi2Zo9/X11fjx4/OkKAAAAAAAAORvuQ6lDh8+rNDQ0AztwcHBSkhIyJOiAAAAAAAAkL/lOpTy9fXVr7/+mqH9l19+kY+PT54UBQAAAAAAgPwt16FU27Zt1adPH61fv16pqalKTU3VunXr1LdvX7Vt2/Ze1AgAAAAAAIB8JtdP3xs3bpwOHz6shg0bysnp5uppaWnq0KEDc0oBAAAAAAAgR3IdSrm4uGjhwoUaN26cdu3aJXd3d1WpUkXBwcH3oj4AAAAAAADkQ7kOpdKVLVtWZcuWzctaAAAAAAAA8IDI9ZxSrVu31oQJEzK0v/XWW3rmmWfypCgAAAAAAADkb7kOpTZu3KhmzZplaH/iiSe0adOmPCkKAAAAAAAA+VuuQ6krV67IxcUlQ7uzs7MuXbqUJ0UBAAAAAAAgf8t1KBUWFqaFCxdmaF+wYIEqVaqUJ0UBAAAAAAAgf8v1ROfDhw/X008/rYMHD+rxxx+XJH377bf67LPPtHjx4jwvEAAAAAAAAPlPrkOpJ598UkuXLtX48eO1ePFiubu7q2rVqlq3bp28vLzuRY0AAAAAAADIZ3IdSklSs2bNrJOdX7hwQfPnz1e/fv30yy+/KDU1NU8LBAAAAAAAQP6T6zml0q1bt04vvPCCAgMD9d5776lp06bavn17XtYGAAAAAACAfCpXI6WOHj2quLg4zZ49W1evXlWbNm2UkpKiJUuWMMk5AAAAAAAAcizHI6WaNm2qSpUqad++fZo2bZqOHz+uadOm3cvaAAAAAAAAkE/leKTU6tWr1adPH7300ksqW7bsvawJAAAAAAAA+VyOR0pt3rxZly9fVmRkpB555BG99957On369L2sDQAAAAAAAPlUjkOpmjVr6qOPPlJiYqJ69OihBQsWqHjx4kpLS9OaNWt0+fLle1knAAAAAAAA8pFcP32vQIEC6tKli7777jvt3r1bAwcO1IQJE+Tr66snn3zyXtQIAAAAAACAfCbXodQ/lS9fXhMnTtTRo0f1+eef51VNAAAAAAAAyOfuKpRK5+joqFatWmnZsmV5sTkAAAAAAADkc3kSSgEAAAAAAAC5QSgFAAAAAAAA0xFKAQAAAAAAwHSEUgAAAAAAADAdoRQAAAAAAABMRygFAAAAAAAA0xFKAQAAAAAAwHSEUgAAAAAAADAdoRQAAAAAAABMRygFAAAAAAAA0xFKAQAAAAAAwHSEUgAAAAAAADAdoRQAAAAAAABMRygFAAAAAAAA0xFKAQAAAAAAwHSEUgAAAAAAADAdoRQAAAAAAABMRygFAAAAAAAA0xFKAQAAAAAAwHSEUgAAAAAAADAdoRQAAAAAAABMRygFAAAAAAAA0xFKAQAAAAAAwHSEUgAAAAAAADCd3UOp6dOnKzQ0VG5uboqIiNDmzZtztN73338vJycnVatW7d4WCAAAAAAAgDxn11Bq4cKF6tevn4YNG6adO3eqTp06atKkiRISErJd7+LFi+rQoYMaNmxoUqUAAAAAAADIS3YNpSZPnqyuXbuqW7duqlixoqZOnaqgoCDNmDEj2/V69Oih559/XjVr1jSpUgAAAAAAAOQlu4VSycnJ2rFjhxo3bmzT3rhxY23ZsiXL9ebMmaODBw9q5MiR97pEAAAAAAAA3CNO9trxmTNnlJqaKj8/P5t2Pz8/nThxItN1fv/9dw0ePFibN2+Wk1POSk9KSlJSUpL1/aVLl+68aAAAAAAAAOQJu090brFYbN4bhpGhTZJSU1P1/PPPa/To0SpXrlyOtx8bGytvb2/rKygo6K5rBgAAAAAAwN2xWyhVtGhROTo6ZhgVderUqQyjpyTp8uXL2r59u3r37i0nJyc5OTlpzJgx+uWXX+Tk5KR169Zlup8hQ4bo4sWL1teRI0fuyfEAAAAAAAAg5+x2+56Li4siIiK0Zs0aPfXUU9b2NWvWqGXLlhn6e3l5affu3TZt06dP17p167R48WKFhoZmuh9XV1e5urrmbfEAAAAAAAC4K3YLpSRpwIABat++vSIjI1WzZk3NnDlTCQkJiomJkXRzlNOxY8f0ySefyMHBQWFhYTbr+/r6ys3NLUM7AAAAAAAA7m92DaWeffZZnT17VmPGjFFiYqLCwsK0YsUKBQcHS5ISExOVkJBgzxIBAAAAAABwD1gMwzDsXYSZLl26JG9vb128eFFeXl72LueuTdh5xt4lIB8b/FBRe5cAIB/jGoZ7hesXgHuNaxjulfxyDctp9mL3p+8BAAAAAADgwUMoBQAAAAAAANMRSgEAAAAAAMB0hFIAAAAAAAAwHaEUAAAAAAAATEcoBQAAAAAAANMRSgEAAAAAAMB0hFIAAAAAAAAwHaEUAAAAAAAATEcoBQAAAAAAANMRSgEAAAAAAMB0hFIAAAAAAAAwHaEUAAAAAAAATEcoBQAAAAAAANMRSgEAAAAAAMB0hFIAAAAAAAAwHaEUAAAAAAAATEcoBQAAAAAAANMRSgEAAAAAAMB0hFIAAAAAAAAwHaEUAAAAAAAATEcoBQAAAAAAANMRSgEAAAAAAMB0hFIAAAAAAAAwHaEUAAAAAAAATEcoBQAAAAAAANMRSgEAAAAAAMB0hFIAAAAAAAAwHaEUAAAAAAAATEcoBQAAAAAAANMRSgEAAAAAAMB0hFIAAAAAAAAwHaEUAAAAAAAATEcoBQAAAAAAANMRSgEAAAAAAMB0hFIAAAAAAAAwHaEUAAAAAAAATEcoBQAAAAAAANMRSgEAAAAAAMB0hFIAAAAAAAAwHaEUAAAAAAAATEcoBQAAAAAAANMRSgEAAAAAAMB0hFIAAAAAAAAwHaEUAAAAAAAATEcoBQAAAAAAANMRSgEAAAAAAMB0hFIAAAAAAAAwHaEUAAAAAAAATEcoBQAAAAAAANMRSgEAAAAAAMB0hFIAAAAAAAAwHaEUAAAAAAAATEcoBQAAAAAAANMRSgEAAAAAAMB0hFIAAAAAAAAwHaEUAAAAAAAATEcoBQAAAAAAANMRSgEAAAAAAMB0hFIAAAAAAAAwHaEUAAAAAAAATEcoBQAAAAAAANMRSgEAAAAAAMB0hFIAAAAAAAAwHaEUAAAAAAAATEcoBQAAAAAAANMRSgEAAAAAAMB0hFIAAAAAAAAwHaEUAAAAAAAATEcoBQAAAAAAANMRSgEAAAAAAMB0hFIAAAAAAAAwHaEUAAAAAAAATEcoBQAAAAAAANMRSgEAAAAAAMB0hFIAAAAAAAAwHaEUAAAAAAAATEcoBQAAAAAAANMRSgEAAAAAAMB0hFIAAAAAAAAwHaEUAAAAAAAATEcoBQAAAAAAANMRSgEAAAAAAMB0hFIAAAAAAAAwHaEUAAAAAAAATEcoBQAAAAAAANMRSgEAAAAAAMB0hFIAAAAAAAAwHaEUAAAAAAAATEcoBQAAAAAAANMRSgEAAAAAAMB0hFIAAAAAAAAwHaEUAAAAAAAATEcoBQAAAAAAANMRSgEAAAAAAMB0hFIAAAAAAAAwHaEUAAAAAAAATEcoBQAAAAAAANMRSgEAAAAAAMB0hFIAAAAAAAAwHaEUAAAAAAAATEcoBQAAAAAAANMRSgEAAAAAAMB0hFIAAAAAAAAwHaEUAAAAAAAATEcoBQAAAAAAANMRSgEAAAAAAMB0hFIAAAAAAAAwHaEUAAAAAAAATEcoBQAAAAAAANMRSgEAAAAAAMB0hFIAAAAAAAAwHaEUAAAAAAAATEcoBQAAAAAAANMRSgEAAAAAAMB0dg+lpk+frtDQULm5uSkiIkKbN2/Osu8XX3yhqKgoFStWTF5eXqpZs6ZWrVplYrUAAAAAAADIC3YNpRYuXKh+/fpp2LBh2rlzp+rUqaMmTZooISEh0/6bNm1SVFSUVqxYoR07dqhBgwZq0aKFdu7caXLlAAAAAAAAuBt2DaUmT56srl27qlu3bqpYsaKmTp2qoKAgzZgxI9P+U6dO1auvvqoaNWqobNmyGj9+vMqWLav//ve/JlcOAAAAAACAu2G3UCo5OVk7duxQ48aNbdobN26sLVu25GgbaWlpunz5sooUKZJln6SkJF26dMnmBQAAAAAAAPuyWyh15swZpaamys/Pz6bdz89PJ06cyNE23n77bV29elVt2rTJsk9sbKy8vb2tr6CgoLuqGwAAAAAAAHfP7hOdWywWm/eGYWRoy8znn3+uUaNGaeHChfL19c2y35AhQ3Tx4kXr68iRI3ddMwAAAAAAAO6Ok712XLRoUTk6OmYYFXXq1KkMo6dutXDhQnXt2lWLFi1So0aNsu3r6uoqV1fXu64XAAAAAAAAecduI6VcXFwUERGhNWvW2LSvWbNGtWrVynK9zz//XJ06ddJnn32mZs2a3esyAQAAAAAAcA/YbaSUJA0YMEDt27dXZGSkatasqZkzZyohIUExMTGSbt56d+zYMX3yySeSbgZSHTp00DvvvKNHH33UOsrK3d1d3t7edjsOAAAAAAAA5I5dQ6lnn31WZ8+e1ZgxY5SYmKiwsDCtWLFCwcHBkqTExEQlJCRY+3/44Ye6ceOGevXqpV69elnbO3bsqLi4OLPLBwAAAAAAwB2yayglST179lTPnj0zXXZr0LRhw4Z7XxAAAAAAAADuObs/fQ8AAAAAAAAPHkIpAAAAAAAAmI5QCgAAAAAAAKYjlAIAAAAAAIDpCKUAAAAAAABgOkIpAAAAAAAAmI5QCgAAAAAAAKYjlAIAAAAAAIDpCKUAAAAAAABgOkIpAAAAAAAAmI5QCgAAAAAAAKYjlAIAAAAAAIDpCKUAAAAAAABgOkIpAAAAAAAAmI5QCgAAAAAAAKYjlAIAAAAAAIDpCKUAAAAAAABgOkIpAAAAAAAAmI5QCgAAAAAAAKYjlAIAAAAAAIDpCKUAAAAAAABgOkIpAAAAAAAAmI5QCgAAAAAAAKYjlAIAAAAAAIDpCKUAAAAAAABgOkIpAAAAAAAAmI5QCgAAAAAAAKYjlAIAAAAAAIDpCKUAAAAAAABgOkIpAAAAAAAAmI5QCgAAAAAAAKYjlAIAAAAAAIDpCKUAAAAAAABgOkIpAAAAAAAAmI5QCgAAAAAAAKYjlAIAAAAAAIDpCKUAAAAAAABgOkIpAAAAAAAAmI5QCgAAAAAAAKYjlAIAAAAAAIDpCKUAAAAAAABgOkIpAAAAAAAAmI5QCgAAAAAAAKYjlAIAAAAAAIDpCKUAAAAAAABgOkIpAAAAAAAAmI5QCgAAAAAAAKYjlAIAAAAAAIDpCKUAAAAAAABgOkIpAAAAAAAAmI5QCgAAAAAAAKYjlAIAAAAAAIDpCKUAAAAAAABgOkIpAAAAAAAAmI5QCgAAAAAAAKYjlAIAAAAAAIDpCKUAAAAAAABgOkIpAAAAAAAAmI5QCgAAAAAAAKYjlAIAAAAAAIDpCKUAAAAAAABgOkIpAAAAAAAAmI5QCgAAAAAAAKYjlAIAAAAAAIDpCKUAAAAAAABgOkIpAAAAAAAAmI5QCgAAAAAAAKYjlAIAAAAAAIDpCKUAAAAAAABgOkIpAAAAAAAAmI5QCgAAAAAAAKYjlAIAAAAAAIDpCKUAAAAAAABgOkIpAAAAAAAAmI5QCgAAAAAAAKYjlAIAAAAAAIDpCKUAAAAAAABgOkIpAAAAAAAAmI5QCgAAAAAAAKYjlAIAAAAAAIDpCKUAAAAAAABgOkIpAAAAAAAAmI5QCgAAAAAAAKYjlAIAAAAAAIDpCKUAAAAAAABgOkIpAAAAAAAAmI5QCgAAAAAAAKYjlAIAAAAAAIDpCKUAAAAAAABgOkIpAAAAAAAAmI5QCgAAAAAAAKYjlAIAAAAAAIDpCKUAAAAAAABgOkIpAAAAAAAAmI5QCgAAAAAAAKYjlAIAAAAAAIDpCKUAAAAAAABgOkIpAAAAAAAAmI5QCgAAAAAAAKYjlAIAAAAAAIDpCKUAAAAAAABgOkIpAAAAAAAAmI5QCgAAAAAAAKYjlAIAAAAAAIDpCKUAAAAAAABgOkIpAAAAAAAAmI5QCgAAAAAAAKYjlAIAAAAAAIDpCKUAAAAAAABgOkIpAAAAAAAAmI5QCgAAAAAAAKYjlAIAAAAAAIDpCKUAAAAAAABgOkIpAAAAAAAAmM7uodT06dMVGhoqNzc3RUREaPPmzdn237hxoyIiIuTm5qZSpUrpgw8+MKlSAAAAAAAA5BW7hlILFy5Uv379NGzYMO3cuVN16tRRkyZNlJCQkGn/Q4cOqWnTpqpTp4527typoUOHqk+fPlqyZInJlQMAAAAAAOBu2DWUmjx5srp27apu3bqpYsWKmjp1qoKCgjRjxoxM+3/wwQcqWbKkpk6dqooVK6pbt27q0qWLJk2aZHLlAAAAAAAAuBtO9tpxcnKyduzYocGDB9u0N27cWFu2bMl0nR9++EGNGze2aYuOjtasWbOUkpIiZ2fnDOskJSUpKSnJ+v7ixYuSpEuXLt3tIdwX/r5y2d4lIB+7dMnF3iUAyMe4huFe4foF4F7jGoZ7Jb9cw9IzF8Mwsu1nt1DqzJkzSk1NlZ+fn027n5+fTpw4kek6J06cyLT/jRs3dObMGQUEBGRYJzY2VqNHj87QHhQUdBfVAw+GjP/nAABw/+P6BQD4t8pv17DLly/L29s7y+V2C6XSWSwWm/eGYWRou13/zNrTDRkyRAMGDLC+T0tL07lz5+Tj45PtfpD/XLp0SUFBQTpy5Ii8vLzsXQ4AADnGNQwA8G/E9evBZRiGLl++rMDAwGz72S2UKlq0qBwdHTOMijp16lSG0VDp/P39M+3v5OQkHx+fTNdxdXWVq6urTVuhQoXuvHD863l5efELEQDwr8Q1DADwb8T168GU3QipdHab6NzFxUURERFas2aNTfuaNWtUq1atTNepWbNmhv6rV69WZGRkpvNJAQAAAAAA4P5k16fvDRgwQB9//LFmz56t+Ph49e/fXwkJCYqJiZF089a7Dh06WPvHxMTo8OHDGjBggOLj4zV79mzNmjVLr7zyir0OAQAAAAAAAHfArnNKPfvsszp79qzGjBmjxMREhYWFacWKFQoODpYkJSYmKiEhwdo/NDRUK1asUP/+/fX+++8rMDBQ7777rp5++ml7HQL+RVxdXTVy5MgMt3MCAHC/4xoGAPg34vqF27EYt3s+HwAAAAAAAJDH7Hr7HgAAAAAAAB5MhFIAAAAAAAAwHaEUAAAAAAAATEcoBQAAcA/99ddfslgs2rVrl71LAQDAbkaNGqVq1arZuwzcZwilkCOdOnWSxWKxvnx8fPTEE0/o119/tXdpkqTr16+rcOHCKlKkiK5fv27vcgAAdpZ+3ZowYYJN+9KlS2WxWOxU1e199tlncnR0VExMjL1LAQDYQadOndSqVasM7Rs2bJDFYtGFCxdMrymvvPLKK/r222/veP1Ro0bJYrHoiSeeyLBs4sSJslgsql+//l1UCHsglEKOPfHEE0pMTFRiYqK+/fZbOTk5qXnz5vYuS5K0ZMkShYWFqVKlSvriiy/sWothGLpx44ZdawAASG5ubnrzzTd1/vx5e5eSY7Nnz9arr76qBQsW6Nq1a3atJTk52a77BwDkLXv/Xi9YsKB8fHzuahsBAQFav369jh49atM+Z84clSxZ8q62DfsglEKOubq6yt/fX/7+/qpWrZpee+01HTlyRKdPn7b2ee2111SuXDkVKFBApUqV0vDhw5WSkmJd/ssvv6hBgwby9PSUl5eXIiIitH37duvyLVu2qG7dunJ3d1dQUJD69Omjq1ev3ra2WbNm6YUXXtALL7ygWbNmZVi+d+9eNWvWTF5eXvL09FSdOnV08OBB6/LZs2ercuXKcnV1VUBAgHr37i0p81suLly4IIvFog0bNkj6318tVq1apcjISLm6umrz5s06ePCgWrZsKT8/PxUsWFA1atTQ2rVrbepKSkrSq6++qqCgILm6uqps2bKaNWuWDMNQmTJlNGnSJJv+e/bskYODg03tAIDMNWrUSP7+/oqNjc2235IlS6zXgJCQEL399tvWZUOGDNGjjz6aYZ3w8HCNHDnS+n7OnDmqWLGi3NzcVKFCBU2fPj3X9f7111/asmWLBg8erAoVKmjx4sUZ+mR1vZJuXp+6d+8uPz8/ubm5KSwsTMuXL5eU+S0TU6dOVUhIiPV9+l/nY2NjFRgYqHLlykmS5s2bp8jISHl6esrf31/PP/+8Tp06ZbOtrK6zmzZtkrOzs06cOGHTf+DAgapbt26uzxEA4KazZ8/queeeU4kSJVSgQAFVqVJFn3/+uU2f+vXrq3fv3howYICKFi2qqKgom+8uDz30kNzd3fX444/r1KlT+uabb1SxYkV5eXnpueees/njSFJSkvr06SNfX1+5ubnpscce07Zt26zL07f77bffKjIyUgUKFFCtWrW0f/9+a5/MrkXZXdcy4+vrq8aNG2vu3LnWti1btujMmTNq1qxZhv63uz7f7vtres2ffvqpQkJC5O3trbZt2+ry5cvZ1omcI5TCHbly5Yrmz5+vMmXK2KTdnp6eiouL0759+/TOO+/oo48+0pQpU6zL27VrpxIlSmjbtm3asWOHBg8eLGdnZ0nS7t27FR0drf/85z/69ddftXDhQn333Xe3/cV08OBB/fDDD2rTpo3atGmjLVu26M8//7QuP3bsmOrWrSs3NzetW7dOO3bsUJcuXayjmWbMmKFevXqpe/fu2r17t5YtW6YyZcrk+py8+uqrio2NVXx8vMLDw3XlyhU1bdpUa9eu1c6dOxUdHa0WLVooISHBuk6HDh20YMECvfvuu4qPj9cHH3ygggULymKxqEuXLpozZ47NPmbPnq06deqodOnSua4PAB40jo6OGj9+vKZNm5bhL6rpduzYoTZt2qht27bavXu3Ro0apeHDhysuLk7SzevW1q1bbf4YsHfvXu3evVvt2rWTJH300UcaNmyY3njjDcXHx2v8+PEaPny4zT+Yc2L27Nlq1qyZvL29M/0jS3bXq7S0NDVp0kRbtmzRvHnztG/fPk2YMEGOjo65quHbb79VfHy81qxZYw20kpOTNXbsWP3yyy9aunSpDh06pE6dOlnXye46W7duXZUqVUqffvqptf+NGzc0b948de7cOVe1AQD+5++//1ZERISWL1+uPXv2qHv37mrfvr22bt1q02/u3LlycnLS999/rw8//NDaPmrUKL333nvasmWLjhw5ojZt2mjq1Kn67LPP9PXXX2vNmjWaNm2atf+rr76qJUuWaO7cufr5559VpkwZRUdH69y5czb7GzZsmN5++21t375dTk5O6tKlS5bHcKffw7p06WK9Tks3r5/t2rWTi4uLTb+cXJ9v9/1Vuvl9c+nSpVq+fLmWL1+ujRs3ZpgeAHfBAHKgY8eOhqOjo+Hh4WF4eHgYkoyAgABjx44d2a43ceJEIyIiwvre09PTiIuLy7Rv+/btje7du9u0bd682XBwcDCuX7+e5T6GDh1qtGrVyvq+ZcuWxrBhw6zvhwwZYoSGhhrJycmZrh8YGGjT/58OHTpkSDJ27txpbTt//rwhyVi/fr1hGIaxfv16Q5KxdOnSLGtMV6lSJWPatGmGYRjG/v37DUnGmjVrMu17/Phxw9HR0di6dathGIaRnJxsFCtWLMvzBwD4n44dOxotW7Y0DMMwHn30UaNLly6GYRjGl19+afzznz/PP/+8ERUVZbPuoEGDjEqVKlnfh4eHG2PGjLG+HzJkiFGjRg3r+6CgIOOzzz6z2cbYsWONmjVrGoaR+bXkVqmpqUZQUJD1WnL69GnD2dnZ+P333619srterVq1ynBwcDD279+f6fKRI0caVatWtWmbMmWKERwcbH3fsWNHw8/Pz0hKSsqyTsMwjJ9++smQZFy+fNkwjNtfZ998802jYsWK1vdLly41ChYsaFy5ciXb/QDAg+jW713pLzc3N0OScf78+SzXbdq0qTFw4EDr+3r16hnVqlWz6ZP+3WXt2rXWttjYWEOScfDgQWtbjx49jOjoaMMwDOPKlSuGs7OzMX/+fOvy5ORkIzAw0Jg4cWKW2/36668NSdbvcrdei7K7rmUmff3k5GTD19fX2Lhxo3HlyhXD09PT+OWXX4y+ffsa9erVs/a/3fU5M7d+fx05cqRRoEAB49KlS9a2QYMGGY888kiO60b2GCmFHGvQoIF27dqlXbt2aevWrWrcuLGaNGmiw4cPW/ssXrxYjz32mPz9/VWwYEENHz7cZmTQgAED1K1bNzVq1EgTJkyw+cvzjh07FBcXp4IFC1pf0dHRSktL06FDhzKtKTU1VXPnztULL7xgbXvhhRc0d+5cpaamSpJ27dqlOnXqWEdk/dOpU6d0/PhxNWzY8K7PT2RkpM37q1ev6tVXX1WlSpVUqFAhFSxYUL/99pv1fOzatUuOjo6qV69eptsLCAhQs2bNNHv2bEnS8uXL9ffff+uZZ56561oB4EHy5ptvau7cudq3b1+GZfHx8apdu7ZNW+3atfX7779bryPt2rXT/PnzJd2cN/Dzzz+3jpI6ffq0jhw5oq5du9pcv8aNG5erW61Xr16tq1evqkmTJpKkokWLqnHjxtZrwO2uV7t27VKJEiWst9zdqSpVqmT4S/POnTvVsmVLBQcHy9PT0zqJ7D+vZ1ldZ6WbtwX+8ccf+vHHHyXd/It2mzZt5OHhcVe1AkB+9c/vXemvjz/+2KZPamqq3njjDYWHh8vHx0cFCxbU6tWrbb57SRm/o6QLDw+3/refn5/19rV/tqXfqn3w4EGlpKTYXC+dnZ318MMPKz4+PsvtBgQESFKGW77T2+70e5izs7NeeOEFzZkzR4sWLVK5cuVs9ivl/Pp8u++vkhQSEiJPT0+b48rsmHBnnOxdAP49PDw8bIZTRkREyNvbWx999JHGjRunH3/8UW3bttXo0aMVHR0tb29vLViwwGZujlGjRun555/X119/rW+++UYjR47UggUL9NRTTyktLU09evRQnz59Muw7q0nrVq1apWPHjunZZ5+1aU9NTdXq1avVpEkTubu7Z3lM2S2TJAeHm7mtYRjWtn/eY/xPt/7jetCgQVq1apUmTZqkMmXKyN3dXa1bt7ZOMHi7fUtSt27d1L59e02ZMkVz5szRs88+qwIFCtx2PQDA/9StW1fR0dEaOnSozW1n0s3f77c+je+fv/Ml6fnnn9fgwYP1888/6/r16zpy5Ijatm0r6eZtc9LNWwQeeeQRm/Vyc+vc7Nmzde7cOZvf8Wlpadq5c6fGjh1722tGTq5ntx5XZtezW69lV69eVePGjdW4cWPNmzdPxYoVU0JCgqKjo3N8PfP19VWLFi00Z84clSpVSitWrLDOywgAyOjW712SMtyG/vbbb2vKlCmaOnWqqlSpIg8PD/Xr1y/DZOZZ/QHgn39IsFgsGf6wYLFYrNe49OtHZtfLW9tu3a70v2vlP+Xku1B2unTpokceeUR79uzJ9BbBnFyfc/L99dZjkmzPDe4eoRTumMVikYODg65fvy5J+v777xUcHKxhw4ZZ+/xzFFW6cuXKqVy5curfv7+ee+45zZkzR0899ZSqV6+uvXv35mo+p1mzZqlt27Y2+5SkCRMmaNasWWrSpInCw8M1d+5cpaSkZPiF4unpqZCQEH377bdq0KBBhu0XK1ZMkpSYmKiHHnpIkmwmPc/O5s2b1alTJz311FOSbs7D9ddff1mXV6lSRWlpadq4caMaNWqU6TaaNm0qDw8PzZgxQ9988402bdqUo30DAGxNmDBB1apVyzCSqFKlSvruu+9s2rZs2aJy5cpZ/9FaokQJ1a1bV/Pnz9f169fVqFEj+fn5Sbr5l+TixYvrzz//tI6eyq2zZ8/qq6++0oIFC1S5cmVre1pamurUqaNvvvlGzZs3z/Z6FR4erqNHj+rAgQOZjpYqVqyYTpw4YfMFIifXs99++01nzpzRhAkTFBQUJEk2DyhJ33dW19l03bp1U9u2bVWiRAmVLl06w+g0AEDubN68WS1btrTeMZKWlqbff/9dFStWzPN9lSlTRi4uLvruu+/0/PPPS7r5h43t27erX79+d7TN230Pu53KlSurcuXK+vXXX601/VNOrs85/f6Ke4tQCjmWlJRkfXrO+fPn9d577+nKlStq0aKFpJu/rBISErRgwQLVqFFDX3/9tb788kvr+tevX9egQYPUunVrhYaG6ujRo9q2bZuefvppSTeffPDoo4+qV69eevHFF+Xh4WGdbPWfk+ylO336tP773/9q2bJlCgsLs1nWsWNHNWvWTKdPn1bv3r01bdo0tW3bVkOGDJG3t7d+/PFHPfzwwypfvrxGjRqlmJgY+fr6qkmTJrp8+bK+//57vfzyy3J3d9ejjz6qCRMmKCQkRGfOnNHrr7+eo/NVpkwZffHFF2rRooUsFouGDx9uk6iHhISoY8eO6tKli959911VrVpVhw8f1qlTp9SmTRtJN1P8Tp06aciQISpTpoxq1qyZi58YACBdlSpV1K5duwzXk4EDB6pGjRoaO3asnn32Wf3www967733Mjydp127dho1apSSk5MzTIA6atQo9enTR15eXmrSpImSkpK0fft2nT9/XgMGDLhtbZ9++ql8fHz0zDPPWEfopmvevLlmzZql5s2bZ3u9qlevnurWraunn35akydPVpkyZfTbb7/JYrHoiSeeUP369XX69GlNnDhRrVu31sqVK/XNN9/Iy8sr29pKliwpFxcXTZs2TTExMdqzZ4/Gjh1r0+d211lJ1r9Ajxs3TmPGjLntOQEAZK9MmTJasmSJtmzZosKFC2vy5Mk6ceLEPQmlPDw89NJLL2nQoEEqUqSISpYsqYkTJ+ratWvq2rXrHW83u+taTqxbt04pKSkqVKhQltvP7vp8u++vMAdzSiHHVq5cqYCAAAUEBOiRRx7Rtm3btGjRIuvcEi1btlT//v3Vu3dvVatWTVu2bNHw4cOt6zs6Ours2bPq0KGDypUrpzZt2qhJkyYaPXq0pJt/ad24caN+//131alTRw899JCGDx9uvRf5Vp988ok8PDwyvQ+5QYMG8vT0tP5Df926dbpy5Yrq1auniIgIffTRR9a/5nbs2FFTp07V9OnTVblyZTVv3ly///67dVuzZ89WSkqKIiMj1bdvX40bNy5H52vKlCkqXLiwatWqpRYtWig6OlrVq1e36TNjxgy1bt1aPXv2VIUKFfTiiy/q6tWrNn26du2q5OTkbJ9cAQC4vbFjx2a4ha169er6v//7Py1YsEBhYWEaMWKExowZk+E2v2eeeUZnz57VtWvX1KpVK5tl3bp108cff6y4uDhVqVJF9erVU1xcnEJDQ3NU1+zZs/XUU09lCKQk6emnn9by5ct18uTJ216vlixZoho1aui5555TpUqV9Oqrr1rnxapYsaKmT5+u999/X1WrVtVPP/2kV1555ba1FStWTHFxcVq0aJEqVaqkCRMmaNKkSTZ9bnedlW7ePtipUyelpqaqQ4cOOTovAICsDR8+XNWrV1d0dLTq168vf3//DNenvDRhwgQ9/fTTat++vapXr64//vhDq1atUuHChe94m7e7rt2Oh4dHloGUdPvr8+2+v8IcFuPWf50BuK98//33ql+/vo4ePWq9XQQAgH+bF198USdPntSyZcvsXQoAALhPcPsecJ9KSkrSkSNHNHz4cLVp04ZACgDwr3Tx4kVt27ZN8+fP11dffWXvcgAAwH2E2/eA+9Tnn3+u8uXL6+LFi5o4caK9ywEA4I60bNlSTz75pHr06KGoqCh7lwMAAO4j3L4HAAAAAAAA0zFSCgAAAAAAAKYjlAIAAAAAAIDpCKUAAAAAAABgOkIpAAAAAAAAmI5QCgAAAAAAAKYjlAIAAAAAAIDpCKUAAAAAAABgOkIpAAAAAAAAmI5QCgAAAAAAAKb7fyL9UCCt6VRTAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 1200x700 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot Zero-Shot Baseline\n",
    "labels = ['Base Accuracy', 'Novel Accuracy', 'Harmonic Mean']\n",
    "zeroshot_scores = [base_accuracy_zeroshot, novel_accuracy_zeroshot, hm_zeroshot]\n",
    "x = np.arange(len(labels))\n",
    "width = 0.35\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 7))\n",
    "rects = ax.bar(x, zeroshot_scores, width, label='Zero-Shot CLIP', color='skyblue')\n",
    "\n",
    "ax.set_ylabel('Accuracy')\n",
    "ax.set_title('Zero-Shot CLIP Performance on Flowers102')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(labels)\n",
    "ax.set_ylim(0, 1.0)\n",
    "ax.legend()\n",
    "\n",
    "def autolabel(rects):\n",
    "    for rect in rects:\n",
    "        height = rect.get_height()\n",
    "        ax.annotate(f'{height*100:.2f}%',\n",
    "                    xy=(rect.get_x() + rect.get_width() / 2, height),\n",
    "                    xytext=(0, 3),  # 3 points vertical offset\n",
    "                    textcoords=\"offset points\",\n",
    "                    ha='center', va='bottom')\n",
    "\n",
    "autolabel(rects)\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a51b5e3e",
   "metadata": {},
   "source": [
    "### LoRA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11fe8b56",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "**Low-Rank Adaptation (LoRA)** is a parameter-efficient fine-tuning method that constrains weight updates during adaptation to a low-rank subspace. It was introduced to address the high memory and computational cost of updating all parameters in large-scale pre-trained models such as CLIP.\n",
    "\n",
    "### Theoretical Motivation\n",
    "\n",
    "Pre-trained deep networks, including transformer-based vision-language models, have weight matrices that are generally **full rank**. However, prior work (Aghajanyan et al., 2020) shows that such models often have a **low intrinsic dimension**, meaning that their learned representations and the space of useful adaptations occupy a much smaller subspace than the total parameter space.  \n",
    "\n",
    "This implies that during fine-tuning, the effective change in weights can be approximated by a low-rank update without significantly limiting the modelâ€™s ability to learn the new task. Instead of updating a large $d \\times k$ weight matrix directly, LoRA re-parameterizes the update as the product of two much smaller matrices:  \n",
    "\n",
    "$$\n",
    "W = W_0 + \\Delta W, \\quad \\Delta W = BA\n",
    "$$\n",
    "\n",
    "Here, $W_0 \\in \\mathbb{R}^{d \\times k}$ is the pre-trained weight matrix (frozen during adaptation), $B \\in \\mathbb{R}^{d \\times r}$ and $A \\in \\mathbb{R}^{r \\times k}$ are trainable matrices, and $r \\ll \\min(d, k)$ is the rank of the update. The rank $r$ controls the adaptation capacity: a small $r$ limits the expressiveness of the update but keeps parameter count low, while a larger $r$ allows more flexibility at the cost of increased parameters.\n",
    "\n",
    "In standard matrix multiplication, the output is:\n",
    "\n",
    "$$\n",
    "h = W_0 x\n",
    "$$\n",
    "\n",
    "With LoRA, the forward pass becomes:\n",
    "\n",
    "$$\n",
    "h = W_0 x + \\gamma \\cdot (B A x)\n",
    "$$\n",
    "\n",
    "where $\\gamma$ is a scaling factor applied to the low-rank update to stabilize training. $A$ is initialized, and $B$ is set to all zeros, ensuring $\\Delta W = 0$ at the start, so the initial output is identical to the pre-trained model. This initialization makes LoRA particularly stable in few-shot learning: at the beginning of training, the model performs identically to the base model, and adaptation happens gradually as $A$ and $B$ learn.\n",
    "\n",
    "For a typical CLIP transformer layer, the number of additional parameters introduced by LoRA with rank r, is:\n",
    "\n",
    "$$\n",
    "\\text{Params} = r \\cdot (d + k)\n",
    "$$\n",
    "\n",
    "This is a small fraction of the full $d \\times k$ parameters. By freezing $W_0$, memory usage is reduced since no gradient storage is needed for frozen weights, overfitting risk is lowered in few-shot scenarios, and training is speed up while preserving most pre-trained knowledge.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f87d463",
   "metadata": {},
   "source": [
    "### Application to CLIP\n",
    "\n",
    "CLIP consists of a vision encoder and a text encoder, both implemented as transformer stacks with multi-head attention (MHA) modules. Each MHA block contains **query**, **key**, and **value** projection matrices, which are prime candidates for LoRA insertion. In our setup, LoRA is applied only to the vision encoder, while the text encoder remains frozen. \n",
    "\n",
    "\n",
    "### Implementation Details\n",
    "\n",
    "The Loraload function loads the original CLIP model, inserts LoRA modules into the vision encoder, marks only LoRA parameters along with positional_embedding and logit_scale as trainable, and keeps all other parameters frozen to maintain parameter efficiency.\n",
    "\n",
    "### Fine-Tuning Procedure\n",
    "\n",
    "The fine-tuning begins by reloading the CLIP ViT-B/16 model from scratch for a clean start, followed by LoRA injection into the vision encoder with rank $r = 8$. The training loop uses AdamW with a learning rate of $5 e^{-4}$ and cross-entropy loss for 15 epochs. For each batch, class names are tokenized into the format \"a photo of a {class_name}, a type of flower.\", images are encoded using the LoRA-adapted vision encoder, and text is encoded using the frozen text encoder. Features are normalized, cosine similarity logits are computed, targets are mapped to base-class indices, and loss is computed to update only LoRA parameters.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83ba3cf7",
   "metadata": {},
   "source": [
    "### Why Only Vision-Encoder?\n",
    "\n",
    "We choose to apply LoRA only to the vision encoder because it provides the best trade-off between adaptation effectiveness and preserving CLIPâ€™s strong zero-shot capabilities. The vision encoder extracts visual representations from images, so fine-tuning it allows the model to better capture task-specific visual patterns and adapt to the characteristics of the new dataset. Meanwhile, freezing the text encoder preserves the alignment between natural language prompts and class names, ensuring the model retains its robust zero-shot interpretability. This approach is also more parameter-efficient and computationally lighter compared to tuning both encoders, making training faster and more stable. Empirically, adapting just the vision encoder yields better results than tuning the text encoder or both encoders, improving base-class accuracy without sacrificing generalization to unseen categories."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bcb096a",
   "metadata": {},
   "source": [
    "In order to write this section we took inspirtation from various resources:\n",
    " - Microsoft LoRA (https://github.com/microsoft/LoRA)\n",
    " - Lora-Clip Easy Wrapper (https://github.com/jaisidhsingh/LoRA-CLIP/tree/main?tab=readme-ov-file)\n",
    " - Clip-Lora (https://github.com/MaxZanella/CLIP-LoRA/tree/main)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7754ff5a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-23T11:07:51.689449Z",
     "iopub.status.busy": "2025-08-23T11:07:51.689231Z",
     "iopub.status.idle": "2025-08-23T11:07:51.997676Z",
     "shell.execute_reply": "2025-08-23T11:07:51.997051Z",
     "shell.execute_reply.started": "2025-08-23T11:07:51.689431Z"
    }
   },
   "outputs": [],
   "source": [
    "# Here we define all the functions used to run LoRA\n",
    "class LoRALayer():\n",
    "    def __init__(\n",
    "        self,\n",
    "        r: int,\n",
    "        lora_alpha: int,\n",
    "        lora_dropout: float,\n",
    "        merge_weights: bool,\n",
    "    ):\n",
    "        self.r = r\n",
    "        self.lora_alpha = lora_alpha\n",
    "        # Optional dropout\n",
    "        if lora_dropout > 0.:\n",
    "            self.lora_dropout = nn.Dropout(p=lora_dropout)\n",
    "        else:\n",
    "            self.lora_dropout = lambda x: x\n",
    "        # Mark the weight as unmerged\n",
    "        self.merged = False\n",
    "        self.merge_weights = merge_weights\n",
    "\n",
    "\n",
    "class Embedding(nn.Embedding, LoRALayer):\n",
    "    # LoRA implemented in a dense layer\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_embeddings: int,\n",
    "        embedding_dim: int,\n",
    "        r: int = 0,\n",
    "        lora_alpha: int = 1,\n",
    "        merge_weights: bool = True,\n",
    "        **kwargs\n",
    "    ):\n",
    "        nn.Embedding.__init__(self, num_embeddings, embedding_dim, **kwargs)\n",
    "        LoRALayer.__init__(self, r=r, lora_alpha=lora_alpha, lora_dropout=0,\n",
    "                           merge_weights=merge_weights)\n",
    "        # Actual trainable parameters \n",
    "        if r > 0:\n",
    "            self.lora_A = nn.Parameter(self.weight.new_zeros((r, num_embeddings)))\n",
    "            self.lora_B = nn.Parameter(self.weight.new_zeros((embedding_dim, r)))\n",
    "            self.lora_gate = nn.Parameter(torch.tensor(0.0))\n",
    "            self.scaling = self.lora_alpha / self.r\n",
    "            # Freezing the pre-trained weight matrix\n",
    "            self.weight.requires_grad = False\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        nn.Embedding.reset_parameters(self)\n",
    "        if hasattr(self, 'lora_A'):\n",
    "            # initialize A the same way as the default for nn.Linear and B to zero\n",
    "            nn.init.zeros_(self.lora_A)\n",
    "            nn.init.normal_(self.lora_B)\n",
    "\n",
    "    def train(self, mode: bool = True):\n",
    "        nn.Embedding.train(self, mode)\n",
    "        if self.merge_weights and self.merged:\n",
    "            # Make sure that the weights are not merged\n",
    "            if self.r > 0:\n",
    "                self.weight.data -= (self.lora_B @ self.lora_A).T * self.scaling\n",
    "            self.merged = False\n",
    "\n",
    "    def eval(self):\n",
    "        nn.Linear.eval(self)\n",
    "        if self.merge_weights and not self.merged:\n",
    "            # Merge the weights and mark it\n",
    "            if self.r > 0:\n",
    "                self.weight.data += (self.lora_B @ self.lora_A) * self.scaling\n",
    "            self.merged = True\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        if self.r > 0 and not self.merged:\n",
    "            result = nn.Embedding.forward(self, x)\n",
    "            if self.r > 0:\n",
    "                after_A = F.embedding(\n",
    "                    x, self.lora_A.T, self.padding_idx, self.max_norm,\n",
    "                    self.norm_type, self.scale_grad_by_freq, self.sparse\n",
    "                )\n",
    "                result += (after_A @ self.lora_B.T) * self.scaling\n",
    "            return result\n",
    "        else:\n",
    "            return nn.Embedding.forward(self, x)\n",
    "\n",
    "\n",
    "class Linear(nn.Linear, LoRALayer):\n",
    "    # LoRA implemented in a dense layer\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_features: int,\n",
    "        out_features: int,\n",
    "        r: int = 0,\n",
    "        lora_alpha: int = 1,\n",
    "        lora_dropout: float = 0.,\n",
    "        fan_in_fan_out: bool = False, # Set this to True if the layer to replace stores weight like (fan_in, fan_out)\n",
    "        merge_weights: bool = True,\n",
    "        **kwargs\n",
    "    ):\n",
    "        nn.Linear.__init__(self, in_features, out_features, **kwargs)\n",
    "        LoRALayer.__init__(self, r=r, lora_alpha=lora_alpha, lora_dropout=lora_dropout,\n",
    "                           merge_weights=merge_weights)\n",
    "\n",
    "        self.fan_in_fan_out = fan_in_fan_out\n",
    "        # Actual trainable parameters\n",
    "        if r > 0:\n",
    "            self.lora_A = nn.Parameter(self.weight.new_zeros((r, in_features)))\n",
    "            self.lora_B = nn.Parameter(self.weight.new_zeros((out_features, r)))\n",
    "            self.lora_gate = nn.Parameter(torch.tensor(0.0)) \n",
    "            self.scaling = self.lora_alpha / self.r\n",
    "            # Freezing the pre-trained weight matrix\n",
    "            self.weight.requires_grad = False\n",
    "        self.reset_parameters()\n",
    "        if fan_in_fan_out:\n",
    "            self.weight.data = self.weight.data.T\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        nn.Linear.reset_parameters(self)\n",
    "        if hasattr(self, 'lora_A'):\n",
    "            # initialize A the same way as the default for nn.Linear and B to zero\n",
    "            nn.init.kaiming_uniform_(self.lora_A, a=math.sqrt(5))\n",
    "            nn.init.zeros_(self.lora_B)\n",
    "\n",
    "    def train(self, mode: bool = True):\n",
    "        def T(w):\n",
    "            return w.T if self.fan_in_fan_out else w\n",
    "        nn.Linear.train(self, mode)\n",
    "        if self.merge_weights and self.merged:\n",
    "            # Make sure that the weights are not merged\n",
    "            if self.r > 0:\n",
    "                self.weight.data -= T(self.lora_B @ self.lora_A) * self.scaling\n",
    "            self.merged = False\n",
    "\n",
    "    def eval(self):\n",
    "        def T(w):\n",
    "            return w.T if self.fan_in_fan_out else w\n",
    "        nn.Linear.eval(self)\n",
    "        if self.merge_weights and not self.merged:\n",
    "            # Merge the weights and mark it\n",
    "            if self.r > 0:\n",
    "                self.weight.data += T(self.lora_B @ self.lora_A) * self.scaling\n",
    "            self.merged = True\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        def T(w):\n",
    "            return w.T if self.fan_in_fan_out else w\n",
    "        if self.r > 0 and not self.merged:\n",
    "            result = F.linear(x, T(self.weight), bias=self.bias)\n",
    "            if self.r > 0:\n",
    "                result += ((self.lora_dropout(x) @ self.lora_A.T @ self.lora_B.T) * torch.sigmoid(self.lora_gate)) * self.scaling \n",
    "            return result\n",
    "        else:\n",
    "            return F.linear(x, T(self.weight), bias=self.bias)\n",
    "\n",
    "\n",
    "class MergedLinear(nn.Linear, LoRALayer):\n",
    "    # LoRA implemented in a dense layer\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_features: int,\n",
    "        out_features: int,\n",
    "        r: int = 0,\n",
    "        lora_alpha: int = 1,\n",
    "        lora_dropout: float = 0.,\n",
    "        enable_lora: List[bool] = [False],\n",
    "        fan_in_fan_out: bool = False,\n",
    "        merge_weights: bool = True,\n",
    "        **kwargs\n",
    "    ):\n",
    "        nn.Linear.__init__(self, in_features, out_features, **kwargs)\n",
    "        LoRALayer.__init__(self, r=r, lora_alpha=lora_alpha, lora_dropout=lora_dropout,\n",
    "                           merge_weights=merge_weights)\n",
    "        assert out_features % len(enable_lora) == 0, \\\n",
    "            'The length of enable_lora must divide out_features'\n",
    "        self.enable_lora = enable_lora\n",
    "        self.fan_in_fan_out = fan_in_fan_out\n",
    "        # Actual trainable parameters\n",
    "        if r > 0 and any(enable_lora):\n",
    "            self.lora_A = nn.Parameter(\n",
    "                self.weight.new_zeros((r * sum(enable_lora), in_features)))\n",
    "            self.lora_B = nn.Parameter(\n",
    "                self.weight.new_zeros((out_features // len(enable_lora) * sum(enable_lora), r))\n",
    "            ) # weights for Conv1D with groups=sum(enable_lora)\n",
    "            self.lora_gate = nn.Parameter(torch.tensor(0.0))\n",
    "            self.scaling = self.lora_alpha / self.r\n",
    "            # Freezing the pre-trained weight matrix\n",
    "            self.weight.requires_grad = False\n",
    "            # Compute the indices\n",
    "            self.lora_ind = self.weight.new_zeros(\n",
    "                (out_features, ), dtype=torch.bool\n",
    "            ).view(len(enable_lora), -1)\n",
    "            self.lora_ind[enable_lora, :] = True\n",
    "            self.lora_ind = self.lora_ind.view(-1)\n",
    "        self.reset_parameters()\n",
    "        if fan_in_fan_out:\n",
    "            self.weight.data = self.weight.data.T\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        nn.Linear.reset_parameters(self)\n",
    "        if hasattr(self, 'lora_A'):\n",
    "            # initialize A the same way as the default for nn.Linear and B to zero\n",
    "            nn.init.kaiming_uniform_(self.lora_A, a=math.sqrt(5))\n",
    "            nn.init.zeros_(self.lora_B)\n",
    "\n",
    "    def zero_pad(self, x):\n",
    "        result = x.new_zeros((*x.shape[:-1], self.out_features))\n",
    "        result = result.view(-1, self.out_features)\n",
    "        result[:, self.lora_ind] = x.reshape(\n",
    "            -1, self.out_features // len(self.enable_lora) * sum(self.enable_lora)\n",
    "        )\n",
    "        return result.view((*x.shape[:-1], self.out_features))\n",
    "\n",
    "    def train(self, mode: bool = True):\n",
    "        def T(w):\n",
    "            return w.T if self.fan_in_fan_out else w\n",
    "        nn.Linear.train(self, mode)\n",
    "        if self.merge_weights and self.merged:\n",
    "            # Make sure that the weights are not merged\n",
    "            if self.r > 0 and any(self.enable_lora):\n",
    "                delta_w = F.conv1d(\n",
    "                    self.lora_A.data.unsqueeze(0),\n",
    "                    self.lora_B.data.unsqueeze(-1),\n",
    "                    groups=sum(self.enable_lora)\n",
    "                ).squeeze(0)\n",
    "                self.weight.data -= self.zero_pad(T(delta_w * self.scaling))\n",
    "            self.merged = False\n",
    "\n",
    "    def eval(self):\n",
    "        def T(w):\n",
    "            return w.T if self.fan_in_fan_out else w\n",
    "        nn.Linear.eval(self)\n",
    "        if self.merge_weights and not self.merged:\n",
    "            # Merge the weights and mark it\n",
    "            if self.r > 0 and any(self.enable_lora):\n",
    "                delta_w = F.conv1d(\n",
    "                    self.lora_A.data.unsqueeze(0),\n",
    "                    self.lora_B.data.unsqueeze(-1),\n",
    "                    groups=sum(self.enable_lora)\n",
    "                ).squeeze(0)\n",
    "                self.weight.data += self.zero_pad(T(delta_w * self.scaling))\n",
    "            self.merged = True\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        def T(w):\n",
    "            return w.T if self.fan_in_fan_out else w\n",
    "        if self.merged:\n",
    "            return F.linear(x, T(self.weight), bias=self.bias)\n",
    "        else:\n",
    "            result = F.linear(x, T(self.weight), bias=self.bias)\n",
    "            if self.r > 0:\n",
    "                after_A = F.linear(self.lora_dropout(x), self.lora_A)\n",
    "                after_B = F.conv1d(\n",
    "                    after_A.transpose(-2, -1),\n",
    "                    self.lora_B.unsqueeze(-1),\n",
    "                    groups=sum(self.enable_lora)\n",
    "                ).transpose(-2, -1)\n",
    "                result += self.zero_pad(after_B) * self.scaling\n",
    "            return result\n",
    "\n",
    "\n",
    "class Conv2d(nn.Conv2d, LoRALayer):\n",
    "    # LoRA implemented in a dense layer\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int,\n",
    "        out_channels: int,\n",
    "        kernel_size: int,\n",
    "        r: int = 0,\n",
    "        lora_alpha: int = 1,\n",
    "        lora_dropout: float = 0.,\n",
    "        merge_weights: bool = True,\n",
    "        **kwargs\n",
    "    ):\n",
    "        nn.Conv2d.__init__(self, in_channels, out_channels, kernel_size, **kwargs)\n",
    "        LoRALayer.__init__(self, r=r, lora_alpha=lora_alpha, lora_dropout=lora_dropout,\n",
    "                           merge_weights=merge_weights)\n",
    "        assert type(kernel_size) is int\n",
    "        # Actual trainable parameters\n",
    "        if r > 0:\n",
    "            self.lora_A = nn.Parameter(\n",
    "                self.weight.new_zeros((r*kernel_size, in_channels*kernel_size))\n",
    "            )\n",
    "            self.lora_B = nn.Parameter(\n",
    "                self.weight.new_zeros((out_channels*kernel_size, r*kernel_size))\n",
    "            )\n",
    "            self.lora_gate = nn.Parameter(torch.tensor(0.0))\n",
    "            self.scaling = self.lora_alpha / self.r\n",
    "            # Freezing the pre-trained weight matrix\n",
    "            self.weight.requires_grad = False\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        nn.Conv2d.reset_parameters(self)\n",
    "        if hasattr(self, 'lora_A'):\n",
    "            # initialize A the same way as the default for nn.Linear and B to zero\n",
    "            nn.init.kaiming_uniform_(self.lora_A, a=math.sqrt(5))\n",
    "            nn.init.zeros_(self.lora_B)\n",
    "\n",
    "    def train(self, mode: bool = True):\n",
    "        nn.Conv2d.train(self, mode)\n",
    "        if self.merge_weights and self.merged:\n",
    "            # Make sure that the weights are not merged\n",
    "            self.weight.data -= (self.lora_B @ self.lora_A).view(self.weight.shape) * self.scaling\n",
    "            self.merged = False\n",
    "\n",
    "    def eval(self):\n",
    "        nn.Conv2d.eval(self)\n",
    "        if self.merge_weights and not self.merged:\n",
    "            # Merge the weights and mark it\n",
    "            self.weight.data += (self.lora_B @ self.lora_A).view(self.weight.shape) * self.scaling\n",
    "            self.merged = True\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        if self.r > 0 and not self.merged:\n",
    "            return F.conv2d(\n",
    "                x,\n",
    "                self.weight + (self.lora_B @ self.lora_A).view(self.weight.shape) * self.scaling,\n",
    "                self.bias, self.stride, self.padding, self.dilation, self.groups\n",
    "            )\n",
    "        return nn.Conv2d.forward(self, x)\n",
    "\n",
    "class MultiheadAttention(nn.Module):\n",
    "    r\"\"\"Allows the model to jointly attend to information\n",
    "    from different representation subspaces.\n",
    "    See reference: Attention Is All You Need\n",
    "    .. math::\n",
    "        \\text{MultiHead}(Q, K, V) = \\text{Concat}(head_1,\\dots,head_h)W^O\n",
    "        \\text{where} head_i = \\text{Attention}(QW_i^Q, KW_i^K, VW_i^V)\n",
    "    Args:\n",
    "        embed_dim: total dimension of the model.\n",
    "        num_heads: parallel attention heads.\n",
    "        dropout: a Dropout layer on attn_output_weights. Default: 0.0.\n",
    "        bias: add bias as module parameter. Default: True.\n",
    "        add_bias_kv: add bias to the key and value sequences at dim=0.\n",
    "        add_zero_attn: add a new batch of zeros to the key and\n",
    "                       value sequences at dim=1.\n",
    "        kdim: total number of features in key. Default: None.\n",
    "        vdim: total number of features in value. Default: None.\n",
    "        Note: if kdim and vdim are None, they will be set to embed_dim such that\n",
    "        query, key, and value have the same number of features.\n",
    "    Examples::\n",
    "        >>> multihead_attn = nn.MultiheadAttention(embed_dim, num_heads)\n",
    "        >>> attn_output, attn_output_weights = multihead_attn(query, key, value)\n",
    "    \"\"\"\n",
    "    bias_k: Optional[torch.Tensor]\n",
    "    bias_v: Optional[torch.Tensor]\n",
    "\n",
    "    def __init__(self, embed_dim, num_heads, dropout=0., bias=True, add_bias_kv=False, add_zero_attn=False, kdim=None, vdim=None, lora_alpha: int = 1, r=0):\n",
    "        super(MultiheadAttention, self).__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.kdim = kdim if kdim is not None else embed_dim\n",
    "        self.vdim = vdim if vdim is not None else embed_dim\n",
    "        self._qkv_same_embed_dim = self.kdim == embed_dim and self.vdim == embed_dim\n",
    "\n",
    "        self.num_heads = num_heads\n",
    "        self.dropout = dropout\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "        assert self.head_dim * num_heads == self.embed_dim, \"embed_dim must be divisible by num_heads\"\n",
    "\n",
    "        assert r > 0\n",
    "\n",
    "        self.lora_alpha = lora_alpha\n",
    "        self.r = r\n",
    "        self.scaling = self.lora_alpha / self.r\n",
    "\n",
    "        if self._qkv_same_embed_dim is False:\n",
    "            self.q_proj_weight = nn.Parameter(torch.Tensor(embed_dim, embed_dim))\n",
    "            self.k_proj_weight = nn.Parameter(torch.Tensor(embed_dim, self.kdim))\n",
    "            self.v_proj_weight = nn.Parameter(torch.Tensor(embed_dim, self.vdim))\n",
    "            self.q_proj_weight_lora_A = nn.Parameter(torch.Tensor(r, embed_dim))\n",
    "            self.k_proj_weight_lora_A = nn.Parameter(torch.Tensor(r, self.kdim))\n",
    "            self.v_proj_weight_lora_A = nn.Parameter(torch.Tensor(r, self.vdim))\n",
    "            self.q_proj_weight_lora_B = nn.Parameter(torch.Tensor(embed_dim, r))\n",
    "            self.k_proj_weight_lora_B = nn.Parameter(torch.Tensor(embed_dim, r))\n",
    "            self.v_proj_weight_lora_B = nn.Parameter(torch.Tensor(embed_dim, r))\n",
    "            self.register_parameter('in_proj_weight', None)\n",
    "            self.register_parameter('in_proj_weight_lora_A', None)\n",
    "            self.register_parameter('in_proj_weight_lora_B', None)\n",
    "        else:\n",
    "            self.in_proj_weight = nn.Parameter(torch.empty(3 * embed_dim, embed_dim))\n",
    "            self.in_proj_weight_lora_A = nn.Parameter(torch.empty(r, embed_dim))\n",
    "            self.in_proj_weight_lora_B = nn.Parameter(torch.empty(3 * embed_dim, r))\n",
    "            self.register_parameter('q_proj_weight', None)\n",
    "            self.register_parameter('k_proj_weight', None)\n",
    "            self.register_parameter('v_proj_weight', None)\n",
    "            self.register_parameter('q_proj_weight_lora_A', None)\n",
    "            self.register_parameter('k_proj_weight_lora_A', None)\n",
    "            self.register_parameter('v_proj_weight_lora_A', None)\n",
    "            self.register_parameter('q_proj_weight_lora_B', None)\n",
    "            self.register_parameter('k_proj_weight_lora_B', None)\n",
    "            self.register_parameter('v_proj_weight_lora_B', None)\n",
    "\n",
    "        if bias:\n",
    "            self.in_proj_bias = nn.Parameter(torch.empty(3 * embed_dim))\n",
    "        else:\n",
    "            self.register_parameter('in_proj_bias', None)\n",
    "        self.out_proj = Linear(embed_dim, embed_dim, bias=True, merge_weights=False, lora_alpha=lora_alpha, r=r)\n",
    "\n",
    "        if add_bias_kv:\n",
    "            self.bias_k = nn.Parameter(torch.empty(1, 1, embed_dim))\n",
    "            self.bias_v = nn.Parameter(torch.empty(1, 1, embed_dim))\n",
    "        else:\n",
    "            self.bias_k = self.bias_v = None\n",
    "\n",
    "        self.add_zero_attn = add_zero_attn\n",
    "\n",
    "        self._reset_parameters()\n",
    "\n",
    "    def _reset_parameters(self):\n",
    "        if self._qkv_same_embed_dim:\n",
    "            xavier_uniform_(self.in_proj_weight)\n",
    "            xavier_uniform_(self.in_proj_weight_lora_A)\n",
    "            xavier_uniform_(self.in_proj_weight_lora_B)\n",
    "        else:\n",
    "            xavier_uniform_(self.q_proj_weight)\n",
    "            xavier_uniform_(self.k_proj_weight)\n",
    "            xavier_uniform_(self.v_proj_weight)\n",
    "            xavier_uniform_(self.q_proj_weight_lora_A)\n",
    "            xavier_uniform_(self.k_proj_weight_lora_A)\n",
    "            xavier_uniform_(self.v_proj_weight_lora_A)\n",
    "            xavier_uniform_(self.q_proj_weight_lora_B)\n",
    "            xavier_uniform_(self.k_proj_weight_lora_B)\n",
    "            xavier_uniform_(self.v_proj_weight_lora_B)\n",
    "\n",
    "        if self.in_proj_bias is not None:\n",
    "            constant_(self.in_proj_bias, 0.)\n",
    "            constant_(self.out_proj.bias, 0.)\n",
    "        if self.bias_k is not None:\n",
    "            xavier_normal_(self.bias_k)\n",
    "        if self.bias_v is not None:\n",
    "            xavier_normal_(self.bias_v)\n",
    "\n",
    "    def __setstate__(self, state):\n",
    "        # Support loading old MultiheadAttention checkpoints generated by v1.1.0\n",
    "        if '_qkv_same_embed_dim' not in state:\n",
    "            state['_qkv_same_embed_dim'] = True\n",
    "\n",
    "        super(MultiheadAttention, self).__setstate__(state)\n",
    "\n",
    "    def forward(self, query, key, value, key_padding_mask=None,\n",
    "                need_weights=True, attn_mask=None):\n",
    "        # type: (Tensor, Tensor, Tensor, Optional[Tensor], bool, Optional[Tensor]) -> Tuple[Tensor, Optional[Tensor]]\n",
    "        r\"\"\"\n",
    "    Args:\n",
    "        query, key, value: map a query and a set of key-value pairs to an output.\n",
    "            See \"Attention Is All You Need\" for more details.\n",
    "        key_padding_mask: if provided, specified padding elements in the key will\n",
    "            be ignored by the attention. When given a binary mask and a value is True,\n",
    "            the corresponding value on the attention layer will be ignored. When given\n",
    "            a byte mask and a value is non-zero, the corresponding value on the attention\n",
    "            layer will be ignored\n",
    "        need_weights: output attn_output_weights.\n",
    "        attn_mask: 2D or 3D mask that prevents attention to certain positions. A 2D mask will be broadcasted for all\n",
    "            the batches while a 3D mask allows to specify a different mask for the entries of each batch.\n",
    "    Shape:\n",
    "        - Inputs:\n",
    "        - query: :math:`(L, N, E)` where L is the target sequence length, N is the batch size, E is\n",
    "          the embedding dimension.\n",
    "        - key: :math:`(S, N, E)`, where S is the source sequence length, N is the batch size, E is\n",
    "          the embedding dimension.\n",
    "        - value: :math:`(S, N, E)` where S is the source sequence length, N is the batch size, E is\n",
    "          the embedding dimension.\n",
    "        - key_padding_mask: :math:`(N, S)` where N is the batch size, S is the source sequence length.\n",
    "          If a ByteTensor is provided, the non-zero positions will be ignored while the position\n",
    "          with the zero positions will be unchanged. If a BoolTensor is provided, the positions with the\n",
    "          value of ``True`` will be ignored while the position with the value of ``False`` will be unchanged.\n",
    "        - attn_mask: 2D mask :math:`(L, S)` where L is the target sequence length, S is the source sequence length.\n",
    "          3D mask :math:`(N*num_heads, L, S)` where N is the batch size, L is the target sequence length,\n",
    "          S is the source sequence length. attn_mask ensure that position i is allowed to attend the unmasked\n",
    "          positions. If a ByteTensor is provided, the non-zero positions are not allowed to attend\n",
    "          while the zero positions will be unchanged. If a BoolTensor is provided, positions with ``True``\n",
    "          is not allowed to attend while ``False`` values will be unchanged. If a FloatTensor\n",
    "          is provided, it will be added to the attention weight.\n",
    "        - Outputs:\n",
    "        - attn_output: :math:`(L, N, E)` where L is the target sequence length, N is the batch size,\n",
    "          E is the embedding dimension.\n",
    "        - attn_output_weights: :math:`(N, L, S)` where N is the batch size,\n",
    "          L is the target sequence length, S is the source sequence length.\n",
    "        \"\"\"\n",
    "        if not self._qkv_same_embed_dim:\n",
    "            return multi_head_attention_forward(\n",
    "                query, key, value, self.embed_dim, self.num_heads,\n",
    "                self.in_proj_weight, self.in_proj_bias, self.in_proj_weight_lora_A, self.in_proj_weight_lora_B, self.scaling,\n",
    "                self.bias_k, self.bias_v, self.add_zero_attn,\n",
    "                self.dropout, self.out_proj.weight, self.out_proj.bias, self.out_proj.lora_A, self.out_proj.lora_B, self.out_proj.scaling,\n",
    "                training=self.training,\n",
    "                key_padding_mask=key_padding_mask, need_weights=need_weights,\n",
    "                attn_mask=attn_mask, use_separate_proj_weight=True,\n",
    "                q_proj_weight=self.q_proj_weight, k_proj_weight=self.k_proj_weight, v_proj_weight=self.v_proj_weight,\n",
    "                q_proj_weight_A=self.q_proj_weight_lora_A, k_proj_weight_A=self.k_proj_weight_lora_A, v_proj_weight_A=self.v_proj_weight_lora_A,\n",
    "                q_proj_weight_B=self.q_proj_weight_lora_B, k_proj_weight_B=self.k_proj_weight_lora_B, v_proj_weight_B=self.v_proj_weight_lora_B,\n",
    "                q_proj_weight_scaling=self.scaling, k_proj_weight_scaling=self.scaling, v_proj_weight_scaling=self.scaling\n",
    "            )\n",
    "        else:\n",
    "            return multi_head_attention_forward(\n",
    "                query, key, value, self.embed_dim, self.num_heads,\n",
    "                self.in_proj_weight, self.in_proj_bias, self.in_proj_weight_lora_A, self.in_proj_weight_lora_B, self.scaling,\n",
    "                self.bias_k, self.bias_v, self.add_zero_attn,\n",
    "                self.dropout, self.out_proj.weight, self.out_proj.bias, self.out_proj.lora_A, self.out_proj.lora_B, self.out_proj.scaling,\n",
    "                training=self.training,\n",
    "                key_padding_mask=key_padding_mask, need_weights=need_weights,\n",
    "                attn_mask=attn_mask)\n",
    "\n",
    "from torch import Tensor\n",
    "from torch.nn.functional import linear, pad, softmax, dropout\n",
    "def multi_head_attention_forward(query: Tensor,\n",
    "                                 key: Tensor,\n",
    "                                 value: Tensor,\n",
    "                                 embed_dim_to_check: int,\n",
    "                                 num_heads: int,\n",
    "                                 in_proj_weight: Tensor,\n",
    "                                 in_proj_bias: Tensor,\n",
    "                                 in_proj_weight_A: Tensor,\n",
    "                                 in_proj_weight_B: Tensor,\n",
    "                                 in_proj_weight_scaling: Tensor,\n",
    "                                 bias_k: Optional[Tensor],\n",
    "                                 bias_v: Optional[Tensor],\n",
    "                                 add_zero_attn: bool,\n",
    "                                 dropout_p: float,\n",
    "                                 out_proj_weight: Tensor,\n",
    "                                 out_proj_bias: Tensor,\n",
    "                                 out_proj_weight_A: Tensor,\n",
    "                                 out_proj_weight_B: Tensor,\n",
    "                                 out_proj_weight_scaling: Tensor,\n",
    "                                 training: bool = True,\n",
    "                                 key_padding_mask: Optional[Tensor] = None,\n",
    "                                 need_weights: bool = True,\n",
    "                                 attn_mask: Optional[Tensor] = None,\n",
    "                                 use_separate_proj_weight: bool = False,\n",
    "                                 q_proj_weight: Optional[Tensor] = None,\n",
    "                                 k_proj_weight: Optional[Tensor] = None,\n",
    "                                 v_proj_weight: Optional[Tensor] = None,\n",
    "                                 q_proj_weight_A: Optional[Tensor] = None,\n",
    "                                 k_proj_weight_A: Optional[Tensor] = None,\n",
    "                                 v_proj_weight_A: Optional[Tensor] = None,\n",
    "                                 q_proj_weight_B: Optional[Tensor] = None,\n",
    "                                 k_proj_weight_B: Optional[Tensor] = None,\n",
    "                                 v_proj_weight_B: Optional[Tensor] = None,\n",
    "                                 q_proj_weight_scaling = None,\n",
    "                                 k_proj_weight_scaling = None,\n",
    "                                 v_proj_weight_scaling = None,\n",
    "                                 static_k: Optional[Tensor] = None,\n",
    "                                 static_v: Optional[Tensor] = None\n",
    "                                 ) -> Tuple[Tensor, Optional[Tensor]]:\n",
    "    r\"\"\"\n",
    "    Args:\n",
    "        query, key, value: map a query and a set of key-value pairs to an output.\n",
    "            See \"Attention Is All You Need\" for more details.\n",
    "        embed_dim_to_check: total dimension of the model.\n",
    "        num_heads: parallel attention heads.\n",
    "        in_proj_weight, in_proj_bias: input projection weight and bias.\n",
    "        bias_k, bias_v: bias of the key and value sequences to be added at dim=0.\n",
    "        add_zero_attn: add a new batch of zeros to the key and\n",
    "                       value sequences at dim=1.\n",
    "        dropout_p: probability of an element to be zeroed.\n",
    "        out_proj_weight, out_proj_bias: the output projection weight and bias.\n",
    "        training: apply dropout if is ``True``.\n",
    "        key_padding_mask: if provided, specified padding elements in the key will\n",
    "            be ignored by the attention. This is an binary mask. When the value is True,\n",
    "            the corresponding value on the attention layer will be filled with -inf.\n",
    "        need_weights: output attn_output_weights.\n",
    "        attn_mask: 2D or 3D mask that prevents attention to certain positions. A 2D mask will be broadcasted for all\n",
    "            the batches while a 3D mask allows to specify a different mask for the entries of each batch.\n",
    "        use_separate_proj_weight: the function accept the proj. weights for query, key,\n",
    "            and value in different forms. If false, in_proj_weight will be used, which is\n",
    "            a combination of q_proj_weight, k_proj_weight, v_proj_weight.\n",
    "        q_proj_weight, k_proj_weight, v_proj_weight, in_proj_bias: input projection weight and bias.\n",
    "        static_k, static_v: static key and value used for attention operators.\n",
    "    Shape:\n",
    "        Inputs:\n",
    "        - query: :math:`(L, N, E)` where L is the target sequence length, N is the batch size, E is\n",
    "          the embedding dimension.\n",
    "        - key: :math:`(S, N, E)`, where S is the source sequence length, N is the batch size, E is\n",
    "          the embedding dimension.\n",
    "        - value: :math:`(S, N, E)` where S is the source sequence length, N is the batch size, E is\n",
    "          the embedding dimension.\n",
    "        - key_padding_mask: :math:`(N, S)` where N is the batch size, S is the source sequence length.\n",
    "          If a ByteTensor is provided, the non-zero positions will be ignored while the zero positions\n",
    "          will be unchanged. If a BoolTensor is provided, the positions with the\n",
    "          value of ``True`` will be ignored while the position with the value of ``False`` will be unchanged.\n",
    "        - attn_mask: 2D mask :math:`(L, S)` where L is the target sequence length, S is the source sequence length.\n",
    "          3D mask :math:`(N*num_heads, L, S)` where N is the batch size, L is the target sequence length,\n",
    "          S is the source sequence length. attn_mask ensures that position i is allowed to attend the unmasked\n",
    "          positions. If a ByteTensor is provided, the non-zero positions are not allowed to attend\n",
    "          while the zero positions will be unchanged. If a BoolTensor is provided, positions with ``True``\n",
    "          are not allowed to attend while ``False`` values will be unchanged. If a FloatTensor\n",
    "          is provided, it will be added to the attention weight.\n",
    "        - static_k: :math:`(N*num_heads, S, E/num_heads)`, where S is the source sequence length,\n",
    "          N is the batch size, E is the embedding dimension. E/num_heads is the head dimension.\n",
    "        - static_v: :math:`(N*num_heads, S, E/num_heads)`, where S is the source sequence length,\n",
    "          N is the batch size, E is the embedding dimension. E/num_heads is the head dimension.\n",
    "        Outputs:\n",
    "        - attn_output: :math:`(L, N, E)` where L is the target sequence length, N is the batch size,\n",
    "          E is the embedding dimension.\n",
    "        - attn_output_weights: :math:`(N, L, S)` where N is the batch size,\n",
    "          L is the target sequence length, S is the source sequence length.\n",
    "    \"\"\"\n",
    "\n",
    "    tgt_len, bsz, embed_dim = query.size()\n",
    "    assert embed_dim == embed_dim_to_check\n",
    "    # Allow MHA to have different sizes for the feature dimension\n",
    "    assert key.size(0) == value.size(0) and key.size(1) == value.size(1)\n",
    "\n",
    "    head_dim = embed_dim // num_heads\n",
    "    assert head_dim * num_heads == embed_dim, \"embed_dim must be divisible by num_heads\"\n",
    "    scaling = float(head_dim) ** -0.5\n",
    "\n",
    "    if not use_separate_proj_weight:\n",
    "        if torch.equal(query, key) and torch.equal(key, value):\n",
    "            # Self-attention\n",
    "            qkv = linear(query, in_proj_weight, in_proj_bias)\n",
    "            qkv += linear(linear(query, in_proj_weight_A), in_proj_weight_B) * in_proj_weight_scaling\n",
    "            q, k, v = qkv.chunk(3, dim=-1)\n",
    "\n",
    "        elif torch.equal(key, value):\n",
    "            # Encoder-decoder attention\n",
    "            # This is inline in_proj function with in_proj_weight and in_proj_bias\n",
    "            _b = in_proj_bias\n",
    "            _start = 0\n",
    "            _end = embed_dim\n",
    "            _w = in_proj_weight[_start:_end, :]\n",
    "            if _b is not None:\n",
    "                _b = _b[_start:_end]\n",
    "            q = linear(query, _w, _b)\n",
    "            q += linear(linear(query, in_proj_weight_A), in_proj_weight_B[_start:_end, :]) * in_proj_weight_scaling\n",
    "\n",
    "            if key is None:\n",
    "                assert value is None\n",
    "                k = None\n",
    "                v = None\n",
    "            else:\n",
    "\n",
    "                # This is inline in_proj function with in_proj_weight and in_proj_bias\n",
    "                _b = in_proj_bias\n",
    "                _start = embed_dim\n",
    "                _end = None\n",
    "                _w = in_proj_weight[_start:, :]\n",
    "                if _b is not None:\n",
    "                    _b = _b[_start:]\n",
    "                kv = linear(key, _w, _b)\n",
    "                kv += linear(linear(key, in_proj_weight_A), in_proj_weight_B[_start:, :]) * in_proj_weight_scaling\n",
    "                k, v = kv.chunk(2, dim=-1)\n",
    "\n",
    "        else:\n",
    "            # This is inline in_proj function with in_proj_weight and in_proj_bias\n",
    "            _b = in_proj_bias\n",
    "            _start = 0\n",
    "            _end = embed_dim\n",
    "            _w = in_proj_weight[_start:_end, :]\n",
    "            if _b is not None:\n",
    "                _b = _b[_start:_end]\n",
    "            q = linear(query, _w, _b)\n",
    "            q += linear(linear(query, in_proj_weight_A), in_proj_weight_B[_start:_end, :]) * in_proj_weight_scaling\n",
    "\n",
    "            # This is inline in_proj function with in_proj_weight and in_proj_bias\n",
    "            _b = in_proj_bias\n",
    "            _start = embed_dim\n",
    "            _end = embed_dim * 2\n",
    "            _w = in_proj_weight[_start:_end, :]\n",
    "            if _b is not None:\n",
    "                _b = _b[_start:_end]\n",
    "            k = linear(key, _w, _b)\n",
    "            k += linear(linear(key, in_proj_weight_A), in_proj_weight_B[_start:_end, :]) * in_proj_weight_scaling\n",
    "\n",
    "            # This is inline in_proj function with in_proj_weight and in_proj_bias\n",
    "            _b = in_proj_bias\n",
    "            _start = embed_dim * 2\n",
    "            _end = None\n",
    "            _w = in_proj_weight[_start:, :]\n",
    "            if _b is not None:\n",
    "                _b = _b[_start:]\n",
    "            v = linear(value, _w, _b)\n",
    "            v += linear(linear(value, in_proj_weight_A), in_proj_weight_B[_start:, :]) * in_proj_weight_scaling\n",
    "    else:\n",
    "        q_proj_weight_non_opt = torch.jit._unwrap_optional(q_proj_weight)\n",
    "        len1, len2 = q_proj_weight_non_opt.size()\n",
    "        assert len1 == embed_dim and len2 == query.size(-1)\n",
    "\n",
    "        k_proj_weight_non_opt = torch.jit._unwrap_optional(k_proj_weight)\n",
    "        len1, len2 = k_proj_weight_non_opt.size()\n",
    "        assert len1 == embed_dim and len2 == key.size(-1)\n",
    "\n",
    "        v_proj_weight_non_opt = torch.jit._unwrap_optional(v_proj_weight)\n",
    "        len1, len2 = v_proj_weight_non_opt.size()\n",
    "        assert len1 == embed_dim and len2 == value.size(-1)\n",
    "\n",
    "        q_proj_weight_non_opt_A = torch.jit._unwrap_optional(q_proj_weight_A)\n",
    "        k_proj_weight_non_opt_A = torch.jit._unwrap_optional(k_proj_weight_A)\n",
    "        v_proj_weight_non_opt_A = torch.jit._unwrap_optional(v_proj_weight_A)\n",
    "        q_proj_weight_non_opt_B = torch.jit._unwrap_optional(q_proj_weight_B)\n",
    "        k_proj_weight_non_opt_B = torch.jit._unwrap_optional(k_proj_weight_B)\n",
    "        v_proj_weight_non_opt_B = torch.jit._unwrap_optional(v_proj_weight_B)\n",
    "\n",
    "        if in_proj_bias is not None:\n",
    "            q = linear(query, q_proj_weight_non_opt, in_proj_bias[0:embed_dim])\n",
    "            k = linear(key, k_proj_weight_non_opt, in_proj_bias[embed_dim:(embed_dim * 2)])\n",
    "            v = linear(value, v_proj_weight_non_opt, in_proj_bias[(embed_dim * 2):])\n",
    "        else:\n",
    "            q = linear(query, q_proj_weight_non_opt, in_proj_bias)\n",
    "            k = linear(key, k_proj_weight_non_opt, in_proj_bias)\n",
    "            v = linear(value, v_proj_weight_non_opt, in_proj_bias)\n",
    "\n",
    "        q += linear(linear(query, q_proj_weight_non_opt_A), q_proj_weight_non_opt_B) * q_proj_weight_scaling\n",
    "        k += linear(linear(key, k_proj_weight_non_opt_A), k_proj_weight_non_opt_B) * k_proj_weight_scaling\n",
    "        v += linear(linear(value, v_proj_weight_non_opt_A), v_proj_weight_non_opt_B) * v_proj_weight_scaling\n",
    "\n",
    "    q = q * scaling\n",
    "\n",
    "    if attn_mask is not None:\n",
    "        assert attn_mask.dtype == torch.float32 or attn_mask.dtype == torch.float64 or \\\n",
    "            attn_mask.dtype == torch.float16 or attn_mask.dtype == torch.uint8 or attn_mask.dtype == torch.bool, \\\n",
    "            'Only float, byte, and bool types are supported for attn_mask, not {}'.format(attn_mask.dtype)\n",
    "        if attn_mask.dtype == torch.uint8:\n",
    "            warnings.warn(\"Byte tensor for attn_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.\")\n",
    "            attn_mask = attn_mask.to(torch.bool)\n",
    "\n",
    "        if attn_mask.dim() == 2:\n",
    "            attn_mask = attn_mask.unsqueeze(0)\n",
    "            if list(attn_mask.size()) != [1, query.size(0), key.size(0)]:\n",
    "                raise RuntimeError('The size of the 2D attn_mask is not correct.')\n",
    "        elif attn_mask.dim() == 3:\n",
    "            if list(attn_mask.size()) != [bsz * num_heads, query.size(0), key.size(0)]:\n",
    "                raise RuntimeError('The size of the 3D attn_mask is not correct.')\n",
    "        else:\n",
    "            raise RuntimeError(\"attn_mask's dimension {} is not supported\".format(attn_mask.dim()))\n",
    "        # attn_mask's dim is 3 now.\n",
    "\n",
    "    # Convert ByteTensor key_padding_mask to bool\n",
    "    if key_padding_mask is not None and key_padding_mask.dtype == torch.uint8:\n",
    "        warnings.warn(\"Byte tensor for key_padding_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.\")\n",
    "        key_padding_mask = key_padding_mask.to(torch.bool)\n",
    "\n",
    "    if bias_k is not None and bias_v is not None:\n",
    "        if static_k is None and static_v is None:\n",
    "            k = torch.cat([k, bias_k.repeat(1, bsz, 1)])\n",
    "            v = torch.cat([v, bias_v.repeat(1, bsz, 1)])\n",
    "            if attn_mask is not None:\n",
    "                attn_mask = pad(attn_mask, (0, 1))\n",
    "            if key_padding_mask is not None:\n",
    "                key_padding_mask = pad(key_padding_mask, (0, 1))\n",
    "        else:\n",
    "            assert static_k is None, \"bias cannot be added to static key.\"\n",
    "            assert static_v is None, \"bias cannot be added to static value.\"\n",
    "    else:\n",
    "        assert bias_k is None\n",
    "        assert bias_v is None\n",
    "\n",
    "    q = q.contiguous().view(tgt_len, bsz * num_heads, head_dim).transpose(0, 1)\n",
    "    if k is not None:\n",
    "        k = k.contiguous().view(-1, bsz * num_heads, head_dim).transpose(0, 1)\n",
    "    if v is not None:\n",
    "        v = v.contiguous().view(-1, bsz * num_heads, head_dim).transpose(0, 1)\n",
    "\n",
    "    if static_k is not None:\n",
    "        assert static_k.size(0) == bsz * num_heads\n",
    "        assert static_k.size(2) == head_dim\n",
    "        k = static_k\n",
    "\n",
    "    if static_v is not None:\n",
    "        assert static_v.size(0) == bsz * num_heads\n",
    "        assert static_v.size(2) == head_dim\n",
    "        v = static_v\n",
    "\n",
    "    src_len = k.size(1)\n",
    "\n",
    "    if key_padding_mask is not None:\n",
    "        assert key_padding_mask.size(0) == bsz\n",
    "        assert key_padding_mask.size(1) == src_len\n",
    "\n",
    "    if add_zero_attn:\n",
    "        src_len += 1\n",
    "        k = torch.cat([k, torch.zeros((k.size(0), 1) + k.size()[2:], dtype=k.dtype, device=k.device)], dim=1)\n",
    "        v = torch.cat([v, torch.zeros((v.size(0), 1) + v.size()[2:], dtype=v.dtype, device=v.device)], dim=1)\n",
    "        if attn_mask is not None:\n",
    "            attn_mask = pad(attn_mask, (0, 1))\n",
    "        if key_padding_mask is not None:\n",
    "            key_padding_mask = pad(key_padding_mask, (0, 1))\n",
    "\n",
    "    attn_output_weights = torch.bmm(q, k.transpose(1, 2))\n",
    "    assert list(attn_output_weights.size()) == [bsz * num_heads, tgt_len, src_len]\n",
    "\n",
    "    if attn_mask is not None:\n",
    "        if attn_mask.dtype == torch.bool:\n",
    "            attn_output_weights.masked_fill_(attn_mask, float('-inf'))\n",
    "        else:\n",
    "            attn_output_weights += attn_mask\n",
    "\n",
    "\n",
    "    if key_padding_mask is not None:\n",
    "        attn_output_weights = attn_output_weights.view(bsz, num_heads, tgt_len, src_len)\n",
    "        attn_output_weights = attn_output_weights.masked_fill(\n",
    "            key_padding_mask.unsqueeze(1).unsqueeze(2),\n",
    "            float('-inf'),\n",
    "        )\n",
    "        attn_output_weights = attn_output_weights.view(bsz * num_heads, tgt_len, src_len)\n",
    "\n",
    "    attn_output_weights = softmax(\n",
    "        attn_output_weights, dim=-1)\n",
    "    attn_output_weights = dropout(attn_output_weights, p=dropout_p, training=training)\n",
    "\n",
    "    attn_output = torch.bmm(attn_output_weights, v)\n",
    "    assert list(attn_output.size()) == [bsz * num_heads, tgt_len, head_dim]\n",
    "    attn_output_ = attn_output.transpose(0, 1).contiguous().view(tgt_len, bsz, embed_dim)\n",
    "    attn_output = linear(attn_output_, out_proj_weight, out_proj_bias)\n",
    "    attn_output += linear(linear(attn_output_, out_proj_weight_A), out_proj_weight_B) * out_proj_weight_scaling\n",
    "\n",
    "    if need_weights:\n",
    "        # Average attention weights over heads\n",
    "        attn_output_weights = attn_output_weights.view(bsz, num_heads, tgt_len, src_len)\n",
    "        return attn_output, attn_output_weights.sum(dim=1) / num_heads\n",
    "    else:\n",
    "        return attn_output, None\n",
    "def mark_only_lora_as_trainable(model: nn.Module, bias: str = 'none') -> None:\n",
    "    for n, p in model.named_parameters():\n",
    "        if 'lora_' not in n:\n",
    "            p.requires_grad = False\n",
    "    if bias == 'none':\n",
    "        return\n",
    "    elif bias == 'all':\n",
    "        for n, p in model.named_parameters():\n",
    "            if 'bias' in n:\n",
    "                p.requires_grad = True\n",
    "    elif bias == 'lora_only':\n",
    "        for m in model.modules():\n",
    "            if isinstance(m, LoRALayer) and \\\n",
    "                hasattr(m, 'bias') and \\\n",
    "                m.bias is not None:\n",
    "                    m.bias.requires_grad = True\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "def lora_state_dict(model: nn.Module, bias: str = 'none') -> Dict[str, torch.Tensor]:\n",
    "    my_state_dict = model.state_dict()\n",
    "    if bias == 'none':\n",
    "        return {k: my_state_dict[k] for k in my_state_dict if 'lora_' in k}\n",
    "    elif bias == 'all':\n",
    "        return {k: my_state_dict[k] for k in my_state_dict if 'lora_' in k or 'bias' in k}\n",
    "    elif bias == 'lora_only':\n",
    "        to_return = {}\n",
    "        for k in my_state_dict:\n",
    "            if 'lora_' in k:\n",
    "                to_return[k] = my_state_dict[k]\n",
    "                bias_name = k.split('lora_')[0]+'bias'\n",
    "                if bias_name in my_state_dict:\n",
    "                    to_return[bias_name] = my_state_dict[bias_name]\n",
    "        return to_return\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "# Content adapted from CLIP's official github: openai/CLIP\n",
    "\n",
    "\n",
    "class Bottleneck(nn.Module):\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1):\n",
    "        super().__init__()\n",
    "\n",
    "        # All conv layers have stride 1. an avgpool is performed after the second convolution when stride > 1\n",
    "        self.conv1 = nn.Conv2d(inplanes, planes, 1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.relu1 = nn.ReLU(inplace=True)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(planes, planes, 3, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.relu2 = nn.ReLU(inplace=True)\n",
    "\n",
    "        self.avgpool = nn.AvgPool2d(stride) if stride > 1 else nn.Identity()\n",
    "\n",
    "        self.conv3 = nn.Conv2d(planes, planes * self.expansion, 1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(planes * self.expansion)\n",
    "        self.relu3 = nn.ReLU(inplace=True)\n",
    "\n",
    "        self.downsample = None\n",
    "        self.stride = stride\n",
    "\n",
    "        if stride > 1 or inplanes != planes * Bottleneck.expansion:\n",
    "            # Downsampling layer is prepended with an avgpool, and the subsequent convolution has stride 1\n",
    "            self.downsample = nn.Sequential(OrderedDict([\n",
    "                (\"-1\", nn.AvgPool2d(stride)),\n",
    "                (\"0\", nn.Conv2d(inplanes, planes * self.expansion, 1, stride=1, bias=False)),\n",
    "                (\"1\", nn.BatchNorm2d(planes * self.expansion))\n",
    "            ]))\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        identity = x\n",
    "\n",
    "        out = self.relu1(self.bn1(self.conv1(x)))\n",
    "        out = self.relu2(self.bn2(self.conv2(out)))\n",
    "        out = self.avgpool(out)\n",
    "        out = self.bn3(self.conv3(out))\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "\n",
    "        out += identity\n",
    "        out = self.relu3(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class AttentionPool2d(nn.Module):\n",
    "    def __init__(self, spacial_dim: int, embed_dim: int, num_heads: int, output_dim: int = None):\n",
    "        super().__init__()\n",
    "        self.positional_embedding = nn.Parameter(torch.randn(spacial_dim ** 2 + 1, embed_dim) / embed_dim ** 0.5)\n",
    "        self.k_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.q_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.v_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.c_proj = nn.Linear(embed_dim, output_dim or embed_dim)\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.flatten(start_dim=2).permute(2, 0, 1)  # NCHW -> (HW)NC\n",
    "        x = torch.cat([x.mean(dim=0, keepdim=True), x], dim=0)  # (HW+1)NC\n",
    "        x = x + self.positional_embedding[:, None, :].to(x.dtype)  # (HW+1)NC\n",
    "        x, _ = F.multi_head_attention_forward(\n",
    "            query=x[:1], key=x, value=x,\n",
    "            embed_dim_to_check=x.shape[-1],\n",
    "            num_heads=self.num_heads,\n",
    "            q_proj_weight=self.q_proj.weight,\n",
    "            k_proj_weight=self.k_proj.weight,\n",
    "            v_proj_weight=self.v_proj.weight,\n",
    "            in_proj_weight=None,\n",
    "            in_proj_bias=torch.cat([self.q_proj.bias, self.k_proj.bias, self.v_proj.bias]),\n",
    "            bias_k=None,\n",
    "            bias_v=None,\n",
    "            add_zero_attn=False,\n",
    "            dropout_p=0,\n",
    "            out_proj_weight=self.c_proj.weight,\n",
    "            out_proj_bias=self.c_proj.bias,\n",
    "            use_separate_proj_weight=True,\n",
    "            training=self.training,\n",
    "            need_weights=False\n",
    "        )\n",
    "        return x.squeeze(0)\n",
    "\n",
    "\n",
    "class ModifiedResNet(nn.Module):\n",
    "    \"\"\"\n",
    "    A ResNet class that is similar to torchvision's but contains the following changes:\n",
    "    - There are now 3 \"stem\" convolutions as opposed to 1, with an average pool instead of a max pool.\n",
    "    - Performs anti-aliasing strided convolutions, where an avgpool is prepended to convolutions with stride > 1\n",
    "    - The final pooling layer is a QKV attention instead of an average pool\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, layers, output_dim, heads, input_resolution=224, width=64):\n",
    "        super().__init__()\n",
    "        self.output_dim = output_dim\n",
    "        self.input_resolution = input_resolution\n",
    "\n",
    "        # The 3-layer stem\n",
    "        self.conv1 = nn.Conv2d(3, width // 2, kernel_size=3, stride=2, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(width // 2)\n",
    "        self.relu1 = nn.ReLU(inplace=True)\n",
    "        self.conv2 = nn.Conv2d(width // 2, width // 2, kernel_size=3, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(width // 2)\n",
    "        self.relu2 = nn.ReLU(inplace=True)\n",
    "        self.conv3 = nn.Conv2d(width // 2, width, kernel_size=3, padding=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(width)\n",
    "        self.relu3 = nn.ReLU(inplace=True)\n",
    "        self.avgpool = nn.AvgPool2d(2)\n",
    "\n",
    "        # Residual layers\n",
    "        self._inplanes = width  # This is a *mutable* variable used during construction\n",
    "        self.layer1 = self._make_layer(width, layers[0])\n",
    "        self.layer2 = self._make_layer(width * 2, layers[1], stride=2)\n",
    "        self.layer3 = self._make_layer(width * 4, layers[2], stride=2)\n",
    "        self.layer4 = self._make_layer(width * 8, layers[3], stride=2)\n",
    "\n",
    "        embed_dim = width * 32  # The ResNet feature dimension\n",
    "        self.attnpool = AttentionPool2d(input_resolution // 32, embed_dim, heads, output_dim)\n",
    "\n",
    "    def _make_layer(self, planes, blocks, stride=1):\n",
    "        layers = [Bottleneck(self._inplanes, planes, stride)]\n",
    "\n",
    "        self._inplanes = planes * Bottleneck.expansion\n",
    "        for _ in range(1, blocks):\n",
    "            layers.append(Bottleneck(self._inplanes, planes))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        def stem(x):\n",
    "            x = self.relu1(self.bn1(self.conv1(x)))\n",
    "            x = self.relu2(self.bn2(self.conv2(x)))\n",
    "            x = self.relu3(self.bn3(self.conv3(x)))\n",
    "            x = self.avgpool(x)\n",
    "            return x\n",
    "\n",
    "        x = x.type(self.conv1.weight.dtype)\n",
    "        x = stem(x)\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        x = self.attnpool(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class LayerNorm(nn.LayerNorm):\n",
    "    \"\"\"Subclass torch's LayerNorm to handle fp16.\"\"\"\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        orig_type = x.dtype\n",
    "        ret = super().forward(x.type(torch.float32))\n",
    "        return ret.type(orig_type)\n",
    "\n",
    "\n",
    "class QuickGELU(nn.Module):\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        return x * torch.sigmoid(1.702 * x)\n",
    "\n",
    "\n",
    "class ResidualAttentionBlock(nn.Module):\n",
    "    def __init__(self, d_model: int, n_head: int, attn_mask: torch.Tensor = None):\n",
    "        super().__init__()\n",
    "\n",
    "        self.attn = nn.MultiheadAttention(d_model, n_head)\n",
    "        self.ln_1 = LayerNorm(d_model)\n",
    "        self.mlp = nn.Sequential(OrderedDict([\n",
    "            (\"c_fc\", nn.Linear(d_model, d_model * 4)),\n",
    "            (\"gelu\", QuickGELU()),\n",
    "            (\"c_proj\", nn.Linear(d_model * 4, d_model))\n",
    "        ]))\n",
    "        self.ln_2 = LayerNorm(d_model)\n",
    "        self.attn_mask = attn_mask\n",
    "\n",
    "    def attention(self, x: torch.Tensor):\n",
    "        self.attn_mask = self.attn_mask.to(dtype=x.dtype, device=x.device) if self.attn_mask is not None else None\n",
    "        return self.attn(x, x, x, need_weights=False, attn_mask=self.attn_mask)[0]\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        x = x + self.attention(self.ln_1(x))\n",
    "        x = x + self.mlp(self.ln_2(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "# LoRA implementation of ResidualAttentionBlock:\n",
    "class LoRAResidualAttentionBlock(nn.Module):\n",
    "    def __init__(self, d_model: int, n_head: int, attn_mask: torch.Tensor = None, r=4):\n",
    "        super().__init__()\n",
    "\n",
    "        self.attn = MultiheadAttention(d_model, n_head, r=r) # LoRA rank set as 4\n",
    "        self.ln_1 = LayerNorm(d_model)\n",
    "        self.mlp = nn.Sequential(OrderedDict([\n",
    "            (\"c_fc\", Linear(d_model, d_model * 4, r=r)),\n",
    "            (\"gelu\", QuickGELU()),\n",
    "            (\"c_proj\", Linear(d_model * 4, d_model, r=r))\n",
    "        ]))\n",
    "        self.ln_2 = LayerNorm(d_model)\n",
    "        self.attn_mask = attn_mask\n",
    "\n",
    "    def attention(self, x: torch.Tensor):\n",
    "        self.attn_mask = self.attn_mask.to(dtype=x.dtype, device=x.device) if self.attn_mask is not None else None\n",
    "        return self.attn(x, x, x, need_weights=False, attn_mask=self.attn_mask)[0]\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        x = x + self.attention(self.ln_1(x))\n",
    "        x = x + self.mlp(self.ln_2(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, width: int, layers: int, heads: int, attn_mask: torch.Tensor = None):\n",
    "        super().__init__()\n",
    "        self.width = width\n",
    "        self.layers = layers\n",
    "        self.resblocks = nn.Sequential(*[ResidualAttentionBlock(width, heads, attn_mask) for _ in range(layers)])\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        return self.resblocks(x)\n",
    "\n",
    "\n",
    "# LoRA implementation of Transformer:\n",
    "class LoRATransformer(nn.Module):\n",
    "    def __init__(self, width: int, layers: int, heads: int, attn_mask: torch.Tensor = None, r = 4):\n",
    "        super().__init__()\n",
    "        self.width = width\n",
    "        self.layers = layers\n",
    "        self.resblocks = nn.Sequential(*[LoRAResidualAttentionBlock(width, heads, attn_mask, r=r) for _ in range(layers)])\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        return self.resblocks(x)\n",
    "\n",
    "\n",
    "class VisionTransformer(nn.Module):\n",
    "    def __init__(self, input_resolution: int, patch_size: int, width: int, layers: int, heads: int, output_dim: int):\n",
    "        super().__init__()\n",
    "        self.input_resolution = input_resolution\n",
    "        self.output_dim = output_dim\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=width, kernel_size=patch_size, stride=patch_size, bias=False)\n",
    "\n",
    "        scale = width ** -0.5\n",
    "        self.class_embedding = nn.Parameter(scale * torch.randn(width))\n",
    "        self.positional_embedding = nn.Parameter(scale * torch.randn((input_resolution // patch_size) ** 2 + 1, width))\n",
    "        self.ln_pre = LayerNorm(width)\n",
    "\n",
    "        self.transformer = Transformer(width, layers, heads)\n",
    "\n",
    "        self.ln_post = LayerNorm(width)\n",
    "        self.proj = nn.Parameter(scale * torch.randn(width, output_dim))\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        x = self.conv1(x)  # shape = [*, width, grid, grid]\n",
    "        x = x.reshape(x.shape[0], x.shape[1], -1)  # shape = [*, width, grid ** 2]\n",
    "        x = x.permute(0, 2, 1)  # shape = [*, grid ** 2, width]\n",
    "        x = torch.cat([self.class_embedding.to(x.dtype) + torch.zeros(x.shape[0], 1, x.shape[-1], dtype=x.dtype, device=x.device), x], dim=1)  # shape = [*, grid ** 2 + 1, width]\n",
    "        x = x + self.positional_embedding.to(x.dtype)\n",
    "        x = self.ln_pre(x)\n",
    "\n",
    "        x = x.permute(1, 0, 2)  # NLD -> LND\n",
    "        x = self.transformer(x)\n",
    "        x = x.permute(1, 0, 2)  # LND -> NLD\n",
    "\n",
    "        x = self.ln_post(x[:, 0, :])\n",
    "\n",
    "        if self.proj is not None:\n",
    "            x = x @ self.proj\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class LoRAVisionTransformer(nn.Module):\n",
    "    def __init__(self, input_resolution: int, patch_size: int, width: int, layers: int, heads: int, output_dim: int, r: int):\n",
    "        super().__init__()\n",
    "        self.input_resolution = input_resolution\n",
    "        self.output_dim = output_dim\n",
    "        self.conv1 = Conv2d(in_channels=3, out_channels=width, kernel_size=patch_size, stride=patch_size, bias=False)\n",
    "\n",
    "        scale = width ** -0.5\n",
    "        self.class_embedding = nn.Parameter(scale * torch.randn(width))\n",
    "        self.positional_embedding = nn.Parameter(scale * torch.randn((input_resolution // patch_size) ** 2 + 1, width))\n",
    "        self.ln_pre = LayerNorm(width)\n",
    "\n",
    "        self.transformer = LoRATransformer(width, layers, heads)\n",
    "\n",
    "        self.ln_post = LayerNorm(width)\n",
    "        self.proj = nn.Parameter(scale * torch.randn(width, output_dim))\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        x = self.conv1(x)  # shape = [*, width, grid, grid]\n",
    "        x = x.reshape(x.shape[0], x.shape[1], -1)  # shape = [*, width, grid ** 2]\n",
    "        x = x.permute(0, 2, 1)  # shape = [*, grid ** 2, width]\n",
    "        x = torch.cat([self.class_embedding.to(x.dtype) + torch.zeros(x.shape[0], 1, x.shape[-1], dtype=x.dtype, device=x.device), x], dim=1)  # shape = [*, grid ** 2 + 1, width]\n",
    "        x = x + self.positional_embedding.to(x.dtype)\n",
    "        x = self.ln_pre(x)\n",
    "\n",
    "        x = x.permute(1, 0, 2)  # NLD -> LND\n",
    "        x = self.transformer(x)\n",
    "        x = x.permute(1, 0, 2)  # LND -> NLD\n",
    "\n",
    "        x = self.ln_post(x[:, 0, :])\n",
    "\n",
    "        if self.proj is not None:\n",
    "            x = x @ self.proj\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class CLIP(nn.Module):\n",
    "    def __init__(self,\n",
    "                 embed_dim: int,\n",
    "                 # vision\n",
    "                 image_resolution: int,\n",
    "                 vision_layers: Union[Tuple[int, int, int, int], int],\n",
    "                 vision_width: int,\n",
    "                 vision_patch_size: int,\n",
    "                 # text\n",
    "                 context_length: int,\n",
    "                 vocab_size: int,\n",
    "                 transformer_width: int,\n",
    "                 transformer_heads: int,\n",
    "                 transformer_layers: int\n",
    "                 ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.context_length = context_length\n",
    "\n",
    "        if isinstance(vision_layers, (tuple, list)):\n",
    "            vision_heads = vision_width * 32 // 64\n",
    "            self.visual = ModifiedResNet(\n",
    "                layers=vision_layers,\n",
    "                output_dim=embed_dim,\n",
    "                heads=vision_heads,\n",
    "                input_resolution=image_resolution,\n",
    "                width=vision_width\n",
    "            )\n",
    "        else:\n",
    "            vision_heads = vision_width // 64\n",
    "            self.visual = VisionTransformer(\n",
    "                input_resolution=image_resolution,\n",
    "                patch_size=vision_patch_size,\n",
    "                width=vision_width,\n",
    "                layers=vision_layers,\n",
    "                heads=vision_heads,\n",
    "                output_dim=embed_dim\n",
    "            )\n",
    "\n",
    "        self.transformer = Transformer(\n",
    "            width=transformer_width,\n",
    "            layers=transformer_layers,\n",
    "            heads=transformer_heads,\n",
    "            attn_mask=self.build_attention_mask()\n",
    "        )\n",
    "\n",
    "        self.vocab_size = vocab_size\n",
    "        self.token_embedding = nn.Embedding(vocab_size, transformer_width)\n",
    "        self.positional_embedding = nn.Parameter(torch.empty(self.context_length, transformer_width))\n",
    "        self.ln_final = LayerNorm(transformer_width)\n",
    "\n",
    "        self.text_projection = nn.Parameter(torch.empty(transformer_width, embed_dim))\n",
    "        self.logit_scale = nn.Parameter(torch.ones([]) * np.log(1 / 0.07))\n",
    "\n",
    "        self.initialize_parameters()\n",
    "\n",
    "    def initialize_parameters(self):\n",
    "        nn.init.normal_(self.token_embedding.weight, std=0.02)\n",
    "        nn.init.normal_(self.positional_embedding, std=0.01)\n",
    "\n",
    "        if isinstance(self.visual, ModifiedResNet):\n",
    "            if self.visual.attnpool is not None:\n",
    "                std = self.visual.attnpool.c_proj.in_features ** -0.5\n",
    "                nn.init.normal_(self.visual.attnpool.q_proj.weight, std=std)\n",
    "                nn.init.normal_(self.visual.attnpool.k_proj.weight, std=std)\n",
    "                nn.init.normal_(self.visual.attnpool.v_proj.weight, std=std)\n",
    "                nn.init.normal_(self.visual.attnpool.c_proj.weight, std=std)\n",
    "\n",
    "            for resnet_block in [self.visual.layer1, self.visual.layer2, self.visual.layer3, self.visual.layer4]:\n",
    "                for name, param in resnet_block.named_parameters():\n",
    "                    if name.endswith(\"bn3.weight\"):\n",
    "                        nn.init.zeros_(param)\n",
    "\n",
    "        proj_std = (self.transformer.width ** -0.5) * ((2 * self.transformer.layers) ** -0.5)\n",
    "        attn_std = self.transformer.width ** -0.5\n",
    "        fc_std = (2 * self.transformer.width) ** -0.5\n",
    "        for block in self.transformer.resblocks:\n",
    "            nn.init.normal_(block.attn.in_proj_weight, std=attn_std)\n",
    "            nn.init.normal_(block.attn.out_proj.weight, std=proj_std)\n",
    "            nn.init.normal_(block.mlp.c_fc.weight, std=fc_std)\n",
    "            nn.init.normal_(block.mlp.c_proj.weight, std=proj_std)\n",
    "\n",
    "        if self.text_projection is not None:\n",
    "            nn.init.normal_(self.text_projection, std=self.transformer.width ** -0.5)\n",
    "\n",
    "    def build_attention_mask(self):\n",
    "        # lazily create causal attention mask, with full attention between the vision tokens\n",
    "        # pytorch uses additive attention mask; fill with -inf\n",
    "        mask = torch.empty(self.context_length, self.context_length)\n",
    "        mask.fill_(float(\"-inf\"))\n",
    "        mask.triu_(1)  # zero out the lower diagonal\n",
    "        return mask\n",
    "\n",
    "    @property\n",
    "    def dtype(self):\n",
    "        return self.visual.conv1.weight.dtype\n",
    "\n",
    "    def encode_image(self, image):\n",
    "        return self.visual(image.type(self.dtype))\n",
    "\n",
    "    def encode_text(self, text):\n",
    "        x = self.token_embedding(text).type(self.dtype)  # [batch_size, n_ctx, d_model]\n",
    "\n",
    "        x = x + self.positional_embedding.type(self.dtype)\n",
    "        x = x.permute(1, 0, 2)  # NLD -> LND\n",
    "        x = self.transformer(x)\n",
    "        x = x.permute(1, 0, 2)  # LND -> NLD\n",
    "        x = self.ln_final(x).type(self.dtype)\n",
    "\n",
    "        # x.shape = [batch_size, n_ctx, transformer.width]\n",
    "        # take features from the eot embedding (eot_token is the highest number in each sequence)\n",
    "        x = x[torch.arange(x.shape[0]), text.argmax(dim=-1)] @ self.text_projection\n",
    "\n",
    "        return x\n",
    "\n",
    "    def forward(self, image, text):\n",
    "        image_features = self.encode_image(image)\n",
    "        text_features = self.encode_text(text)\n",
    "\n",
    "        # normalized features\n",
    "        image_features = image_features / image_features.norm(dim=1, keepdim=True)\n",
    "        text_features = text_features / text_features.norm(dim=1, keepdim=True)\n",
    "\n",
    "        # cosine similarity as logits\n",
    "        logit_scale = self.logit_scale.exp()\n",
    "        logits_per_image = logit_scale * image_features @ text_features.t()\n",
    "        logits_per_text = logits_per_image.t()\n",
    "\n",
    "        # shape = [global_batch_size, global_batch_size]\n",
    "        return logits_per_image, logits_per_text\n",
    "\n",
    "\n",
    "class LoRACLIP(nn.Module):\n",
    "    def __init__(self,\n",
    "                 embed_dim: int,\n",
    "                 # vision\n",
    "                 image_resolution: int,\n",
    "                 vision_layers: Union[Tuple[int, int, int, int], int],\n",
    "                 vision_width: int,\n",
    "                 vision_patch_size: int,\n",
    "                 # text\n",
    "                 context_length: int,\n",
    "                 vocab_size: int,\n",
    "                 transformer_width: int,\n",
    "                 transformer_heads: int,\n",
    "                 transformer_layers: int,\n",
    "                 r: int,\n",
    "                 lora_mode: str\n",
    "                 ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.context_length = context_length\n",
    "\n",
    "        if isinstance(vision_layers, (tuple, list)):\n",
    "            vision_heads = vision_width * 32 // 64\n",
    "            self.visual = ModifiedResNet(\n",
    "                layers=vision_layers,\n",
    "                output_dim=embed_dim,\n",
    "                heads=vision_heads,\n",
    "                input_resolution=image_resolution,\n",
    "                width=vision_width\n",
    "            )\n",
    "        else:\n",
    "            vision_heads = vision_width // 64\n",
    "\n",
    "            if \"vision\" in lora_mode:\n",
    "                self.visual = LoRAVisionTransformer(\n",
    "                    input_resolution=image_resolution,\n",
    "                    patch_size=vision_patch_size,\n",
    "                    width=vision_width,\n",
    "                    layers=vision_layers,\n",
    "                    heads=vision_heads,\n",
    "                    output_dim=embed_dim,\n",
    "                    r=r\n",
    "                )\n",
    "            else:\n",
    "                self.visual = VisionTransformer(\n",
    "                    input_resolution=image_resolution,\n",
    "                    patch_size=vision_patch_size,\n",
    "                    width=vision_width,\n",
    "                    layers=vision_layers,\n",
    "                    heads=vision_heads,\n",
    "                    output_dim=embed_dim\n",
    "                )\n",
    "\n",
    "        if \"text\" in lora_mode:\n",
    "            self.transformer = LoRATransformer(\n",
    "                width=transformer_width,\n",
    "                layers=transformer_layers,\n",
    "                heads=transformer_heads,\n",
    "                attn_mask=self.build_attention_mask(),\n",
    "                r = r\n",
    "            )\n",
    "\n",
    "        else:\n",
    "            self.transformer = Transformer(\n",
    "                width=transformer_width,\n",
    "                layers=transformer_layers,\n",
    "                heads=transformer_heads,\n",
    "                attn_mask=self.build_attention_mask()\n",
    "            )\n",
    "\n",
    "        self.vocab_size = vocab_size\n",
    "\n",
    "        self.positional_embedding = nn.Parameter(torch.empty(self.context_length, transformer_width))\n",
    "        self.ln_final = LayerNorm(transformer_width)\n",
    "        self.logit_scale = nn.Parameter(torch.ones([]) * np.log(1 / 0.07))\n",
    "\n",
    "        self.text_projection = nn.Parameter(torch.empty(transformer_width, embed_dim))\n",
    "\n",
    "        if \"text\" in lora_mode:\n",
    "            self.lora_text_projection = Linear(transformer_width, embed_dim, r=r, bias=False)\n",
    "            self.token_embedding = Embedding(vocab_size, transformer_width, r=r)\n",
    "\n",
    "        else:\n",
    "            self.lora_text_projection = nn.Linear(transformer_width, embed_dim, bias=False)\n",
    "            self.token_embedding = nn.Embedding(vocab_size, transformer_width)\n",
    "\n",
    "        self.initialize_parameters()\n",
    "\n",
    "    def initialize_parameters(self):\n",
    "        nn.init.normal_(self.token_embedding.weight, std=0.02)\n",
    "        nn.init.normal_(self.positional_embedding, std=0.01)\n",
    "\n",
    "        if isinstance(self.visual, ModifiedResNet):\n",
    "            if self.visual.attnpool is not None:\n",
    "                std = self.visual.attnpool.c_proj.in_features ** -0.5\n",
    "                nn.init.normal_(self.visual.attnpool.q_proj.weight, std=std)\n",
    "                nn.init.normal_(self.visual.attnpool.k_proj.weight, std=std)\n",
    "                nn.init.normal_(self.visual.attnpool.v_proj.weight, std=std)\n",
    "                nn.init.normal_(self.visual.attnpool.c_proj.weight, std=std)\n",
    "\n",
    "            for resnet_block in [self.visual.layer1, self.visual.layer2, self.visual.layer3, self.visual.layer4]:\n",
    "                for name, param in resnet_block.named_parameters():\n",
    "                    if name.endswith(\"bn3.weight\"):\n",
    "                        nn.init.zeros_(param)\n",
    "\n",
    "        proj_std = (self.transformer.width ** -0.5) * ((2 * self.transformer.layers) ** -0.5)\n",
    "        attn_std = self.transformer.width ** -0.5\n",
    "        fc_std = (2 * self.transformer.width) ** -0.5\n",
    "        for block in self.transformer.resblocks:\n",
    "            nn.init.normal_(block.attn.in_proj_weight, std=attn_std)\n",
    "            nn.init.normal_(block.attn.out_proj.weight, std=proj_std)\n",
    "            nn.init.normal_(block.mlp.c_fc.weight, std=fc_std)\n",
    "            nn.init.normal_(block.mlp.c_proj.weight, std=proj_std)\n",
    "\n",
    "        if self.text_projection is not None:\n",
    "            nn.init.normal_(self.text_projection, std=self.transformer.width ** -0.5)\n",
    "\n",
    "    def build_attention_mask(self):\n",
    "        # lazily create causal attention mask, with full attention between the vision tokens\n",
    "        # pytorch uses additive attention mask; fill with -inf\n",
    "        mask = torch.empty(self.context_length, self.context_length)\n",
    "        mask.fill_(float(\"-inf\"))\n",
    "        mask.triu_(1)  # zero out the lower diagonal\n",
    "        return mask\n",
    "\n",
    "    @property\n",
    "    def dtype(self):\n",
    "        return self.visual.conv1.weight.dtype\n",
    "\n",
    "    def encode_image(self, image):\n",
    "        return self.visual(image.type(self.dtype))\n",
    "\n",
    "    def encode_text(self, text):\n",
    "        x = self.token_embedding(text).type(self.dtype)  # [batch_size, n_ctx, d_model]\n",
    "\n",
    "        x = x + self.positional_embedding.type(self.dtype)\n",
    "        x = x.permute(1, 0, 2)  # NLD -> LND\n",
    "        x = self.transformer(x)\n",
    "        x = x.permute(1, 0, 2)  # LND -> NLD\n",
    "        x = self.ln_final(x).type(self.dtype)\n",
    "\n",
    "        # x.shape = [batch_size, n_ctx, transformer.width]\n",
    "        # take features from the eot embedding (eot_token is the highest number in each sequence)\n",
    "        # x = x[torch.arange(x.shape[0]), text.argmax(dim=-1)] @ self.text_projection\n",
    "        x = x[torch.arange(x.shape[0]), text.argmax(dim=-1)]\n",
    "        x = self.lora_text_projection(x)\n",
    "        return x\n",
    "\n",
    "    def forward(self, image, text):\n",
    "        image_features = self.encode_image(image)\n",
    "        text_features = self.encode_text(text)\n",
    "\n",
    "        # normalized features\n",
    "        image_features = image_features / image_features.norm(dim=1, keepdim=True)\n",
    "        text_features = text_features / text_features.norm(dim=1, keepdim=True)\n",
    "\n",
    "        # cosine similarity as logits\n",
    "        logit_scale = self.logit_scale.exp()\n",
    "        logits_per_image = logit_scale * image_features @ text_features.t()\n",
    "        logits_per_text = logits_per_image.t()\n",
    "\n",
    "        # shape = [global_batch_size, global_batch_size]\n",
    "        return logits_per_image, logits_per_text\n",
    "\n",
    "def convert_weights(model: nn.Module):\n",
    "    \"\"\"Convert applicable model parameters to fp16\"\"\"\n",
    "\n",
    "    def _convert_weights_to_fp16(l):\n",
    "        if isinstance(l, (nn.Conv1d, nn.Conv2d, nn.Linear)):\n",
    "            l.weight.data = l.weight.data.half()\n",
    "            if l.bias is not None:\n",
    "                l.bias.data = l.bias.data.half()\n",
    "\n",
    "        if isinstance(l, nn.MultiheadAttention):\n",
    "            for attr in [*[f\"{s}_proj_weight\" for s in [\"in\", \"q\", \"k\", \"v\"]], \"in_proj_bias\", \"bias_k\", \"bias_v\"]:\n",
    "                tensor = getattr(l, attr)\n",
    "                if tensor is not None:\n",
    "                    tensor.data = tensor.data.half()\n",
    "\n",
    "        for name in [\"text_projection\", \"proj\"]:\n",
    "            if hasattr(l, name):\n",
    "                attr = getattr(l, name)\n",
    "                if attr is not None:\n",
    "                    attr.data = attr.data.half()\n",
    "\n",
    "    model.apply(_convert_weights_to_fp16)\n",
    "\n",
    "\n",
    "def build_model(state_dict: dict):\n",
    "    vit = \"visual.proj\" in state_dict\n",
    "\n",
    "    if vit:\n",
    "        vision_width = state_dict[\"visual.conv1.weight\"].shape[0]\n",
    "        vision_layers = len([k for k in state_dict.keys() if k.startswith(\"visual.\") and k.endswith(\".attn.in_proj_weight\")])\n",
    "        vision_patch_size = state_dict[\"visual.conv1.weight\"].shape[-1]\n",
    "        grid_size = round((state_dict[\"visual.positional_embedding\"].shape[0] - 1) ** 0.5)\n",
    "        image_resolution = vision_patch_size * grid_size\n",
    "    else:\n",
    "        counts: list = [len(set(k.split(\".\")[2] for k in state_dict if k.startswith(f\"visual.layer{b}\"))) for b in [1, 2, 3, 4]]\n",
    "        vision_layers = tuple(counts)\n",
    "        vision_width = state_dict[\"visual.layer1.0.conv1.weight\"].shape[0]\n",
    "        output_width = round((state_dict[\"visual.attnpool.positional_embedding\"].shape[0] - 1) ** 0.5)\n",
    "        vision_patch_size = None\n",
    "        assert output_width ** 2 + 1 == state_dict[\"visual.attnpool.positional_embedding\"].shape[0]\n",
    "        image_resolution = output_width * 32\n",
    "\n",
    "    embed_dim = state_dict[\"text_projection\"].shape[1]\n",
    "    context_length = state_dict[\"positional_embedding\"].shape[0]\n",
    "    vocab_size = state_dict[\"token_embedding.weight\"].shape[0]\n",
    "    transformer_width = state_dict[\"ln_final.weight\"].shape[0]\n",
    "    transformer_heads = transformer_width // 64\n",
    "    transformer_layers = len(set(k.split(\".\")[2] for k in state_dict if k.startswith(\"transformer.resblocks\")))\n",
    "\n",
    "    model = CLIP(\n",
    "        embed_dim,\n",
    "        image_resolution, vision_layers, vision_width, vision_patch_size,\n",
    "        context_length, vocab_size, transformer_width, transformer_heads, transformer_layers\n",
    "    )\n",
    "\n",
    "    for key in [\"input_resolution\", \"context_length\", \"vocab_size\"]:\n",
    "        if key in state_dict:\n",
    "            del state_dict[key]\n",
    "\n",
    "    convert_weights(model)\n",
    "    model.load_state_dict(state_dict)\n",
    "    return model.eval()\n",
    "\n",
    "def build_LoRA_model(state_dict: dict, r: int, lora_mode: str):\n",
    "    vit = \"visual.proj\" in state_dict\n",
    "\n",
    "    if vit:\n",
    "        vision_width = state_dict[\"visual.conv1.weight\"].shape[0]\n",
    "        vision_layers = len([k for k in state_dict.keys() if k.startswith(\"visual.\") and k.endswith(\".attn.in_proj_weight\")])\n",
    "        vision_patch_size = state_dict[\"visual.conv1.weight\"].shape[-1]\n",
    "        grid_size = round((state_dict[\"visual.positional_embedding\"].shape[0] - 1) ** 0.5)\n",
    "        image_resolution = vision_patch_size * grid_size\n",
    "    else:\n",
    "        counts: list = [len(set(k.split(\".\")[2] for k in state_dict if k.startswith(f\"visual.layer{b}\"))) for b in [1, 2, 3, 4]]\n",
    "        vision_layers = tuple(counts)\n",
    "        vision_width = state_dict[\"visual.layer1.0.conv1.weight\"].shape[0]\n",
    "        output_width = round((state_dict[\"visual.attnpool.positional_embedding\"].shape[0] - 1) ** 0.5)\n",
    "        vision_patch_size = None\n",
    "        assert output_width ** 2 + 1 == state_dict[\"visual.attnpool.positional_embedding\"].shape[0]\n",
    "        image_resolution = output_width * 32\n",
    "\n",
    "    embed_dim = state_dict[\"text_projection\"].shape[1]\n",
    "    context_length = state_dict[\"positional_embedding\"].shape[0]\n",
    "    vocab_size = state_dict[\"token_embedding.weight\"].shape[0]\n",
    "    transformer_width = state_dict[\"ln_final.weight\"].shape[0]\n",
    "    transformer_heads = transformer_width // 64\n",
    "    transformer_layers = len(set(k.split(\".\")[2] for k in state_dict if k.startswith(\"transformer.resblocks\")))\n",
    "\n",
    "    model = LoRACLIP(\n",
    "        embed_dim,\n",
    "        image_resolution, vision_layers, vision_width, vision_patch_size,\n",
    "        context_length, vocab_size, transformer_width, transformer_heads, transformer_layers,\n",
    "        r, lora_mode\n",
    "    )\n",
    "\n",
    "    for key in [\"input_resolution\", \"context_length\", \"vocab_size\"]:\n",
    "        if key in state_dict:\n",
    "            del state_dict[key]\n",
    "\n",
    "    new_state_dict = state_dict\n",
    "    new_state_dict[\"lora_text_projection.weight\"] = state_dict[\"text_projection\"]\n",
    "    \n",
    "    res = model.load_state_dict(new_state_dict, strict=False)\n",
    "    missing_keys = res.missing_keys\n",
    "    unexpected_keys = res.unexpected_keys\n",
    "    missing_keys = [x for x in missing_keys if 'lora_' not in x]  # ignore LoRA extra weights\n",
    "\n",
    "    print(\"Model loaded\")\n",
    "    if len(missing_keys) != 0:\n",
    "        print(f\"Missing keys: {missing_keys}\")\n",
    "\n",
    "    if len(unexpected_keys) != 0:\n",
    "        print(f\"Unexpected keys: {unexpected_keys}\")\n",
    "\n",
    "    print(\" \")\n",
    "\n",
    "    # here we mark only lora parameters as trainable\n",
    "    mark_only_lora_as_trainable(model)\n",
    "    # some caveats for loading a model for fine-tuning:\n",
    "    if \"text\" in lora_mode:\n",
    "        for name, param in model.named_parameters():\n",
    "            if \"positional_embedding\" in name:\n",
    "                param.requires_grad = True\n",
    "            if \"text_projection\" in name:\n",
    "                param.requires_grad = True\n",
    "            if \"logit_scale\" in name:\n",
    "                param.requires_grad = True\n",
    "\n",
    "    if \"vision\" in lora_mode:\n",
    "        for name, param in model.named_parameters():\n",
    "            if \"visual.proj\" in name:\n",
    "                param.requires_grad = True\n",
    "            if \"visual.class_embedding\" in name:\n",
    "                param.requires_grad = True\n",
    "            if \"visual.positional_embedding\" in name:\n",
    "                param.requires_grad = True\n",
    "\n",
    "    return model.eval()\n",
    "\n",
    "from torchvision.transforms import Compose, Resize, CenterCrop, ToTensor, Normalize\n",
    "from PIL import Image\n",
    "from typing import Callable, Union\n",
    "\n",
    "\n",
    "def _convert_image_to_rgb(image):\n",
    "    return image.convert(\"RGB\")\n",
    "\n",
    "def _transform(n_px):\n",
    "    return Compose([\n",
    "        Resize(n_px, interpolation=Image.BICUBIC),\n",
    "        CenterCrop(n_px),\n",
    "        _convert_image_to_rgb,\n",
    "        ToTensor(),\n",
    "        Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711)),\n",
    "    ])\n",
    "\n",
    "def Loraload(name: str,\n",
    "             device: Union[str, torch.device] = \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "             r: int = 4,\n",
    "             lora_mode: str = \"text\",\n",
    "             pretrained_model: torch.nn.Module = None,\n",
    "             preprocess: Callable = None):\n",
    "    \"\"\"\n",
    "    Apply LoRA to a pre-loaded CLIP model.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    name : str\n",
    "        Name of the CLIP model (not used here, included for compatibility).\n",
    "\n",
    "    device : str or torch.device\n",
    "        Device where the model should be placed.\n",
    "\n",
    "    r : int\n",
    "        LoRA rank.\n",
    "\n",
    "    lora_mode : str\n",
    "        Which encoder to modify: \"vision\", \"text\", or \"vision+text\".\n",
    "\n",
    "    pretrained_model : torch.nn.Module\n",
    "        The pre-loaded CLIP model.\n",
    "\n",
    "    preprocess : Callable\n",
    "        Preprocessing function for images.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    model : torch.nn.Module\n",
    "        CLIP model with LoRA applied.\n",
    "\n",
    "    preprocess : Callable\n",
    "        Preprocessing function (same as input).\n",
    "    \"\"\"\n",
    "    if pretrained_model is None:\n",
    "        raise ValueError(\"pretrained_model must be provided\")\n",
    "\n",
    "    model = build_LoRA_model(pretrained_model.state_dict(), r, lora_mode).to(device)\n",
    "\n",
    "\n",
    "    if str(device) == \"cpu\":\n",
    "        model.float()\n",
    "\n",
    "    return model, preprocess\n",
    "\n",
    "def tokenize(texts, context_length=77, truncate=False):\n",
    "    return clip.tokenize(texts, truncate=truncate)\n",
    "\n",
    "def print_trainable_parameters(model: nn.Module):\n",
    "    \"\"\"\n",
    "    Prints the number of trainable parameters in the model.\n",
    "    \"\"\"\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    all_param = sum(p.numel() for p in model.parameters())\n",
    "    print(f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param:.2f}%\")\n",
    "    \n",
    "    lora_present = any('lora_' in name for name, p in model.named_parameters() if p.requires_grad)\n",
    "    print(f\"Are LoRA parameters correctly present? {'Yes' if lora_present else 'No'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15ef08d6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-23T11:07:51.998800Z",
     "iopub.status.busy": "2025-08-23T11:07:51.998526Z",
     "iopub.status.idle": "2025-08-23T11:13:35.790660Z",
     "shell.execute_reply": "2025-08-23T11:13:35.789990Z",
     "shell.execute_reply.started": "2025-08-23T11:07:51.998781Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LoRA Fine-tuning on Vision Encoder\n",
      "Loading new LoRA model with adaptation only on Vision Encoder...\n",
      "Model loaded\n",
      " \n",
      "\n",
      "Verifying trainable parameters (vision-only):\n",
      "trainable params: 1397284 || all params: 150472741 || trainable%: 0.93%\n",
      "Are LoRA parameters correctly present? Yes\n",
      "\n",
      "Starting LoRA fine-tuning (vision-only)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/15: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:16<00:00,  1.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gate penalty:  0.0\n",
      "Loss:  1.1097882986068726\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/15: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:16<00:00,  1.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gate penalty:  0.0\n",
      "Loss:  0.5536289811134338\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/15: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:16<00:00,  1.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gate penalty:  0.0\n",
      "Loss:  0.09587176144123077\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/15: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:16<00:00,  1.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gate penalty:  0.0\n",
      "Loss:  0.02785930223762989\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/15: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:16<00:00,  1.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gate penalty:  0.0\n",
      "Loss:  0.0060172672383487225\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/15: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:16<00:00,  1.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gate penalty:  tensor(0.2069, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss:  0.007949995808303356\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/15: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:16<00:00,  1.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gate penalty:  tensor(0.0623, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss:  0.010825090110301971\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/15: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:16<00:00,  1.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gate penalty:  tensor(0.0084, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss:  0.005615874193608761\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/15: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:16<00:00,  1.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gate penalty:  tensor(0.0058, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss:  0.004095203243196011\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/15: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:16<00:00,  1.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gate penalty:  tensor(0.0037, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss:  0.003215175587683916\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/15: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:16<00:00,  1.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gate penalty:  tensor(0.0048, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss:  0.005093134939670563\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/15: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:16<00:00,  1.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gate penalty:  tensor(0.0041, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss:  0.004338990896940231\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/15: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:16<00:00,  1.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gate penalty:  tensor(0.0034, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss:  0.004297136794775724\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/15: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:16<00:00,  1.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gate penalty:  tensor(0.0030, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss:  0.004280951805412769\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/15: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:16<00:00,  1.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gate penalty:  tensor(0.0031, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss:  0.0046112677082419395\n",
      "\n",
      "Evaluating LoRA fine-tuned model (vision-only)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "âœï¸ LoRA-Vision on Base: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:32<00:00,  1.61s/it]\n",
      "âœï¸ LoRA-Vision on Novel: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [00:47<00:00,  1.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ” LoRA-Vision on Base Accuracy: 95.31%\n",
      "ðŸ” LoRA-Vision on Novel Accuracy: 32.18%\n",
      "ðŸ” LoRA-Vision Harmonic Mean: 48.12%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"LoRA Fine-tuning on Vision Encoder\")\n",
    "print(\"Loading new LoRA model with adaptation only on Vision Encoder...\")\n",
    "\n",
    "# Reload the original model to start from a clean base\n",
    "original_clip_model, clip_preprocess = clip.load(\"ViT-B/16\", device=device)\n",
    "\n",
    "# Apply LoRA only to the vision part of the model\n",
    "lora_model, _ = Loraload(\n",
    "    name=\"ViT-B/16\",\n",
    "    device=device,\n",
    "    r=8,\n",
    "    lora_mode=\"vision\",  # Key modification\n",
    "    pretrained_model=original_clip_model,\n",
    "    preprocess=clip_preprocess\n",
    ")\n",
    "\n",
    "print(\"\\nVerifying trainable parameters (vision-only):\")\n",
    "print_trainable_parameters(lora_model)\n",
    "\n",
    "# A simple training loop \n",
    "lora_model.train()\n",
    "learning_rate = 5e-4\n",
    "optimizer = optim.AdamW(filter(lambda p: p.requires_grad, lora_model.parameters()), lr=learning_rate)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "num_epochs = 15\n",
    "train_base_loader = torch.utils.data.DataLoader(train_base, batch_size=32, shuffle=True)\n",
    "\n",
    "print(\"\\nStarting LoRA fine-tuning (vision-only)...\")\n",
    "for epoch in range(num_epochs):\n",
    "    pbar = tqdm(train_base_loader, desc=f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "    for image, target in pbar:\n",
    "        image, target = image.to(device), target.to(device)\n",
    "        text_inputs = clip.tokenize([f\"a photo of a {CLASS_NAMES[c]}, a type of flower.\" for c in base_classes]).to(device)\n",
    "        \n",
    "        image_features = lora_model.encode_image(image)\n",
    "        text_features = lora_model.encode_text(text_inputs)\n",
    "        \n",
    "        image_features = image_features / image_features.norm(dim=-1, keepdim=True)\n",
    "        text_features = text_features / text_features.norm(dim=-1, keepdim=True)\n",
    "        \n",
    "        logits = lora_model.logit_scale.exp() * image_features @ text_features.T\n",
    "        remapped_target = torch.tensor([base_classes.index(t.item()) for t in target]).to(device)\n",
    "        loss = loss_fn(logits, remapped_target)\n",
    "####################################################################################################\n",
    "        gate_penalty = 0.0\n",
    "        max_penalty = 1.5\n",
    "        start_epoch = 5\n",
    "        end_epoch = num_epochs\n",
    "        current_coeff = 0.0\n",
    "        if epoch >= start_epoch:\n",
    "            progress = (epoch - start_epoch) / (end_epoch - start_epoch)\n",
    "            progress = min(progress, 1.0)\n",
    "            current_coeff = max_penalty * progress\n",
    "            for name, param in lora_model.named_parameters():\n",
    "                if \"lora_gate\" in name:\n",
    "                    # print(\"lora_gate: \", param)\n",
    "                    gate_penalty += torch.abs(param) # Add gate value to the penalty\n",
    "            loss += (current_coeff * gate_penalty.sum())\n",
    "####################################################################################################\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(\"Gate penalty: \", gate_penalty)\n",
    "    print(\"Loss: \", loss.item())\n",
    "\n",
    "print(\"\\nEvaluating LoRA fine-tuned model (vision-only)...\")\n",
    "lora_model.eval()\n",
    "\n",
    "# Re-evaluate the model with the LoRA-tuned vision encoder on base and novel classes\n",
    "base_accuracy_lora_vision = eval_zeroshot(model=lora_model, dataset=test_base, categories=base_classes, batch_size=128, device=device, label=\"âœï¸ LoRA-Vision on Base\")\n",
    "novel_accuracy_lora_vision = eval_zeroshot(model=lora_model, dataset=test_novel, categories=novel_classes, batch_size=128, device=device, label=\"âœï¸ LoRA-Vision on Novel\")\n",
    "\n",
    "hm_lora_vision = harmonic_mean(base_accuracy_lora_vision, novel_accuracy_lora_vision)\n",
    "\n",
    "print(f\"\\nðŸ” LoRA-Vision on Base Accuracy: {base_accuracy_lora_vision*100:.2f}%\")\n",
    "print(f\"ðŸ” LoRA-Vision on Novel Accuracy: {novel_accuracy_lora_vision*100:.2f}%\")\n",
    "print(f\"ðŸ” LoRA-Vision Harmonic Mean: {hm_lora_vision*100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "670b642c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-23T11:13:35.792105Z",
     "iopub.status.busy": "2025-08-23T11:13:35.791717Z",
     "iopub.status.idle": "2025-08-23T11:13:35.988137Z",
     "shell.execute_reply": "2025-08-23T11:13:35.987552Z",
     "shell.execute_reply.started": "2025-08-23T11:13:35.792078Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x700 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot comparison between Zero-Shot CLIP and LoRA-Vision\n",
    "labels = ['Base Accuracy', 'Novel Accuracy', 'Harmonic Mean']\n",
    "clip_scores = [base_accuracy_zeroshot, novel_accuracy_zeroshot, hm_zeroshot]\n",
    "lora_scores = [base_accuracy_lora_vision, novel_accuracy_lora_vision, hm_lora_vision]\n",
    "\n",
    "x = np.arange(len(labels))\n",
    "width = 0.35\n",
    "fig, ax = plt.subplots(figsize=(12, 7))\n",
    "rects1 = ax.bar(x - width/2, clip_scores, width, label='Zero-Shot CLIP', color='skyblue')\n",
    "rects2 = ax.bar(x + width/2, lora_scores, width, label='LoRA', color='darkkhaki')\n",
    "\n",
    "ax.set_ylabel('Accuracy')\n",
    "ax.set_title('Comparison: Zero-Shot CLIP vs. LoRA Fine-tuning on Vision Encoder')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(labels)\n",
    "ax.set_ylim(0, 1.0)\n",
    "ax.legend()\n",
    "autolabel(rects1)\n",
    "autolabel(rects2)\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3b4653f",
   "metadata": {},
   "source": [
    "Unfortunately, the initial fine-tuning results are strong on the base classes but show poor accuracy on the novel classes. This indicates that the model struggles to generalize beyond the base distribution, a phenomenon known as \"Catastrophic Forgetting\". To address this, we first attempted to implement an advanced strategy inspired by the paper \"Fine Tuning without Catastrophic Forgetting via Selective Low Rank Adaptation\" [5].\n",
    "\n",
    "The core idea is to introduce a mechanism to selectively activate or deactivate LoRA modules. This is achieved by adding a learnable parameter, the lora_gate ($g_i$), for each LoRA module, which modulates its output:\n",
    "$$\n",
    "Î”W(x)=Î”W(x)â‹…Ïƒ(g_{i})\n",
    "$$\n",
    "To encourage selectivity, a penalty is added to the main loss function, pushing non-essential gates towards zero:\n",
    "$$\n",
    "L_{total}=L_{original}+Î»_{i}âˆ‘âˆ£g_{i}âˆ£\n",
    "$$\n",
    "Where $L_{original}$ is the standard cross-entropy loss and $Î»$ is a coefficient that controls the strength of the penalty. \n",
    "During first attempts the penalty doesn't descrease during the training phase.\n",
    "This led us to a more sophisticated hypothesis: the issue might not be the magnitude of the coefficient, but rather the timing of its application. At the beginning of the training, the model has no information about which LoRA modules are useful. Imposing a strong penalty from the start would force the model to deactivate all modules as the safest strategy.\n",
    "\n",
    "We therefore decided to allow the model an initial \"exploration\" phase. In our final implementation, we started the training with $Î»=0$ for the first few epochs, and then gradually increased its value up to a maximum of 1.5 in the subsequent epochs.\n",
    "\n",
    "However, our experiments showed that this improvement was not sufficient to mitigate the problem in our specific context. To address this, we explore additional techniques aimed at preserving performance on novel classes while maintaining or improving accuracy on base classes. Specifically, we apply Tip-Adapter [4], followed by a logit ensembling strategy combined with LoRA-based adaptation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3b451a8-b79e-499b-a45e-3339954267a2",
   "metadata": {},
   "source": [
    "### Tip-Adapter-F and Adaptive Ensemble"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33f288bd",
   "metadata": {},
   "source": [
    "We build on a two-expert framework combining the strengths of the original CLIP model (the generalist) and a LoRA-finetuned CLIP model specialized for base classes (the specialist). This approach leverages Tip-Adapter-F, a cache-based method that enhances few-shot adaptation by storing and reusing features extracted from the training set, alongside an adaptive ensemble that dynamically weights predictions from both experts to optimize accuracy on both base and novel classes.\n",
    "\n",
    "### Theory\n",
    "The key idea behind Tip-Adapter-F is to construct a feature cache from the training images using the LoRA-finetuned model, which captures specialized representations for the base classes. At inference, the similarity between test image features and this cache is combined with zero-shot logits from the fine-tuned model to boost prediction confidence. This mechanism effectively integrates explicit memory of the training data into the modelâ€™s decision process, improving base-class performance without retraining.\n",
    "\n",
    "To maintain generalization on unseen (novel) classes, the original CLIP modelâ€™s zero-shot predictions are preserved as a complementary generalist expert. The adaptive ensemble blends the specialist (Tip-Adapter-F-enhanced fine-tuned model) and the generalist (original CLIP) by weighting their softmax probabilities with a parameter Î² that reflects confidence: higher weight is given to the specialist on base classes and to the generalist on novel classes. This dynamic weighting balances specialization with generalization, addressing the common challenge of base-to-novel trade-offs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ce0206",
   "metadata": {},
   "source": [
    "### Implementation Details\n",
    "#### Cache Creation:\n",
    "We create the Tip-Adapter feature cache by extracting normalized image embeddings from the LoRA-finetuned model over the base training set, saving both features and labels. This cache acts as a memory bank representing specialized knowledge acquired during fine-tuning.\n",
    "\n",
    "#### Adaptive Ensemble Evaluation:\n",
    "During evaluation, both the original and LoRA-finetuned models encode text prompts for the target classes to generate normalized text features. For each test batch:\n",
    "\n",
    "1. The original model produces zero-shot image features, from which logits and softmax probabilities over classes are computed.\n",
    "\n",
    "2. The LoRA-finetuned model produces image features used for two predictions: zero-shot logits (similar to the original model) and cache-based logits derived from similarity between image features and the cached training features. These are combined with a scaling hyperparameter (tip_alpha) to form Tip-Adapter logits and probabilities.\n",
    "\n",
    "3. If evaluating on base classes, the Tip-Adapter logits include cache information; for novel classes, the cache is ignored, relying solely on the fine-tuned modelâ€™s zero-shot predictions.\n",
    "\n",
    "4. The ensemble probability distribution is then calculated by interpolating between the generalist and specialist probabilities using Î² (higher Î² on base classes, lower on novel classes).\n",
    "\n",
    "5. Final predictions are made by selecting the class with the highest ensembled probability.\n",
    "\n",
    "Accuracy is computed for both base and novel test sets, and their harmonic mean is reported to quantify balanced performance across known and unknown classes.\n",
    "\n",
    "This method effectively leverages the LoRA fine-tuning for specialization while preserving CLIPâ€™s original zero-shot generalization through adaptive ensemble weighting and cache-based memory augmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bcd4b69c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-23T11:13:35.989334Z",
     "iopub.status.busy": "2025-08-23T11:13:35.989058Z",
     "iopub.status.idle": "2025-08-23T11:15:19.155353Z",
     "shell.execute_reply": "2025-08-23T11:15:19.154710Z",
     "shell.execute_reply.started": "2025-08-23T11:13:35.989314Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tip-Adapter-F and Adaptive Ensemble\n",
      "Combining Tip-Adapter-F with an Adaptive Ensemble on the LoRA fine-tuned model.\n",
      "Creating feature cache from train_base with LoRA model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:06<00:00,  2.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cache created.\n",
      "\n",
      "Final evaluation with Tip-Adapter-F and Adaptive Ensemble...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ðŸš€ Final on Base: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:38<00:00,  1.94s/it]\n",
      "ðŸš€ Final on Novel: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [00:57<00:00,  1.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… Final Base Accuracy: 96.00%\n",
      "âœ… Final Novel Accuracy: 77.64%\n",
      "âœ… Final Harmonic Mean: 85.85%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Tip-Adapter-F and Adaptive Ensemble\")\n",
    "print(\"Combining Tip-Adapter-F with an Adaptive Ensemble on the LoRA fine-tuned model.\")\n",
    "\n",
    "# Create the cache for Tip-Adapter-F using the LoRA-fine-tuned model\n",
    "@torch.no_grad()\n",
    "def create_cache(model, train_loader, device):\n",
    "    \"\"\"Extracts features from the training set to create the Tip-Adapter cache.\"\"\"\n",
    "    model.eval()\n",
    "    all_features = []\n",
    "    all_labels = []\n",
    "    print(\"Creating feature cache from train_base with LoRA model...\")\n",
    "    for images, labels in tqdm(train_loader):\n",
    "        images = images.to(device)\n",
    "        features = model.encode_image(images)\n",
    "        features /= features.norm(dim=-1, keepdim=True)\n",
    "        all_features.append(features)\n",
    "        all_labels.append(labels)\n",
    "    return torch.cat(all_features), torch.cat(all_labels)\n",
    "\n",
    "\n",
    "# We use the LoRA model as the \"specialist\" for cache creation\n",
    "train_base_loader_no_shuffle = torch.utils.data.DataLoader(train_base,\n",
    "                                                           batch_size=32,\n",
    "                                                           shuffle=False)\n",
    "cache_features, cache_labels = create_cache(lora_model,\n",
    "                                            train_base_loader_no_shuffle,\n",
    "                                            device)\n",
    "print(\"Cache created.\")\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_adaptive_tip_ensemble(original_model, finetuned_model, cache_feats, cache_labs, dataset, categories, batch_size, device, label=\"\"):\n",
    "    \"\"\"Evaluates the adaptive ensemble using Tip-Adapter-F.\"\"\"\n",
    "    original_model.eval()\n",
    "    finetuned_model.eval()\n",
    "    contig_cat2idx = {cat: idx for idx, cat in enumerate(categories)}\n",
    "    cache_labs_remapped = torch.tensor([base_classes.index(l.item()) for l in cache_labs]).to(device)\n",
    "    \n",
    "    # Get text features for both models\n",
    "    text_inputs = clip.tokenize([f\"a photo of a {CLASS_NAMES[c]}, a type of flower.\" for c in categories]).to(device)\n",
    "    original_text_features = original_model.encode_text(text_inputs)\n",
    "    original_text_features /= original_text_features.norm(dim=-1, keepdim=True)\n",
    "    \n",
    "    finetuned_text_features = finetuned_model.encode_text(text_inputs)\n",
    "    finetuned_text_features /= finetuned_text_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "    correct_predictions = 0\n",
    "    tip_alpha = 1.0  # Hyperparameter for Tip-Adapter\n",
    "    \n",
    "    for images, targets in tqdm(dataloader, desc=label):\n",
    "        targets_remapped = torch.tensor([contig_cat2idx[t.item()] for t in targets]).to(device)\n",
    "        images = images.to(device)\n",
    "\n",
    "        # --- Prediction from Model 1: Original CLIP (Generalist Expert) ---\n",
    "        original_image_features = original_model.encode_image(images)\n",
    "        original_image_features /= original_image_features.norm(dim=-1, keepdim=True)\n",
    "        original_logits = original_image_features @ original_text_features.T\n",
    "        original_probs = F.softmax(original_model.logit_scale.exp() * original_logits, dim=-1)\n",
    "\n",
    "        # --- Prediction from Model 2: Tip-Adapter-F (Specialist Expert) ---\n",
    "        finetuned_image_features = finetuned_model.encode_image(images)\n",
    "        finetuned_image_features /= finetuned_image_features.norm(dim=-1, keepdim=True)\n",
    "        \n",
    "        zeroshot_logits_finetuned = finetuned_model.logit_scale.exp() * (finetuned_image_features @ finetuned_text_features.T)\n",
    "        \n",
    "        # Tip-Adapter cache logits\n",
    "        cache_similarity = finetuned_image_features @ cache_feats.T\n",
    "        cache_logits = torch.zeros(images.shape[0], len(base_classes), dtype=cache_similarity.dtype, device=device)\n",
    "        cache_logits.scatter_add_(1, cache_labs_remapped.repeat(images.shape[0], 1), cache_similarity)\n",
    "\n",
    "        # Combine Tip-Adapter logits\n",
    "        if set(categories) == set(base_classes):\n",
    "             tip_adapter_logits = zeroshot_logits_finetuned + tip_alpha * cache_logits\n",
    "        else: # For novel classes, cache is not relevant, use only zero-shot from fine-tuned model\n",
    "             tip_adapter_logits = zeroshot_logits_finetuned\n",
    "        \n",
    "        tip_adapter_probs = F.softmax(tip_adapter_logits, dim=-1)\n",
    "\n",
    "        # --- Adaptive Ensemble ---\n",
    "        if set(categories) == set(base_classes):\n",
    "            beta = 0.8 # Higher confidence in the specialist for base classes\n",
    "        else:\n",
    "            beta = 0.2 # Higher confidence in the generalist for novel classes\n",
    "        \n",
    "        ensembled_probs = (1 - beta) * original_probs + beta * tip_adapter_probs\n",
    "        \n",
    "        predicted_class = ensembled_probs.argmax(dim=-1)\n",
    "        correct_predictions += (predicted_class == targets_remapped).sum().item()\n",
    "\n",
    "    accuracy = correct_predictions / len(dataset)\n",
    "    return accuracy\n",
    "\n",
    "print(\"\\nFinal evaluation with Tip-Adapter-F and Adaptive Ensemble...\")\n",
    "base_accuracy_final = eval_adaptive_tip_ensemble(original_clip_model, lora_model, cache_features, cache_labels, test_base, base_classes, 128, device, label=\"ðŸš€ Final on Base\")\n",
    "novel_accuracy_final = eval_adaptive_tip_ensemble(original_clip_model, lora_model, cache_features, cache_labels, test_novel, novel_classes, 128, device, label=\"ðŸš€ Final on Novel\")\n",
    "\n",
    "hm_final = harmonic_mean(base_accuracy_final, novel_accuracy_final)\n",
    "\n",
    "print(f\"\\nâœ… Final Base Accuracy: {base_accuracy_final*100:.2f}%\")\n",
    "print(f\"âœ… Final Novel Accuracy: {novel_accuracy_final*100:.2f}%\")\n",
    "print(f\"âœ… Final Harmonic Mean: {hm_final*100:.2f}%\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95f5f599",
   "metadata": {},
   "source": [
    "## Results\n",
    "Here, we present a comparative plot of the results obtained from each method to see their performance differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "84659132",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-23T11:15:19.156753Z",
     "iopub.status.busy": "2025-08-23T11:15:19.156341Z",
     "iopub.status.idle": "2025-08-23T11:15:19.393977Z",
     "shell.execute_reply": "2025-08-23T11:15:19.393322Z",
     "shell.execute_reply.started": "2025-08-23T11:15:19.156725Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## 5. Comparison of All Strategies\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1400x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"## 5. Comparison of All Strategies\")\n",
    "\n",
    "# Store final scores for comparison plot\n",
    "lora_tip_ensemble_scores = [base_accuracy_final, novel_accuracy_final, hm_final]\n",
    "\n",
    "# Prepare scores for plotting\n",
    "labels = ['Base Accuracy', 'Novel Accuracy', 'Harmonic Mean']\n",
    "zeroshot_scores = [base_accuracy_zeroshot, novel_accuracy_zeroshot, hm_zeroshot]\n",
    "lora_text_scores = [base_accuracy_lora_vision, novel_accuracy_lora_vision, hm_lora_vision]\n",
    "lora_tip_ensemble_scores = [base_accuracy_final, novel_accuracy_final, hm_final]\n",
    "\n",
    "x = np.arange(len(labels))\n",
    "width = 0.25\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 8))\n",
    "\n",
    "rects1 = ax.bar(x - width, zeroshot_scores, width, label='Zero-Shot CLIP', color='skyblue')\n",
    "rects2 = ax.bar(x, lora_text_scores, width, label='LoRA (Vision Only) Fine-Tuning', color='darkkhaki')\n",
    "rects3 = ax.bar(x + width, lora_tip_ensemble_scores, width, label='LoRA + Tip-Adapter + Adaptive Ensemble', color='gold')\n",
    "\n",
    "ax.set_ylabel('Accuracy')\n",
    "ax.set_title('Comparison of Few-Shot Adaptation Strategies for CLIP')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(labels)\n",
    "ax.set_ylim(0, 1.1)\n",
    "ax.legend()\n",
    "\n",
    "def autolabel(rects):\n",
    "    for rect in rects:\n",
    "        height = rect.get_height()\n",
    "        ax.annotate(f'{height*100:.2f}%',\n",
    "                    xy=(rect.get_x() + rect.get_width() / 2, height),\n",
    "                    xytext=(0, 3),\n",
    "                    textcoords=\"offset points\",\n",
    "                    ha='center', va='bottom')\n",
    "\n",
    "autolabel(rects1)\n",
    "autolabel(rects2)\n",
    "autolabel(rects3)\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2dfcbb1",
   "metadata": {},
   "source": [
    "### Discussion of the results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "434c9e44",
   "metadata": {},
   "source": [
    "When LoRA was applied to the vision encoder the model became better aligned with the training distribution and achieved strong results on the base classes, although in our experiments it failed to preserve Novel accuracy. This suggests that LoRA modifies the image embedding space too aggressively. The main problem likely lies in the fact that LoRA focuses purely on weight adaptation and does not involve prompt optimization. In few-shot scenarios with very limited data, this often leads to overfitting on Base classes and a consequent loss of performance on Novel classes.\n",
    "\n",
    "In contrast, Tip-Adapter-F exhibited a different behavior. By relying on cache-based similarity with few-shot exemplars, it was able to retain robust recognition of novel classes. Yet this method lacked the same representational flexibility as LoRA, and therefore did not reach comparable accuracy on the base categories. What emerged was a complementary relationship: LoRA specialized strongly on the classes it adapted to, while Tip-Adapter-F preserved the modelâ€™s capacity to generalize more broadly.\n",
    "\n",
    "The ensemble strategy brought these two perspectives together. By combining logits from LoRA and Tip-Adapter-F, the ensemble produced higher scores on both base and novel categories, ultimately achieving the best harmonic mean. This suggests that the hybrid approach acted as a balancing mechanism, drawing on LoRA to refine semantic alignment while relying on Tip-Adapter-F to prevent the system from collapsing into overfitting. The results therefore underline the importance of designing adaptation strategies that do not only maximize performance on seen data but also safeguard the modelâ€™s zero-shot abilities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d891ddc1",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46934c6f",
   "metadata": {},
   "source": [
    "In this work, we examined parameter-efficient adaptation of CLIP for few-shot image classification on the Oxford Flowers102 dataset. Our experiments showed that applying LoRA to the vision encoder significantly increased accuracy on base categories but simultaneously reduced performance on novel ones, highlithing a strong overfitting and loss of generalization by the model. \n",
    "\n",
    "To balance this problem, we explored Tip-Adapter, which leverages a cache of few-shot exemplars to maintain performance on unseen categories. Although it does not reach LoRAâ€™s level on base classes, its strength lies in preserving transferability. Finally, by combining LoRA and Tip-Adapter through logit ensembling, we obtained the most balanced results, achieving improved accuracy on both base and novel categories and thus the highest harmonic mean.\n",
    "\n",
    "These findings suggest that no single parameter-efficient method fully resolves the trade-off between specialization and generalization. Instead, hybrid strategies that integrate complementary approaches provide a more effective path forward. Lots of future work are possible, such as investigate adaptive ensemble weights that adjust dynamically according to task difficulty or incorporate regularization mechanisms to mitigate overfitting in LoRA. Overall, our study underscores the value of combining lightweight adaptation with memory-based strategies to achieve efficient and robust few-shot learning in visionâ€“language models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f788836e",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04c5630c",
   "metadata": {},
   "source": [
    "[1] M. -E. Nilsback and A. Zisserman, \"Automated Flower Classification over a Large Number of Classes,\" 2008 Sixth Indian Conference on Computer Vision, Graphics & Image Processing, Bhubaneswar, India, 2008, pp. 722-729, doi: 10.1109/ICVGIP.2008.47.\n",
    "\n",
    "[2] E. J. Hu, Y. Shen, P. Wallis, Z. Allen-Zhu, Y. Li, S. Wang, L. Wang, and W. Chen, \"LoRA: Low-Rank Adaptation of Large Language Models\"  2021.\n",
    "\n",
    "[3] M. Zanella and I. Ben Ayed, \"Low-Rank Few-Shot Adaptation of Vision-Language Models\" 2022.\n",
    "\n",
    "[4] R. Zhang, W. Zhang, R. Fang, P. Gao, K. Li, J. Dai, Y. Qiao, and H. Li, \"Tip-Adapter: Training-free Adaptation of CLIP for Few-shot Classification\" 2022.\n",
    "\n",
    "[5] R. Akbarian Bafghi, C. Bagwell, A. Ravichandran, A. Shrivastava, and M. Raissi, \"Fine Tuning without Catastrophic Forgetting via Selective Low Rank Adaptation\" 2025.\n",
    "\n",
    "[6] K. Zhou, J. Yang, C. C. Loy, and Z. Liu, â€œLearning to Prompt for Vision-Language Modelsâ€ International Journal of Computer Vision (IJCV), 2022. \n",
    "\n",
    "[7] K. Zhou, J. Yang, C. C. Loy, and Z. Liu, â€œConditional Prompt Learning for Vision-Language Modelsâ€ Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2022. \n",
    "\n",
    "[8] P. Gao, S. Geng, R. Zhang, T. Ma, R. Fang, Y. Zhang, H. Li, and Y. Qiao, â€œCLIP-Adapter: Better Vision-Language Models with Feature Adaptersâ€ International Journal of Computer Vision (IJCV), 2024.\n",
    "\n",
    "[9] H. Liu, D. Tam, M. Muqeeth, J. Mohta, T. Huang, M. Bansal, and C. Raffel, â€œFew-Shot Parameter-Efficient Fine-Tuning is Better and Cheaper than In-Context Learningâ€ Advances in Neural Information Processing Systems (NeurIPS), 2022. \n",
    "\n",
    "[10] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark, et al., â€œLearning Transferable Visual Models from Natural Language Supervisionâ€ Proceedings of the International Conference on Machine Learning (ICML), 2021."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
